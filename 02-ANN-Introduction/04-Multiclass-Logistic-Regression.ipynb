{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logistic Regression\n",
    "\n",
    "Multiclass logistic regression is a generalization of binary logistic regression to multiple classes. In binary logistic regression, the output variable $y$ is binary, taking values in $\\{0, 1\\}$. In multiclass logistic regression, the output variable $y$ can take on $K$ different values, where $K > 2$. The model is also known as multinomial logistic regression.\n",
    "\n",
    "## Model\n",
    "\n",
    "As in the binary classification case, we have a set of $n$ observations, each with $p$ features. The input data is represented by a matrix $X$ of size $n \\times p$, where each row corresponds to an observation and each column corresponds to a feature. The output variable $y$ is represented by a vector of size $n$, where each element is an integer in the range $\\{1, 2, \\ldots, K\\}$ (in `Python` it is convenient for the categories to take values in \n",
    "$\\{0, 1, \\ldots, K - 1\\}$).\n",
    "\n",
    "The model assumes that the probability of an observation $i$ belonging to class $k$ is given by the softmax function:\n",
    "\n",
    "$$\n",
    "P(y_i = k | x_i) = \\frac{e^{w_k^T x_i}}{\\sum_{j=1}^K e^{w_j^T x_i}}\n",
    "$$\n",
    "\n",
    "where $w_k$ is the weight vector for class $k$ and $x_i$ is the feature vector for observation $i$. The softmax function ensures that the predicted probabilities sum to 1 over all classes.\n",
    "\n",
    "The dot product $w_k^T x_i$ is the linear predictor for class $k$ and observation $i$. The likelihood (assuming the observations are independent) is given by:\n",
    "\n",
    "$$\n",
    "L(w) = \\prod_{i=1}^n \\prod_{k=1}^K P(y_i = k | x_i)^{I(y_i = k)}\n",
    "$$\n",
    "\n",
    "where $I(y_i = k)$ is an indicator function that is 1 if $y_i = k$ and 0 otherwise. The log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ell(w) = \\sum_{i=1}^n \\sum_{k=1}^K I(y_i = k) \\log P(y_i = k | x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "## Multiclass Logistic Regression as a Neural Network\n",
    "\n",
    "The multinomial logistic regression model can be represented as a neural network with a single layer of neurons where each neuron corresponds to a class. The input layer has $p$ neurons, one for each feature, and the output layer has $K$ neurons, one for each class. The weights of the model are represented by the edges connecting the input layer to the output layer (the bias terms are not shown in the diagram). The output of each neuron in the output layer is passed through the softmax function to obtain the predicted probabilities.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "%%| label: fig-single-neuron-multiclass\n",
    "%%| fig-width: 6\n",
    "%%| fig-cap: \"ANN model for logistic regression for a single observation\"\n",
    "\n",
    "graph LR\n",
    "    x1[\"$$x_{i1}$$\"] -->|$$w_1$$| B1((\"$$w_{1}^T x_i + b_1$$\"))\n",
    "    x2[\"$$x_{i2}$$\"] -->|$$w_2$$| B2((\"$$w_{2}^T x_i + b_2$$\"))\n",
    "    xp[\"$$x_{ip}$$\"] -->|$$w_p$$| B3((\"$$w_{p}^T x_i + b_p$$\"))\n",
    "    x1 --> B2\n",
    "    x1 --> B3\n",
    "    x2 --> B1\n",
    "    x2 --> B3\n",
    "    xp --> B1\n",
    "    xp --> B2\n",
    "    B1 --> P1[\"$$\\hat{y}$$\"]\n",
    "    B2 --> P2[\"$$\\hat{y}$$\"]\n",
    "    B3 --> P3[\"$$\\hat{y}$$\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross-Entropy\n",
    "\n",
    "In evaluating a model's accuracy, we need a measure between our model's prediction and a perfect (out-of-sample) prediction. This measure should be able to account for the fact that some outcomes (targets) are easier to predict than others. Consider the task of predicting the weather (sunshine/rain) in a deser, where it almost never rains. A model that always predicts sunshine will be correct most of the time, but it is not a very useful model as you will always be surprised when it rains.\n",
    "\n",
    "The *entropy* of a distribution is a measure of its uncertainty that has four properties\n",
    "\n",
    "- It is zero if the distribution is degenerate (i.e. the outcome is always sunshine)\n",
    "- It is continuous, so a small change in the distribution will result in a small change in the entropy\n",
    "- It is higher for distributions with can produce more different outcomes than for distributions that can produce fewer outcomes\n",
    "- It is additive, so the entropy of a distribution is the sum of the entropies of its components. This means that if we first measure the uncertainty about being male/female and then measure the uncertainty about being a soccer fan or not, the uncertainty of the combinations (male/soccer fan, male/not soccer fan, female/soccer-fan, female/not soccer fan) should the sum of the two uncertainties.\n",
    "\n",
    "It is easy to show that the entropy defined as the expected value of the log-probabilities of the outcomes satisfies these four properties.\n",
    "\n",
    "$$\n",
    "H(p) = -\\sum_{k} p_k \\log p_k\n",
    "$$\n",
    "\n",
    "So the entropy gives us the uncertainty when predicting outcomes using the true distribution. In classification problems, however, we don't know this distribution. Instead, we rely on a model to produce probabilities that we hope are close to the true probabilities. We can ask: how much does the uncertainty increase if we use the wrong (the model's) probabilities (Q) instead of the true probabilities? This is the *cross-entropy*.\n",
    "\n",
    "$$\n",
    "H(p, q) = -\\sum_{k} p_k \\log q_k\n",
    "$$\n",
    "\n",
    "It can also be decomposed into the entropy of the true distribution and the Kullback-Leibler divergence between the true distribution and the model distribution.\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(p) + \\text{KL}(p, q)\n",
    "$$\n",
    "\n",
    "In the above expression, H(p) is the entropy of the data-generating distribution, and KL(p, q) is the Kullback-Leibler divergence between the data-generating distribution and the model distribution. The KL divergence is always non-negative, and it is zero if the two distributions are identical. Therefore, the cross-entropy is always greater than or equal to the entropy of the data-generating distribution.\n",
    "\n",
    "$$\n",
    "\\text{KL} = \\sum_{k} p_k (\\log p_k - \\log q_k) = \\sum_{i} p_k \\log \\frac{p_k}{q_k}\n",
    "$$\n",
    "\n",
    "The KL-divergence describes how different P and Q are on average (in units of entropy). You have likely encountered a scaled version of it when studying generalized linear models (GLM) under the name of *deviance*. The deviance is the KL-divergence between the data-generating distribution and the model distribution, scaled by a factor of two.  \n",
    "\n",
    "In gradient descent, we want to minimize the cross-entropy between the true distribution (the labels) and the model distribution (the predicted probabilities). The loss function for multiclass logistic regression is the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\text{CE}(w) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log \\hat{y}_{ik}\n",
    "$$\n",
    "\n",
    "where $y_{ik}$ is an indicator function that is 1 if observation $i$ belongs to class $k$ and 0 otherwise, and $\\hat{y}_{ik}$ is the predicted probability that observation $i$ belongs to class $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: [0.1 0.9]\n",
      "Entropy: 0.3250829733914482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.9])\n",
    "print(\"p:\", p)\n",
    "print(\"Entropy:\", -np.sum(p * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.5 0.5]\n",
      "Entropy: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "q = np.array([1/2, 1/2])\n",
    "print(\"q:\", q)\n",
    "print(\"Entropy:\", -np.sum(q * np.log(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.33333333 0.33333333 0.33333333]\n",
      "Entropy: 1.0986122886681096\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([1/3, 1/3, 1/3])\n",
    "print(\"q:\", q1)\n",
    "print(\"Entropy:\", -np.sum(q1 * np.log(q1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p: 0.639031859650177\n",
      "Entropy of q: 1.0986122886681096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.1, 0.8])\n",
    "\n",
    "# The entropy of p is given by\n",
    "\n",
    "entropy_p = -np.sum(p * np.log(p))\n",
    "print(f\"Entropy of p: {entropy_p}\")\n",
    "\n",
    "# Values from the distribution of p would be easier to predict than values from the following distribution q\n",
    "\n",
    "q = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Its entropy should be higher, reflecting the uncertainty in the distribution.\n",
    "\n",
    "entropy_q = -np.sum(q * np.log(q))\n",
    "print(f\"Entropy of q: {entropy_q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [0.1 0.1 0.8]\n",
      "Q:  [0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \", p)\n",
    "print(\"Q: \", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy of q using q: 1.0986122886681096\n",
      "Cross-entropy of q using p: 1.6094379124341\n",
      "KL divergence between q and p: 0.5108256237659906\n"
     ]
    }
   ],
   "source": [
    "# When we try to predict values from q using q, the cross-entropy is equal to the entropy of q\n",
    "\n",
    "cross_entropy_q_q = -np.sum(q * np.log(q))\n",
    "\n",
    "print(f\"Cross-entropy of q using q: {cross_entropy_q_q}\")\n",
    "\n",
    "# If we use p to predict values from q, the cross-entropy is higher, as we will be wrong more often:\n",
    "\n",
    "cross_entropy_p_q = -np.sum(q * np.log(p))\n",
    "\n",
    "print(f\"Cross-entropy of q using p: {cross_entropy_p_q}\")\n",
    "\n",
    "# The KL divergence between q and p measures the extra (above the inherent prediction difficulty)\n",
    "# difficulty in predicting values from q using p, compared to using q\n",
    "\n",
    "kl_divergence_q_p = -np.sum(q * np.log(p/q))\n",
    "\n",
    "print(f\"KL divergence between q and p: {kl_divergence_q_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Updates\n",
    "\n",
    "As the networks become more complex, it is convenient to represent it in matrix notation and use vectorized operations. To understand it, we must pay special attention to the layout of the data and the shapes of the matrices.\n",
    "\n",
    "The multiclass logistic regresion model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(W^T X)\n",
    "$$\n",
    "\n",
    "The shapes of the matrices are:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{P \\times N}$, where $N$ is the number of observations in the data (or the batch) and $P$ is the number of input features.\n",
    "- $W \\in \\mathbb{R}^{N \\times K}$, where $K$ is the number of classes.\n",
    "\n",
    "\n",
    "The softmax function is applied to each column of the $K \\times N$ matrix $W^T X$ to obtain the predicted probabilities for each class of each observation. The cross-entropy loss is then computed by comparing the predicted probabilities with the true labels.\n",
    "\n",
    "Let's create a small example to illustrate the forward pass and the back-propagation step for a batch of N = 2 observations, each with P = 4 features and K = 3 classes.\n",
    "\n",
    "The input matrix $X \\in \\mathbb{R}^{4 \\times 2}$ is:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The weight matrix $W \\in \\mathbb{R}^{4 \\times 3}$ is:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "1.0 & 1.1 & 1.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The true labels $y \\in \\mathbb{R}^{3 \\times 2}$ are:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The true labels matrix here shows that the first observation belongs to class 0, the second observation belongs to class 1.\n",
    "\n",
    "The forward pass is:\n",
    "\n",
    "$$\n",
    "Z = W^T X = \\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.7 & 1.0 \\\\\n",
    "0.2 & 0.5 & 0.8 & 1.1 \\\\\n",
    "0.3 & 0.6 & 0.9 & 1.2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "11.8 & 14.0 \\\\\n",
    "13.4 & 16.0 \\\\\n",
    "15 & 18\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The softmax function is applied to each column of $Z$ to obtain the predicted probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{softmax}(Z) = \\begin{bmatrix}\n",
    "0.03280241 & 0.01587624 \\\\\n",
    "0.16247141 & 0.11731043 \\\\\n",
    "0.80472617 & 0.86681333\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that the predicted probabilities sum to 1 for each observation (column).\n",
    "\n",
    "The matrix of true labels must have K rows, where K is the number of classes (3 in this case), and N columns, where N is the number of observations (2 in this case). Let the first observation belong to class 0, the second observation to class 2. The matrix of true labels is:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The logarithm of the predicted probabilities is computed element-wise:\n",
    "\n",
    "$$\n",
    "\\log \\hat{Y} = \\begin{bmatrix}\n",
    "-3.4187718 & -4.14313473 \\\\\n",
    "-1.8187718 & -2.14113473 \\\\\n",
    "-0.2187718 & -0.14313473\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6],\n",
       "       [7, 8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.2, 0.3],\n",
       "       [0.4, 0.5, 0.6],\n",
       "       [0.7, 0.8, 0.9],\n",
       "       [1. , 1.1, 1.2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.8, 14. ],\n",
       "       [13.4, 16. ],\n",
       "       [15. , 18. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  133252.35294553,  1202604.28416478],\n",
       "       [  660003.22476616,  8886110.52050787],\n",
       "       [ 3269017.37247211, 65659969.13733051]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(W.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03280241, 0.01587624],\n",
       "       [0.16247141, 0.11731043],\n",
       "       [0.80472617, 0.86681333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = np.exp(W.T @ X) / np.sum(np.exp(W.T @ X), axis=0, keepdims=True)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicator matrix for the true classes\n",
    "\n",
    "Y = np.array([[1, 0, 0], [0, 0, 1]]).T\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.41725321, -4.14293163],\n",
       "       [-1.81725321, -2.14293163],\n",
       "       [-0.21725321, -0.14293163]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96719759, -0.01587624],\n",
       "       [-0.16247141, -0.11731043],\n",
       "       [-0.80472617,  0.13318667]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y - Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
