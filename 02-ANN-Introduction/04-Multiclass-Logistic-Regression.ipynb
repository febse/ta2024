{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logistic Regression\n",
    "\n",
    "Multiclass logistic regression is a generalization of binary logistic regression to multiple classes. In binary logistic regression, the output variable $y$ is binary, taking values in $\\{0, 1\\}$. In multiclass logistic regression, the output variable $y$ can take on $K$ different values, where $K > 2$. The model is also known as multinomial logistic regression.\n",
    "\n",
    "## Model\n",
    "\n",
    "As in the binary classification case, we have a set of $n$ observations, each with $p$ features. The input data is represented by a matrix $X$ of size $n \\times p$, where each row corresponds to an observation and each column corresponds to a feature. The output variable $y$ is represented by a vector of size $n$, where each element is an integer in the range $\\{1, 2, \\ldots, K\\}$ (in `Python` it is convenient for the categories to take values in \n",
    "$\\{0, 1, \\ldots, K - 1\\}$).\n",
    "\n",
    "The model assumes that the probability of an observation $i$ belonging to class $k$ is given by the softmax function:\n",
    "\n",
    "$$\n",
    "P(y_i = k | x_i) = \\frac{e^{w_k^T x_i}}{\\sum_{j=1}^K e^{w_j^T x_i}}\n",
    "$$\n",
    "\n",
    "where $w_k$ is the weight vector for class $k$ and $x_i$ is the feature vector for observation $i$. The softmax function ensures that the predicted probabilities sum to 1 over all classes.\n",
    "\n",
    "The dot product $w_k^T x_i$ is the linear predictor for class $k$ and observation $i$. The likelihood (assuming the observations are independent) is given by:\n",
    "\n",
    "$$\n",
    "L(w) = \\prod_{i=1}^n \\prod_{k=1}^K P(y_i = k | x_i)^{I(y_i = k)}\n",
    "$$\n",
    "\n",
    "where $I(y_i = k)$ is an indicator function that is 1 if $y_i = k$ and 0 otherwise. The log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ell(w) = \\sum_{i=1}^n \\sum_{k=1}^K I(y_i = k) \\log P(y_i = k | x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "## Multiclass Logistic Regression as a Neural Network\n",
    "\n",
    "The multinomial logistic regression model can be represented as a neural network with a single layer of neurons where each neuron corresponds to a class. The input layer has $p$ neurons, one for each feature, and the output layer has $K$ neurons, one for each class. The weights of the model are represented by the edges connecting the input layer to the output layer (the bias terms are not shown in the diagram). The output of each neuron in the output layer is passed through the softmax function to obtain the predicted probabilities.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "%%| label: fig-single-neuron-multiclass\n",
    "%%| fig-width: 6\n",
    "%%| fig-cap: \"ANN model for logistic regression for a single observation\"\n",
    "\n",
    "graph LR\n",
    "    x1[\"$$x_{i1}$$\"] -->|$$w_1$$| B1((\"$$w_{1}^T x_i + b_1$$\"))\n",
    "    x2[\"$$x_{i2}$$\"] -->|$$w_2$$| B2((\"$$w_{2}^T x_i + b_2$$\"))\n",
    "    xp[\"$$x_{ip}$$\"] -->|$$w_p$$| B3((\"$$w_{p}^T x_i + b_p$$\"))\n",
    "    x1 --> B2\n",
    "    x1 --> B3\n",
    "    x2 --> B1\n",
    "    x2 --> B3\n",
    "    xp --> B1\n",
    "    xp --> B2\n",
    "    B1 --> P1[\"$$\\hat{y}$$\"]\n",
    "    B2 --> P2[\"$$\\hat{y}$$\"]\n",
    "    B3 --> P3[\"$$\\hat{y}$$\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross-Entropy\n",
    "\n",
    "In evaluating a model's accuracy, we need a measure between our model's prediction and a perfect (out-of-sample) prediction. This measure should be able to account for the fact that some outcomes (targets) are easier to predict than others. Consider the task of predicting the weather (sunshine/rain) in a deser, where it almost never rains. A model that always predicts sunshine will be correct most of the time, but it is not a very useful model as you will always be surprised when it rains.\n",
    "\n",
    "The *entropy* of a distribution is a measure of its uncertainty that has four properties\n",
    "\n",
    "- It is zero if the distribution is degenerate (i.e. the outcome is always sunshine)\n",
    "- It is continuous, so a small change in the distribution will result in a small change in the entropy\n",
    "- It is higher for distributions with can produce more different outcomes than for distributions that can produce fewer outcomes\n",
    "- It is additive, so the entropy of a distribution is the sum of the entropies of its components. This means that if we first measure the uncertainty about being male/female and then measure the uncertainty about being a soccer fan or not, the uncertainty of the combinations (male/soccer fan, male/not soccer fan, female/soccer-fan, female/not soccer fan) should the sum of the two uncertainties.\n",
    "\n",
    "It is easy to show that the entropy defined as the expected value of the log-probabilities of the outcomes satisfies these four properties.\n",
    "\n",
    "$$\n",
    "H(p) = \\sum_{k} p_k \\log p_k\n",
    "$$\n",
    "\n",
    "So the entropy gives us the uncertainty when predicting outcomes using the true distribution. In classification problems, however, we don't know this distribution. Instead, we rely on a model to produce probabilities that we hope are close to the true probabilities. We can ask: how much does the uncertainty increase if we use the wrong (the model's) probabilities (Q) instead of the true probabilities? This is the *cross-entropy*.\n",
    "\n",
    "$$\n",
    "H(p, q) = -\\sum_{k} p_k \\log q_k\n",
    "$$\n",
    "\n",
    "It can also be decomposed into the entropy of the true distribution and the Kullback-Leibler divergence between the true distribution and the model distribution.\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(p) + \\text{KL}(p, q)\n",
    "$$\n",
    "\n",
    "In the above expression, H(p) is the entropy of the data-generating distribution, and KL(p, q) is the Kullback-Leibler divergence between the data-generating distribution and the model distribution. The KL divergence is always non-negative, and it is zero if the two distributions are identical. Therefore, the cross-entropy is always greater than or equal to the entropy of the data-generating distribution.\n",
    "\n",
    "$$\n",
    "\\text{KL} = \\sum_{k} p_k (\\log p_k - \\log q_k) = \\sum_{i} p_k \\log \\frac{p_k}{q_k}\n",
    "$$\n",
    "\n",
    "The KL-divergence describes how different P and Q are on average (in units of entropy). You have likely encountered a scaled version of it when studying generalized linear models (GLM) under the name of *deviance*. The deviance is the KL-divergence between the data-generating distribution and the model distribution, scaled by a factor of two.  \n",
    "\n",
    "In gradient descent, we want to minimize the cross-entropy between the true distribution (the labels) and the model distribution (the predicted probabilities). The loss function for multiclass logistic regression is the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\text{CE}(w) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log \\hat{y}_{ik}\n",
    "$$\n",
    "\n",
    "where $y_{ik}$ is an indicator function that is 1 if observation $i$ belongs to class $k$ and 0 otherwise, and $\\hat{y}_{ik}$ is the predicted probability that observation $i$ belongs to class $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: [0.1 0.9]\n",
      "Entropy: 0.3250829733914482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.9])\n",
    "print(\"p:\", p)\n",
    "print(\"Entropy:\", -np.sum(p * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.5 0.5]\n",
      "Entropy: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "q = np.array([1/2, 1/2])\n",
    "print(\"q:\", q)\n",
    "print(\"Entropy:\", -np.sum(q * np.log(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.33333333 0.33333333 0.33333333]\n",
      "Entropy: 1.0986122886681096\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([1/3, 1/3, 1/3])\n",
    "print(\"q:\", q1)\n",
    "print(\"Entropy:\", -np.sum(q1 * np.log(q1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.1, 0.8])\n",
    "\n",
    "# The entropy of p is given by\n",
    "\n",
    "entropy_p = -np.sum(p * np.log(p))\n",
    "print(f\"Entropy of p: {entropy_p}\")\n",
    "\n",
    "# Values from the distribution of p would be easier to predict than values from the following distribution q\n",
    "\n",
    "q = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Its entropy should be higher, reflecting the uncertainty in the distribution.\n",
    "\n",
    "entropy_q = -np.sum(q * np.log(q))\n",
    "print(f\"Entropy of q: {entropy_q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [0.1 0.9]\n",
      "Q:  [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \", p)\n",
    "print(\"Q: \", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy of q using q: 0.6931471805599453\n",
      "Cross-entropy of q using p: 1.203972804325936\n",
      "KL divergence between q and p: 0.5108256237659906\n"
     ]
    }
   ],
   "source": [
    "# When we try to predict values from q using q, the cross-entropy is equal to the entropy of q\n",
    "\n",
    "cross_entropy_q_q = -np.sum(q * np.log(q))\n",
    "\n",
    "print(f\"Cross-entropy of q using q: {cross_entropy_q_q}\")\n",
    "\n",
    "# If we use p to predict values from q, the cross-entropy is higher, as we will be wrong more often:\n",
    "\n",
    "cross_entropy_p_q = -np.sum(q * np.log(p))\n",
    "\n",
    "print(f\"Cross-entropy of q using p: {cross_entropy_p_q}\")\n",
    "\n",
    "# The KL divergence between q and p measures the extra (above the inherent prediction difficulty)\n",
    "# difficulty in predicting values from q using p, compared to using q\n",
    "\n",
    "kl_divergence_q_p = -np.sum(q * np.log(p/q))\n",
    "\n",
    "print(f\"KL divergence between q and p: {kl_divergence_q_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Updates\n",
    "\n",
    "\n",
    "\n",
    "First, it is convenient to write down the network model as equations and to keep track of the dimensions of the vectors and matrices. The input data is represented by a matrix $X$ of size $n \\times p$, where each row corresponds to an observation and each column corresponds to a feature. The output variable $y$ is represented by a matrix of size $n \\times K$, where each row corresponds to an observation and each column corresponds to a class. The weight matrix $W$ is of size $p \\times K$, where each column corresponds to the weights for a class.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z & = W^T X \\in \\mathbb{R}^K \\\\\n",
    "\\hat{y} & = \\text{softmax}(z) \\in [0, 1]^{K}\\\\\n",
    "CE(w) & = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log \\hat{y}_{ik} \\in \\mathbb{R} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In order to use gradient descent to find the optimal weights, we need to compute the gradient of the log-likelihood with respect to the weights.\n",
    "\n",
    "The gradient of the loss function with respect to the weights is given by a matrix of the same shape as the weight matrix. The element in row $k$ and column $j$ of the gradient matrix is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial CE(w)}{\\partial w_{jk}} = -\\sum_{i=1}^n x_{ij} (y_{ik} - \\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "In matrix form, the gradient is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial CE(w)}{\\partial W} = -X^T (Y - \\hat{Y})\n",
    "$$\n",
    "\n",
    "where $Y$ is the matrix of true labels and $\\hat{Y}$ is the matrix of predicted probabilities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
