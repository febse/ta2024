{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logistic Regression\n",
    "\n",
    "Multiclass logistic regression is a generalization of binary logistic regression to multiple classes. In binary logistic regression, the output variable $y$ is binary, taking values in $\\{0, 1\\}$. In multiclass logistic regression, the output variable $y$ can take on $K$ different values, where $K > 2$. The model is also known as multinomial logistic regression.\n",
    "\n",
    "## Model\n",
    "\n",
    "As in the binary classification case, we have a set of $n$ observations, each with $p$ features. The input data is represented by a matrix $X$ of size $n \\times p$, where each row corresponds to an observation and each column corresponds to a feature. The output variable $y$ is represented by a vector of size $n$, where each element is an integer in the range $\\{1, 2, \\ldots, K\\}$ (in `Python` it is convenient for the categories to take values in \n",
    "$\\{0, 1, \\ldots, K - 1\\}$).\n",
    "\n",
    "The model assumes that the probability of an observation $i$ belonging to class $k$ is given by the softmax function:\n",
    "\n",
    "$$\n",
    "P(y_i = k | x_i) = \\frac{e^{w_k^T x_i}}{\\sum_{j=1}^K e^{w_j^T x_i}}\n",
    "$$\n",
    "\n",
    "where $w_k$ is the weight vector for class $k$ and $x_i$ is the feature vector for observation $i$. The softmax function ensures that the predicted probabilities sum to 1 over all classes.\n",
    "\n",
    "The dot product $w_k^T x_i$ is the linear predictor for class $k$ and observation $i$. The likelihood (assuming the observations are independent) is given by:\n",
    "\n",
    "$$\n",
    "L(w) = \\prod_{i=1}^n \\prod_{k=1}^K P(y_i = k | x_i)^{I(y_i = k)}\n",
    "$$\n",
    "\n",
    "where $I(y_i = k)$ is an indicator function that is 1 if $y_i = k$ and 0 otherwise. The log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ell(w) = \\sum_{i=1}^n \\sum_{k=1}^K I(y_i = k) \\log P(y_i = k | x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "## Multiclass Logistic Regression as a Neural Network\n",
    "\n",
    "The multinomial logistic regression model can be represented as a neural network with a single layer of neurons where each neuron corresponds to a class. The input layer has $p$ neurons, one for each feature, and the output layer has $K$ neurons, one for each class. The weights of the model are represented by the edges connecting the input layer to the output layer (the bias terms are not shown in the diagram). The output of each neuron in the output layer is passed through the softmax function to obtain the predicted probabilities.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "%%| label: fig-single-neuron-multiclass\n",
    "%%| fig-width: 6\n",
    "%%| fig-cap: \"ANN model for logistic regression for a single observation\"\n",
    "\n",
    "graph LR\n",
    "    x1[\"$$x_{i1}$$\"] -->|$$w_1$$| B1((\"$$w_{1}^T x_i + b_1$$\"))\n",
    "    x2[\"$$x_{i2}$$\"] -->|$$w_2$$| B2((\"$$w_{2}^T x_i + b_2$$\"))\n",
    "    xp[\"$$x_{ip}$$\"] -->|$$w_p$$| B3((\"$$w_{p}^T x_i + b_p$$\"))\n",
    "    x1 --> B2\n",
    "    x1 --> B3\n",
    "    x2 --> B1\n",
    "    x2 --> B3\n",
    "    xp --> B1\n",
    "    xp --> B2\n",
    "    B1 --> P1[\"$$\\hat{y}$$\"]\n",
    "    B2 --> P2[\"$$\\hat{y}$$\"]\n",
    "    B3 --> P3[\"$$\\hat{y}$$\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross-Entropy\n",
    "\n",
    "In evaluating a model's accuracy, we need a measure between our model's prediction and a perfect (out-of-sample) prediction. This measure should be able to account for the fact that some outcomes (targets) are easier to predict than others. Consider the task of predicting the weather (sunshine/rain) in a deser, where it almost never rains. A model that always predicts sunshine will be correct most of the time, but it is not a very useful model as you will always be surprised when it rains.\n",
    "\n",
    "The *entropy* of a distribution is a measure of its uncertainty that has four properties\n",
    "\n",
    "- It is zero if the distribution is degenerate (i.e. the outcome is always sunshine)\n",
    "- It is continuous, so a small change in the distribution will result in a small change in the entropy\n",
    "- It is higher for distributions with can produce more different outcomes than for distributions that can produce fewer outcomes\n",
    "- It is additive, so the entropy of a distribution is the sum of the entropies of its components. This means that if we first measure the uncertainty about being male/female and then measure the uncertainty about being a soccer fan or not, the uncertainty of the combinations (male/soccer fan, male/not soccer fan, female/soccer-fan, female/not soccer fan) should the sum of the two uncertainties.\n",
    "\n",
    "It is easy to show that the entropy defined as the expected value of the log-probabilities of the outcomes satisfies these four properties.\n",
    "\n",
    "$$\n",
    "H(p) = -\\sum_{k} p_k \\log p_k\n",
    "$$\n",
    "\n",
    "So the entropy gives us the uncertainty when predicting outcomes using the true distribution. In classification problems, however, we don't know this distribution. Instead, we rely on a model to produce probabilities that we hope are close to the true probabilities. We can ask: how much does the uncertainty increase if we use the wrong (the model's) probabilities (Q) instead of the true probabilities? This is the *cross-entropy*.\n",
    "\n",
    "$$\n",
    "H(p, q) = -\\sum_{k} p_k \\log q_k\n",
    "$$\n",
    "\n",
    "It can also be decomposed into the entropy of the true distribution and the Kullback-Leibler divergence between the true distribution and the model distribution.\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(p) + \\text{KL}(p, q)\n",
    "$$\n",
    "\n",
    "In the above expression, H(p) is the entropy of the data-generating distribution, and KL(p, q) is the Kullback-Leibler divergence between the data-generating distribution and the model distribution. The KL divergence is always non-negative, and it is zero if the two distributions are identical. Therefore, the cross-entropy is always greater than or equal to the entropy of the data-generating distribution.\n",
    "\n",
    "$$\n",
    "\\text{KL} = \\sum_{k} p_k (\\log p_k - \\log q_k) = \\sum_{i} p_k \\log \\frac{p_k}{q_k}\n",
    "$$\n",
    "\n",
    "The KL-divergence describes how different P and Q are on average (in units of entropy). You have likely encountered a scaled version of it when studying generalized linear models (GLM) under the name of *deviance*. The deviance is the KL-divergence between the data-generating distribution and the model distribution, scaled by a factor of two.  \n",
    "\n",
    "In gradient descent, we want to minimize the cross-entropy between the true distribution (the labels) and the model distribution (the predicted probabilities). The loss function for multiclass logistic regression is the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\text{CE}(w) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log \\hat{y}_{ik}\n",
    "$$\n",
    "\n",
    "where $y_{ik}$ is an indicator function that is 1 if observation $i$ belongs to class $k$ and 0 otherwise, and $\\hat{y}_{ik}$ is the predicted probability that observation $i$ belongs to class $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: [0.1 0.9]\n",
      "Entropy: 0.3250829733914482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.9])\n",
    "print(\"p:\", p)\n",
    "print(\"Entropy:\", -np.sum(p * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.5 0.5]\n",
      "Entropy: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "q = np.array([1/2, 1/2])\n",
    "print(\"q:\", q)\n",
    "print(\"Entropy:\", -np.sum(q * np.log(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [0.33333333 0.33333333 0.33333333]\n",
      "Entropy: 1.0986122886681096\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([1/3, 1/3, 1/3])\n",
    "print(\"q:\", q1)\n",
    "print(\"Entropy:\", -np.sum(q1 * np.log(q1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p: 0.639031859650177\n",
      "Entropy of q: 1.0986122886681096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A small illustration of the cross-entropy loss\n",
    "# Let p be a probability distribution over 3 classes\n",
    "\n",
    "p = np.array([0.1, 0.1, 0.8])\n",
    "\n",
    "# The entropy of p is given by\n",
    "\n",
    "entropy_p = -np.sum(p * np.log(p))\n",
    "print(f\"Entropy of p: {entropy_p}\")\n",
    "\n",
    "# Values from the distribution of p would be easier to predict than values from the following distribution q\n",
    "\n",
    "q = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Its entropy should be higher, reflecting the uncertainty in the distribution.\n",
    "\n",
    "entropy_q = -np.sum(q * np.log(q))\n",
    "print(f\"Entropy of q: {entropy_q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [0.1 0.1 0.8]\n",
      "Q:  [0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \", p)\n",
    "print(\"Q: \", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy of q using q: 1.0986122886681096\n",
      "Cross-entropy of q using p: 1.6094379124341\n",
      "KL divergence between q and p: 0.5108256237659906\n"
     ]
    }
   ],
   "source": [
    "# When we try to predict values from q using q, the cross-entropy is equal to the entropy of q\n",
    "\n",
    "cross_entropy_q_q = -np.sum(q * np.log(q))\n",
    "\n",
    "print(f\"Cross-entropy of q using q: {cross_entropy_q_q}\")\n",
    "\n",
    "# If we use p to predict values from q, the cross-entropy is higher, as we will be wrong more often:\n",
    "\n",
    "cross_entropy_p_q = -np.sum(q * np.log(p))\n",
    "\n",
    "print(f\"Cross-entropy of q using p: {cross_entropy_p_q}\")\n",
    "\n",
    "# The KL divergence between q and p measures the extra (above the inherent prediction difficulty)\n",
    "# difficulty in predicting values from q using p, compared to using q\n",
    "\n",
    "kl_divergence_q_p = -np.sum(q * np.log(p/q))\n",
    "\n",
    "print(f\"KL divergence between q and p: {kl_divergence_q_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass (Prediction)\n",
    "\n",
    "As the networks become more complex, it is convenient to represent it in matrix notation and use vectorized operations. To understand it, we must pay special attention to the layout of the data and the shapes of the matrices.\n",
    "\n",
    "The multi-class logistic regresion model is:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{softmax}(W^T X)\n",
    "$$\n",
    "\n",
    "The shapes of the matrices are:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{P \\times N}$, where $N$ is the number of observations in the data (or the batch) and $P$ is the number of input features.\n",
    "- $W \\in \\mathbb{R}^{N \\times K}$, where $K$ is the number of classes.\n",
    "- $\\hat{Y} \\in \\mathbb{R}^{K \\times N}$, where each column of $\\hat{Y}$ contains the predicted probabilities for each class of each observation.\n",
    "\n",
    "\n",
    "The softmax function is applied to each column of the $K \\times N$ matrix $W^T X$ to obtain the predicted probabilities for each class of each observation. The cross-entropy loss is then computed by comparing the predicted probabilities with the true labels.\n",
    "\n",
    "Let's create a small example to illustrate the forward pass and the back-propagation step for a batch of N = 2 observations, each with P = 4 features and K = 3 classes.\n",
    "\n",
    "The input matrix $X \\in \\mathbb{R}^{4 \\times 2}$ is:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The weight matrix $W \\in \\mathbb{R}^{4 \\times 3}$ is:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "1.0 & 1.1 & 1.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The true labels $y \\in \\mathbb{R}^{3 \\times 2}$ are:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The true labels matrix here shows that the first observation belongs to class 0, the second observation belongs to class 1.\n",
    "\n",
    "The forward pass is:\n",
    "\n",
    "$$\n",
    "Z = W^T X = \\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.7 & 1.0 \\\\\n",
    "0.2 & 0.5 & 0.8 & 1.1 \\\\\n",
    "0.3 & 0.6 & 0.9 & 1.2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "11.8 & 14.0 \\\\\n",
    "13.4 & 16.0 \\\\\n",
    "15 & 18\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The softmax function is applied to each column of $Z$ to obtain the predicted probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{softmax}(Z) = \\begin{bmatrix}\n",
    "0.03280241 & 0.01587624 \\\\\n",
    "0.16247141 & 0.11731043 \\\\\n",
    "0.80472617 & 0.86681333\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that the predicted probabilities sum to 1 for each observation (column).\n",
    "\n",
    "The matrix of true labels must have K rows, where K is the number of classes (3 in this case), and N columns, where N is the number of observations (2 in this case). Let the first observation belong to class 0, the second observation to class 2. The matrix of true labels is:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The logarithm of the predicted probabilities is computed element-wise:\n",
    "\n",
    "$$\n",
    "\\log \\hat{Y} = \\begin{bmatrix}\n",
    "-3.4187718 & -4.14313473 \\\\\n",
    "-1.8187718 & -2.14113473 \\\\\n",
    "-0.2187718 & -0.14313473\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's verify these calculations with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is (4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6],\n",
       "       [7, 8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data \n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "print(\"X is\", X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.2, 0.3],\n",
       "       [0.4, 0.5, 0.6],\n",
       "       [0.7, 0.8, 0.9],\n",
       "       [1. , 1.1, 1.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.8, 14. ],\n",
       "       [13.4, 16. ],\n",
       "       [15. , 18. ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  133252.35294553,  1202604.28416478],\n",
       "       [  660003.22476616,  8886110.52050787],\n",
       "       [ 3269017.37247211, 65659969.13733051]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(W.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03280241, 0.01587624],\n",
       "       [0.16247141, 0.11731043],\n",
       "       [0.80472617, 0.86681333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = np.exp(W.T @ X) / np.sum(np.exp(W.T @ X), axis=0, keepdims=True)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicator matrix for the true classes\n",
    "\n",
    "Y = np.array([[1, 0, 0], [0, 0, 1]]).T\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.41725321, -4.14293163],\n",
       "       [-1.81725321, -2.14293163],\n",
       "       [-0.21725321, -0.14293163]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.41725321, -0.        ],\n",
       "       [-0.        , -0.        ],\n",
       "       [-0.        , -0.14293163]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y * np.log(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.560184843372646"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- np.sum(Y * np.log(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.780092421686323"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The average cross-entropy loss for the batch is\n",
    "\n",
    "- np.sum(Y * np.log(Y_hat)).sum() / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass (Gradient Descent)\n",
    "\n",
    "Now that we have computed the loss for the batch in the forward pass, we can compute the gradients of the loss with respect to the weights. Here it is convenient to use matrix notation and vectorized operations.\n",
    "\n",
    "For a single observation $i$, the predicted probabilities are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z & = W^T x \\\\\n",
    "\\hat{y} & = \\text{softmax}(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The cross-entropy loss is is a scalar value that depends on the weights $W$. The gradient of the loss with respect to the weights is a matrix of the same shape as $W$.\n",
    "\n",
    "$$\n",
    "\\text{CE}(W) = - \\sum_{k=1}^K y_{k} \\log \\hat{Y}_{k}\n",
    "$$\n",
    "\n",
    "The chain rule tells us that the gradient of the loss with respect to the weights is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial W} = \\frac{\\partial \\text{CE}(W)}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "The derivative of the cross-entropy loss with respect to $z$ is actually quite simple. As we are differentiation a mapping from $\\mathbb{R}^{K \\times 1}$ to $\\mathbb{R}$, the derivative is a matrix of the same shape as $z$. It is easy to derive this derivative with respect to a single element of $z$: $z_{k}$. The first thing to notice is that the loss depends on $z_{k}$ through $\\hat{y}_{k}$, and $\\hat{y}_{k}$ depends on $z_{1}$, $Z_{2}$, ..., $Z_{K}$ because the softmax function divides each element of the column by the _sum of all elements_ of the column. The chain rule tells us that the derivative of the loss with respect to $z_{k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial Z_{k'}} = \\sum_{k=1}^K \\frac{y_{k}}{\\hat{y}_{k}}\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k}}\n",
    "$$\n",
    "\n",
    "Note that we are using $k'$ as the index of the class in the derivative to avoid confusion with the summation index $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets tackle the derivative of the softmax function.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{k} = \\frac{e^{Z_{k}}}{\\sum_{j=1}^K e^{z_{j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial Z_{k'i}} \\hat{Y}_{ik} & = \\frac{\\partial}{\\partial Z_{k'i}} \\frac{e^{Z_{ik}}}{\\sum_{j=1}^K e^{Z_{ij}}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here we have two cases to consider. If $k = k'$, then the derivative is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial z_{k'}} \\hat{y}_{k} & = \\frac{\\partial}{\\partial z_{k'}} \\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\\\\n",
    "& = \\frac{e^{z_{k}}\\sum_{j=1}^K e^{z_{j}} - e^{z_{k}}e^{z_{k}}}{\\left(\\sum_{j=1}^K e^{z_{j}}\\right)^2} \\\\\n",
    "& = \\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\left(1 - \\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}}\\right) \\\\\n",
    "& = \\hat{y}_{k} (1 - \\hat{y}_{k})\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case is when $k \\neq k'$. In this case, the derivative is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial z_{k'}} \\hat{y}_{k} & = \\frac{\\partial}{\\partial z_{k'}} \\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\\\\n",
    "& = \\frac{0 - e^{z_{k}}e^{z_{k'}}}{\\left(\\sum_{j=1}^K e^{z_{j}}\\right)^2} \\\\\n",
    "& = -\\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\frac{e^{z_{k'}}}{\\sum_{j=1}^K e^{z_{j}}} \\\\\n",
    "& = -\\hat{y}_{k} \\hat{y}_{k'}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both cases into a single expression using the Kronecker delta $\\delta_{kk'}$ which is 1 if $k = k'$ and 0 otherwise:\n",
    "\n",
    "$$\n",
    "\\delta_{kk'} = \\begin{cases}\n",
    "1 & \\text{if } k = k' \\\\\n",
    "0 & \\text{if } k \\neq k'\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial z_{k'}} \\hat{y}_{k} & = \\hat{y}_{k} (\\delta_{kk'} - \\hat{y}_{k'})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You can check that this expression is correct by verifying that it gives the correct results for the two cases we considered above.\n",
    "\n",
    "Now we are ready to substitute this derivative into the expression for the derivative of the loss with respect to $z_{ki}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial \\hat{y}_{k'}} & = - \\sum_{k=1}^K \\frac{y_{k}}{\\hat{y}_{k}}\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k'}} \\\\\n",
    "& = - \\sum_{k=1}^K \\frac{y_{k}}{\\cancel{\\hat{y}_{k}}} \\cancel{\\hat{y}_{k}} (\\delta_{kk'} - \\hat{y}_{k'}) \\\\\n",
    "& = - \\sum_{k = 1}^{K} y_{ki} (\\delta_{kk'} - \\hat{y}_{k'})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The inner sum simplifies beautifully because of the special structure of $\\delta_{kk'}$ and $y_{ki}$. The inner sum is \n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{k} (\\delta_{kk'} - \\hat{y}_{k'}) = \\sum_{k = 1}^{K} y_{k} \\delta_{kk'} - \\sum_{k = 1}^{K} y_{k} \\hat{y}_{k'}\n",
    "$$\n",
    "\n",
    "Now you need to consider only two things. In the first sum we are multiplying $y_{k}$ by $\\delta_{kk'}$. The Kronecker delta is 1 only when $k = k'$, so the sum is only over the terms where $k = k'$.\n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{k} \\delta_{kk'} = y_{k'}\n",
    "$$\n",
    "\n",
    "For the second sum, notice that the $\\hat{y}_{k'}$ does not depend on the summation index $k$. Therefore, we can take it out of the sum.\n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{ki} \\hat{y}_{k'} = \\hat{y}_{k'} \\sum_{k = 1}^{K} y_{k} = \\hat{y}_{k'}\n",
    "$$\n",
    "\n",
    "The last equality is true because the sum is over the elements of the $i$-th row of $Y$, which is a one-hot encoded vector showing the true class of the $i$-th observation. Therefore, its sum over all elements is 1.\n",
    "\n",
    "In the end, the derivative of the loss with respect to the predicted probabilities is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial z_{k'}} = (-y_{k'} + \\hat{y}_{k'}) = \\hat{y}_{k'} - y_{k'}\n",
    "$$\n",
    "\n",
    "So the $ki$-th element of the gradient of the loss with respect to $z$ is the difference between the predicted probability and the true label. We can write this as a matrix operation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "What remains now is to compute the derivative of $z = W^T x$ with respect to $W$. Here it is helpful to consider the derivative with respect to a single weight $W_{ij}$ and consider a small example.\n",
    "\n",
    "Let $W$ be a $3 \\times 4$ matrix:\n",
    "\n",
    "$$\n",
    "z = W^T x = \\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "w_{13} & w_{23} & w_{33} \\\\\n",
    "w_{14} & w_{24} & w_{34}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "w_{11}x_1 + w_{21}x_2 + w_{31}x_3 \\\\\n",
    "w_{12}x_1 + w_{22}x_2 + w_{32}x_3 \\\\\n",
    "w_{13}x_1 + w_{23}x_2 + w_{33}x_3 \\\\\n",
    "w_{14}x_1 + w_{24}x_2 + w_{34}x_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The derivative of $z_{k}$ with respect to $W_{ij}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{k}}{\\partial W_{ij}} = \\begin{cases}\n",
    "x_{j} & \\text{if } i = k \\\\\n",
    "0 & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So the derivative of the whole vector $z$ with respect to a single weight, say $W_{1j}$ is again a vector of the same shape as $z$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial W_{1j}} = \\begin{bmatrix}\n",
    "x_{j} \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial z}{\\partial W_{2j}} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "x_{j} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial z}{\\partial W_{3j}} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "x_{j} \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The last result implies that when we take the derivative of the loss with respect to the weights, we will get a matrix of the same shape as $W$\n",
    "with the $ij$-th element being the product of the $i$-th row of the derivative of the loss with respect to $z$ and the $j$-th element of the input vector $x$ which is the outer product of the prediction error and the input vector.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial W} = (\\hat{y} - y) x^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6],\n",
       "       [ 8, 10, 12],\n",
       "       [12, 15, 18]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "np.outer(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
