{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicast Logistic Regression\n",
    "\n",
    "The binary classification logistic regression model is a special case of the multicast logistic regression model where the target variable can take more than two values.\n",
    "\n",
    "For a K-class response variable $y_i$ the multinomial regression model is defined as:\n",
    "\n",
    "$$\n",
    "y_i \\sim \\text{Multinomial}(1, p_{i1}, p_{i2}, \\ldots, p_{iK}) \\\\\n",
    "\\sum_{k = 1}^K p_{ik} = 1 \\\\\n",
    "p_{ik} = \\frac{\\exp(b_k + w_{k1} x_{i1} + \\ldots + w_{kp} x_{ip})}{\\sum_{j = 1}^K \\exp(b_j + w_{j1} x_{i1} + \\ldots + w_{jp} x_{ip})}, k = 1, \\ldots, K\n",
    "$$\n",
    "\n",
    "where the softmax function maps the linear predictor to [0, 1] and ensures that the sum of the probabilities is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "n = 2000\n",
    "S = 5\n",
    "K = 3\n",
    "\n",
    "# generate data\n",
    "X = np.random.randn(n, S)\n",
    "beta = np.random.randn(S, K)\n",
    "\n",
    "linpred = np.dot(X, beta)\n",
    "prob = np.exp(linpred)\n",
    "prob /= prob.sum(axis=1)[:, None]\n",
    "\n",
    "y = np.apply_along_axis(lambda x: np.random.multinomial(1, x).argmax(), axis=1, arr=prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34807397,  0.51886255,  0.5165155 ,  1.24904615, -0.6683651 ],\n",
       "       [ 1.28511998, -1.29529563,  0.45909643,  1.64650635, -1.44853151],\n",
       "       [ 0.33237059,  0.11223558,  1.02274435, -1.52986962,  0.11771475]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.47000056e-01,  7.32984167e-01, -1.35208192e-01,\n",
       "         8.18585767e-01, -1.61549481e-03],\n",
       "       [ 2.83915372e-01, -1.19124879e+00, -1.87744273e-01,\n",
       "         1.23329694e+00, -8.36730481e-01],\n",
       "       [-7.30915428e-01,  4.58264622e-01,  3.22952465e-01,\n",
       "        -2.05188271e+00,  8.38345975e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty=None)\n",
    "logreg.fit(X, y)\n",
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#%% md\n",
    "The multinomial logistic regression model can be represented as a neural network with a single layer of neurons where each neuron corresponds to a class. The input layer has $p$ neurons, one for each feature, and the output layer has $K$ neurons, one for each class. The weights of the model are represented by the edges connecting the input layer to the output layer (the bias terms are not shown in the diagram). The output of each neuron in the output layer is passed through the softmax function to obtain the predicted probabilities.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "%%| label: fig-single-neuron-multiclass\n",
    "%%| fig-width: 6\n",
    "%%| fig-cap: \"ANN model for logistic regression for a single observation\"\n",
    "\n",
    "graph LR\n",
    "    x1[\"$$x_{i1}$$\"] -->|$$w_1$$| B1((\"$$w_{1}^T x_i + b_1$$\"))\n",
    "    x2[\"$$x_{i2}$$\"] -->|$$w_2$$| B2((\"$$w_{2}^T x_i + b_2$$\"))\n",
    "    xp[\"$$x_{ip}$$\"] -->|$$w_p$$| B3((\"$$w_{p}^T x_i + b_p$$\"))\n",
    "    x1 --> B2\n",
    "    x1 --> B3\n",
    "    x2 --> B1\n",
    "    x2 --> B3\n",
    "    xp --> B1\n",
    "    xp --> B2\n",
    "    B1 --> P1[\"$$\\hat{y}$$\"]\n",
    "    B2 --> P2[\"$$\\hat{y}$$\"]\n",
    "    B3 --> P3[\"$$\\hat{y}$$\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross-Entropy\n",
    "\n",
    "In evaluating a model's accuracy, we need a measure between our model's prediction and a perfect (out-of-sample) prediction. This measure should be able to account for the fact that some outcomes (targets) are easier to predict than others. Consider the task of predicting the weather (sunshine/rain) in a deser, where it almost never rains. A model that always predicts sunshine will be correct most of the time, but it is not a very useful model as you will always be surprised when it rains.\n",
    "\n",
    "The *entropy* of a distribution is a measure of its uncertainty that has four properties\n",
    "\n",
    "- It is zero if the distribution is degenerate (i.e. the outcome is always sunshine)\n",
    "- It is continuous, so a small change in the distribution will result in a small change in the entropy\n",
    "- It is higher for distributions with can produce more different outcomes than for distributions that can produce fewer outcomes\n",
    "- It is additive, so the entropy of a distribution is the sum of the entropies of its components. This means that if we first measure the uncertainty about being male/female and then measure the uncertainty about being a soccer fan or not, the uncertainty of the combinations (male/soccer fan, male/not soccer fan, female/soccer-fan, female/not soccer fan) should the sum of the two uncertainties.\n",
    "\n",
    "It is easy to show that the entropy defined as the expected value of the log-probabilities of the outcomes satisfies these four properties.\n",
    "\n",
    "$$\n",
    "H(p) = \\sum_{k} p_k \\log p_k\n",
    "$$\n",
    "\n",
    "So the entropy gives us the uncertainty when predicting outcomes using the true distribution. In classification problems, however, we don't know this distribution. Instead, we rely on a model to produce probabilities that we hope are close to the true probabilities. We can ask: how much does the uncertainty increase if we use the wrong (the model's) probabilities (Q) instead of the true probabilities? This is the *cross-entropy*.\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(p) + \\text{KL}(p, q)\n",
    "$$\n",
    "\n",
    "In the above expression, H(p) is the entropy of the data-generating distribution, and KL(p, q) is the Kullback-Leibler divergence between the data-generating distribution and the model distribution. The KL divergence is always non-negative, and it is zero if the two distributions are identical. Therefore, the cross-entropy is always greater than or equal to the entropy of the data-generating distribution.\n",
    "\n",
    "$$\n",
    "\\text{KL} = \\sum_{k} p_k (\\log p_k - \\log q_k) = \\sum_{i} p_k \\log \\frac{p_k}{q_k}\n",
    "$$\n",
    "\n",
    "The KL-divergence describes how different P and Q are on average (in units of entropy). You have likely encountered a scaled version of it when studying generalized linear models (GLM) under the name of *deviance*. The deviance is the KL-divergence between the data-generating distribution and the model distribution, scaled by a factor of two.  \n",
    "\n",
    "\n",
    "The loss function for the multinomial logistic regression model is the cross-entropy loss function. The cross-entropy loss function (for a single observation) is defined as:\n",
    "\n",
    "$$\n",
    "\\text{CE}(y, \\hat{y}) = -\\sum_{k = 1}^K y_k \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "where $y$ is the one-hot encoded target variable and $\\hat{y}$ is the predicted probability vector. The cross-entropy loss function is minimized by adjusting the weights of the model using gradient descent.\n",
    "\n",
    "The input layer is mapped to the output layer using the following equations:\n",
    "\n",
    "$$\n",
    "z_{ik} = b_k + w_{k1} x_{i1} + \\ldots + w_{kS} x_{iS} \\\\\n",
    "p_{ik} = \\frac{\\exp(z_{ik})}{\\sum_{j = 1}^K \\exp(z_{ij})}, k = 1, \\ldots, K\n",
    "$$\n",
    "\n",
    "In matrix notation\n",
    "\n",
    "$$\n",
    "Z = X_{1 \\times S} W^T_{S \\times K} + B_{K \\times 1} \\\\\n",
    "P_{K \\times S} = \\text{softmax}(Z)\n",
    "$$\n",
    "\n",
    "where $X$ is the input matrix, $W$ is the weight matrix, $B$ is the bias matrix, $Z$ is the linear predictor matrix, and $P$ is the predicted probability matrix.\n",
    "\n",
    "The gradient for a weight $w_{jk}$ single observation is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{jk}} = x_{ij} (\\hat{y}_k - y_k)\n",
    "$$\n",
    "\n",
    "Therefore, the gradient for the whole $S \\times K$ weight matrix is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial W} = X_{i} \\otimes (\\hat{y}_{1 \\times K} - y_{1 \\times K})\n",
    "$$\n",
    "\n",
    "\n",
    "As the models grow more complex, it is important to keep track of the dimensions of the matrices and vectors to ensure that the operations are performed correctly.\n",
    "\n",
    "So let's break down the operations for a single observation with S=3 features (and we will drop the observation index $i$ for simplicity) and two classes $K = 2$.\n",
    "\n",
    "The input is a vector $x_{1 \\times S}$ (XXX, resolve the notation conflict with the matrix $X$):\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The weight matrix is $W_{S \\times K}$ and must map the input to the output layer:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ignoring the bias term, the linear predictor matrix $Z_{1 \\times K}$ is given by:\n",
    "\n",
    "$$\n",
    "Z = W^T X = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "w_{11} x_1 + w_{21} x_2 + w_{31} x_3 \\\\\n",
    "w_{12} x_1 + w_{22} x_2 + w_{32} x_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "z_{11} \\\\\n",
    "z_{12}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The predictor matrix is passed through the softmax function to obtain the predicted probability matrix $P_{2 \\times 1}$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(Z) = \\begin{bmatrix}\n",
    "\\frac{\\exp(z_{11})}{\\exp(z_{11}) + \\exp(z_{12})} \\\\\n",
    "\\frac{\\exp(z_{12})}{\\exp(z_{11}) + \\exp(z_{12})}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_{1} \\\\\n",
    "\\hat{y}_{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The labels are one-hot encoded, so the target vector $y_{1 \\times K}$ is:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 & y_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The cross-entropy loss function is:\n",
    "\n",
    "$$\n",
    "\\text{CE}(y, \\hat{y}) = -\\sum_{k = 1}^K y_k \\log(\\hat{y}_k) = -y_1 \\log(\\hat{y}_1) - y_2 \\log(\\hat{y}_2)\n",
    "$$\n",
    "\n",
    "\n",
    "Now let's look into the derivatives of the loss function with respect to the weights. The weight matrix is $2 \\times 3$, \n",
    "therefore the gradient matrix is also $2 \\times 3$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial W} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{11}} & \\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{12}} & \\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{13}} \\\\\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{21}} & \\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{22}} & \\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{23}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's derive the gradient for the first weight $w_{11}$. Using the chain rule for differentiation we obtain:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{11}} = \\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial \\hat{y}_1} \\frac{\\partial \\hat{y}_1}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial w_{11}}\n",
    "$$\n",
    "\n",
    "The first term is the derivative of the cross-entropy loss function with respect to the predicted probability $\\hat{y}_1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial \\hat{y}_1} = - \\frac{\\partial}{\\partial \\hat{y}_1} \\left( y_1 \\log(\\hat{y}_1) + y_2 \\log(\\hat{y}_2) \\right) = - \\frac{y_1}{\\hat{y}_1}\n",
    "$$\n",
    "\n",
    "The second term is the derivative of the softmax function with respect to the linear predictor $z_{11}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_1}{\\partial z_{11}} = \\frac{\\partial}{\\partial z_{11}} \\left( \\frac{\\exp(z_{11})}{\\exp(z_{11}) + \\exp(z_{12})} \\right) = \\hat{y}_1 (1 - \\hat{y}_1)\n",
    "$$\n",
    "\n",
    "The last term is the simplest one, as $z_{11}$ is a linear function of $w_{11}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{11}}{\\partial w_{11}} = \\frac{\\partial}{\\partial w_{11}} \\left( w_{11} x_1 + w_{21} x_2 + w_{31} x_3 \\right) = x_1\n",
    "$$\n",
    "\n",
    "Therefore, the gradient for the weight $w_{11}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{11}} = - \\frac{y_1}{\\hat{y}_1} \\hat{y}_1 (1 - \\hat{y}_1) x_1 = - y_1 (1 - \\hat{y}_1) x_1\n",
    "$$\n",
    "\n",
    "It is important to note that this derivative is non-zero only for the class $k$ that the observation belongs to. For the other classes, the derivative is zero. This means that the weight update will only affect the weights of the class that the observation belongs to, i.e. the column of the weight matrix corresponding to the class (XXX).\n",
    "\n",
    "\n",
    "The derivative for the weight $w_{21}$ is similar to the previous one. First we find the derivative of the cross-entropy loss function with respect to the predicted probability $\\hat{y}_2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial \\hat{y}_2} = - \\frac{y_2}{\\hat{y}_2}\n",
    "$$\n",
    "\n",
    "The derivative of the softmax function with respect to the linear predictor $z_{12}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_2}{\\partial z_{12}} = \\frac{\\partial}{\\partial z_{12}} \\left( \\frac{\\exp(z_{12})}{\\exp(z_{11}) + \\exp(z_{12})} \\right) = - \\hat{y}_1 \\hat{y}_2\n",
    "$$\n",
    "\n",
    "The derivative of the linear predictor $z_{12}$ with respect to the weight $w_{21}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{12}}{\\partial w_{21}} = x_2\n",
    "$$\n",
    "\n",
    "Therefore, the gradient for the weight $w_{21}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial w_{21}} = - \\frac{y_2}{\\hat{y}_2} \\hat{y}_1 \\hat{y}_2 x_2 = - y_2 \\hat{y}_1 x_2\n",
    "$$\n",
    "\n",
    "\n",
    "We can generalize the gradient for the whole weight matrix $W$ as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(y, \\hat{y})}{\\partial W} = X^T (\\hat{y} - y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "W = np.random.randn(S, K)\n",
    "\n",
    "epochsn = 1000\n",
    "lr = 0.001\n",
    "\n",
    "losses = np.zeros(epochsn)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(keepdims=True)\n",
    "\n",
    "for epoch in range(epochsn):\n",
    "    for i in range(n):\n",
    "        x = X[i]\n",
    "        y_ = y[i]\n",
    "        \n",
    "        linpred = np.dot(x, W)\n",
    "        prob = softmax(linpred)\n",
    "        \n",
    "        loss = -np.log(prob[y_])\n",
    "        losses[epoch] += loss\n",
    "        \n",
    "        grad = prob.copy()\n",
    "        grad[y_] -= 1\n",
    "        \n",
    "        W -= lr * np.outer(x, grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
