{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Matrix Differentiation Rules\n",
    "\n",
    "It is often convenient to express the derivatives in matrix form and use vectorized operations when updating the weights in neural networks. The reason for this is that vectorized operations in `numpy` for example are much faster than using Python loops. In this notebook, we will summarize a couple of useful rules for matrix differentiation.\n",
    "\n",
    ":::{#def-m-from-vec-n}\n",
    "Let $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ be a function that maps a m-dimensional vector to an n-dimensional vector. Then the derivative of $f$ with respect to a vector $x$ is a matrix (called the Jacobian matrix of $f$) of shape $m \\times n$ and is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exm-1-to-2}\n",
    "## $f: \\mathbb{R}^{1} \\to \\mathbb{R}^{2}$\n",
    "As an example, let's look at a function $f: \\mathbb{R}^{1} \\rightarrow \\mathbb{R}^{2}$ that maps a scalar to a 2-dimensional vector. The function is defined as\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{pmatrix} x^2 \\\\ x^3 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "According to @def-m-from-vec-n the Jacobian matrix of $f$ is a $2 \\times 1$ matrix and is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} \\\\ \\frac{\\partial f_2}{\\partial x}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "2x \\\\ 3x^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{#def-matrix-to-scalar}\n",
    "\n",
    "Let $f: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}$ be a function that maps a matrix to a scalar (e.g. a loss function). Then the derivative of $f$ with respect to a matrix $W$ is a matrix of the same shape as $W$ and is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial W} = \\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial W_{11}} & \\cdots & \\frac{\\partial f}{\\partial W_{1n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial W_{m1}} & \\cdots & \\frac{\\partial f}{\\partial W_{mn}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#thm-matrix-vector-by-vector}\n",
    "## Matrix Multiplication by Vector\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^{n}$. Then the derivative of $Ax$ with respect to $x$ is $A^T$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Ax}{\\partial x} = A^T\n",
    "$$\n",
    ":::\n",
    ":::{.proof}\n",
    "\n",
    "Let $y = Ax$. Then $y_i = \\sum_{j=1}^{n} A_{ij}x_j$. The derivative of $y_i$ with respect to $x_k$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_k} = A_{ik}\n",
    "$$\n",
    "\n",
    "Therefore, the derivative of $Ax$ with respect to $x$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Ax}{\\partial x} = \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix} = A^T\n",
    "$$\n",
    ":::\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
