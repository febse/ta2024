{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Data in Python\n",
    "\n",
    "## Exercise 1:\n",
    "  \n",
    "  1. Read the `hillary_tweets.txt` file that is located in the \"data\" directory of the workshop repository. Alternatively, you can read the url directly https://raw.githubusercontent.com/boyko/text-analytics-script/main/data/hillary_tweets.txt\n",
    "  2. Split the string on the newline character.\n",
    "  3. Compute the frequency distribution of all words over all tweets using `nltk.FreqDist`.\n",
    "  4. Compute the frequency distribution of all words in the first tweet using `nltk.FreqDist`.\n",
    "  5. Normalise each tweet by:\n",
    "    - compressing whitespace and removing leading and trailing whitespace\n",
    "    - removing the punctuation using regular expressions\n",
    "    - tokenize the tweets using the `nltk.word_tokenize` function.\n",
    "    - remove the stopwords\n",
    "    - applying the Porter stemming algorithm\n",
    "  6. Create a pandas dataframe with one column containing the tweets\n",
    "  7. Summarise each tweet by:\n",
    "    - Counting the number of characters in each tweet\n",
    "    - Counting the number of sentences in each tweet\n",
    "    - Counting the number of hashtags in each tweet\n",
    "    - Extracting the first mention (@) in each tweet in a column called `first_mention`.\n",
    "    - Extract the datetime of each tweet and store it in a column called `timestamp`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "## File path to the data\n",
    "tweets_path = os.path.join(\"..\", \"data\", \"hillary_tweets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "## 1) Read the data\n",
    "## For the relative path to work, make sure that the jupyter process run in the directory\n",
    "## of this file\n",
    "\n",
    "with open(tweets_path, 'r') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2) Number of tweets\n",
    "## The tweets are separated by newline characters, so we split on \"\\n\" on it to\n",
    "## obtain a list of tweets. Alternatively, you can obtain the same result\n",
    "## by using f.readlines() instead of f.read() in the previous step\n",
    "\n",
    "tweets = text.split(\"\\n\")\n",
    "len(tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[(',', 16), ('the', 10), ('@', 10)]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Frequency of words\n",
    "## We need to obtain the words in a single list in order\n",
    "## to invoke nltk.FreqDist.\n",
    "\n",
    "## For example, we can split the entire text on blanks\n",
    "tweet_words_blanks = text.split(\" \")\n",
    "freqs_by_blanks = nltk.FreqDist(tweet_words_blanks)\n",
    "freqs_by_blanks.most_common(3)\n",
    "\n",
    "## Or using the nltk words tokenizers\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    words.extend(nltk.word_tokenize(sent))\n",
    "\n",
    "freq = nltk.FreqDist(words)\n",
    "freq.most_common(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[(\"''\", 2), ('the', 2), ('&', 2)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Frequency distribution of words in the first tweet\n",
    "\n",
    "## To keep the code DRY, we can package the word-splitting in a function\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    txt_sents = nltk.sent_tokenize(txt)\n",
    "    txt_words = []\n",
    "    for s in txt_sents:\n",
    "        txt_words.extend(nltk.word_tokenize(s))\n",
    "\n",
    "    return txt_words\n",
    "\n",
    "words_tweet1 = tokenize_text(tweets[0])\n",
    "\n",
    "freq_tweet1 = nltk.FreqDist(words_tweet1)\n",
    "freq_tweet1.most_common(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['9.11.201610pm',\n \"''\",\n 'littl',\n 'girl',\n 'watch',\n '...',\n 'never',\n 'doubt',\n 'valuabl',\n 'power',\n 'deserv',\n 'everi',\n 'chanc',\n 'opportun',\n 'world',\n \"''\"]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text normalization\n",
    "\n",
    "## tokenize the tweets using the nltk.word_tokenize function.\n",
    "## remove the stopwords\n",
    "## applying the Porter stemming algorithm\n",
    "\n",
    "## We can wrap the sequence of transformations in a small function\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def normalize_text(txt):\n",
    "    ## compressing whitespace and removing leading and trailing whitespace\n",
    "    txt_next = txt.strip()\n",
    "    txt_next = re.sub(\"\\s+\", \" \", txt_next)\n",
    "\n",
    "    ## Remove stopwords\n",
    "    ## First we convert the string to lowercase, because the stopwords\n",
    "    ## in nltk are also in lowercase\n",
    "    txt_next = txt_next.lower()\n",
    "\n",
    "    ## Next, get the words and\n",
    "    txt_words = tokenize_text(txt_next)\n",
    "\n",
    "    ## Filter the stopwords\n",
    "    txt_words = [w for w in txt_words if w not in stop_words]\n",
    "    ## Remove the punctuation\n",
    "    txt_words = [w for w in txt_words if w not in string.punctuation]\n",
    "    ## Apply the stemmer\n",
    "    stems = [stemmer.stem(w) for w in txt_words]\n",
    "\n",
    "    return stems\n",
    "\n",
    "normalize_text(tweets[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               tweet      timestamp\n0  9.11.201610pm\"To all the little girls watching...  9.11.201610pm\n1  9.11.20169amWe're delighted to have spent the ...   9.11.20169am\n2  11.11.20164pmMeet the people who are suing the...  11.11.20164pm\n3  14.11.20163pmHillary Clinton at Wednesday's @d...  14.11.20163pm\n4  10.11.20168pmI'm so proud of @OnwardTogether p...  10.11.20168pm\n5                                                               NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.11.201610pm\"To all the little girls watching...</td>\n      <td>9.11.201610pm</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.11.20169amWe're delighted to have spent the ...</td>\n      <td>9.11.20169am</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11.11.20164pmMeet the people who are suing the...</td>\n      <td>11.11.20164pm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.11.20163pmHillary Clinton at Wednesday's @d...</td>\n      <td>14.11.20163pm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.11.20168pmI'm so proud of @OnwardTogether p...</td>\n      <td>10.11.20168pm</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td></td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas dataframe from the list of tweets\n",
    "dt = pd.DataFrame(tweets, columns = [\"tweet\"])\n",
    "\n",
    "## Count the number of characters\n",
    "dt[\"num_chars\"] = dt.tweet.str.len()\n",
    "\n",
    "## Count the number of sentences\n",
    "dt[\"num_sents\"] = dt.tweet.apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "\n",
    "## Number of hashtags\n",
    "dt[\"num_hashtags\"] = dt.tweet.str.count(\"#\")\n",
    "\n",
    "## First mention\n",
    "## The regular expression matches anyting starting with @ and followed by\n",
    "## any number of word-character (but at least one).\n",
    "dt[\"first_mention\"] = dt.tweet.str.extract(r\"@(\\w+)\")\n",
    "\n",
    "## Extract the datetime\n",
    "## The regexp to match the dates in this format\n",
    "## matches one or two numbers (day) followed by a dot\n",
    "## followed by one or two numbers (month) followed by a dot\n",
    "## followed by four numbers (year)\n",
    "## followed by one or two numbers (hour)\n",
    "## followed by two characters that can be \"a\", \"m\" or \"p\"\n",
    "dt[\"timestamp\"] = dt.tweet.str.extract(r\"(\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\d{1,2}[amp]{2})\")\n",
    "dt[[\"tweet\", \"timestamp\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n",
    "\n",
    "The following data frame contains 5 appointment records.\n",
    "\n",
    "1. Replace each weekday mentioned in the records with its 3 letter abbreviation.\n",
    "2. Extract the time of each appointment (keep the am/pm suffix). Use named capture groups and the\n",
    "    `str.extractall` method of the `text` column.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text\n0     Monday: The doctor's appointment is at 2:45pm.\n1  Tuesday: The dentist's appointment is at 11:30...\n2  Wednesday: At 7:00pm, there is a basketball game!\n3  Thursday: Be back home by 11:15 pm at the latest.\n4  Friday: Take the train at 08:10 am, arrive at ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Monday: The doctor's appointment is at 2:45pm.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tuesday: The dentist's appointment is at 11:30...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Thursday: Be back home by 11:15 pm at the latest.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\",\n",
    "                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n",
    "                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "        hour minute ampm\n  match                 \n0 0        2     45   pm\n1 0       11     30   am\n2 0        7     00   pm\n3 0       11     15   pm\n4 0       08     10   am\n  1       09     00   am",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>hour</th>\n      <th>minute</th>\n      <th>ampm</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>match</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>2</td>\n      <td>45</td>\n      <td>pm</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>11</td>\n      <td>30</td>\n      <td>am</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>0</th>\n      <td>7</td>\n      <td>00</td>\n      <td>pm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <th>0</th>\n      <td>11</td>\n      <td>15</td>\n      <td>pm</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">4</th>\n      <th>0</th>\n      <td>08</td>\n      <td>10</td>\n      <td>am</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09</td>\n      <td>00</td>\n      <td>am</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract the times\n",
    "\n",
    "df.text.str.extractall(r\"(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})\\s*(?P<ampm>[amp]{2})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amarov/anaconda3/envs/text-analytics-script/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": "0          Mon: The doctor's appointment is at 2:45pm.\n1       Tue: The dentist's appointment is at 11:30 am.\n2          Wed: At 7:00pm, there is a basketball game!\n3         Thu: Be back home by 11:15 pm at the latest.\n4    Fri: Take the train at 08:10 am, arrive at 09:...\nName: text, dtype: object"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Replace the weekdays (Monday, Tuesday, etc.) with their three letter abbreviation\n",
    "## This exercise demonstrates the use of functions when replacing text\n",
    "\n",
    "## The functions received a match as its argument\n",
    "def abbreviate_daynames(m):\n",
    "    matched_day =  m.groups()[0]\n",
    "    return matched_day[:3]\n",
    "\n",
    "## It is important to notice, that the whole function \"abbreviate_daynames\"\n",
    "## is passed to the replacement argument of \"replace\". The \"abbreviate_daynames\"\n",
    "## function is called when replace finds a match\n",
    "\n",
    "df.text.str.replace(\"(\\w+day)\", abbreviate_daynames)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
