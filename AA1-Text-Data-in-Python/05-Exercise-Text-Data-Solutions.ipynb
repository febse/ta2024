{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text Data in Python\n",
    "\n",
    "## Exercise 1:\n",
    "  \n",
    "  1. Read the `hillary_tweets.txt` file that is located in the \"data\" directory of the workshop repository. Alternatively, you can read the url directly https://raw.githubusercontent.com/boyko/text-analytics-script/main/data/hillary_tweets.txt\n",
    "  2. Split the string on the newline character.\n",
    "  3. Compute the frequency distribution of all words over all tweets using `nltk.FreqDist`.\n",
    "  4. Compute the frequency distribution of all words in the first tweet using `nltk.FreqDist`.\n",
    "  5. Normalise each tweet by:\n",
    "    - compressing whitespace and removing leading and trailing whitespace\n",
    "    - removing the punctuation using regular expressions\n",
    "    - tokenize the tweets using the `nltk.word_tokenize` function.\n",
    "    - remove the stopwords\n",
    "    - applying the Porter stemming algorithm\n",
    "  6. Create a pandas dataframe with one column containing the tweets\n",
    "  7. Summarise each tweet by:\n",
    "    - Counting the number of characters in each tweet\n",
    "    - Counting the number of sentences in each tweet\n",
    "    - Counting the number of hashtags in each tweet\n",
    "    - Extracting the first mention (@) in each tweet in a column called `first_mention`.\n",
    "    - Extract the datetime of each tweet and store it in a column called `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "## File path to the data\n",
    "tweets_path = os.path.join(\"..\", \"data\", \"hillary_tweets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 1) Read the data\n",
    "## For the relative path to work, make sure that the jupyter process run in the directory\n",
    "## of this file\n",
    "\n",
    "with open(tweets_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2) Number of tweets\n",
    "## The tweets are separated by newline characters, so we split on \"\\n\" on it to\n",
    "## obtain a list of tweets. Alternatively, you can obtain the same result\n",
    "## by using f.readlines() instead of f.read() in the previous step\n",
    "\n",
    "tweets = text.split(\"\\n\")\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 16), ('the', 10), ('@', 10)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Frequency of words\n",
    "## We need to obtain the words in a single list in order\n",
    "## to invoke nltk.FreqDist.\n",
    "\n",
    "## For example, we can split the entire text on blanks\n",
    "tweet_words_blanks = text.split(\" \")\n",
    "freqs_by_blanks = nltk.FreqDist(tweet_words_blanks)\n",
    "freqs_by_blanks.most_common(3)\n",
    "\n",
    "## Or using the nltk words tokenizers\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    words.extend(nltk.word_tokenize(sent))\n",
    "\n",
    "freq = nltk.FreqDist(words)\n",
    "freq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"''\", 2), ('the', 2), ('&', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Frequency distribution of words in the first tweet\n",
    "\n",
    "## To keep the code DRY, we can package the word-splitting in a function\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    txt_sents = nltk.sent_tokenize(txt)\n",
    "    txt_words = []\n",
    "    for s in txt_sents:\n",
    "        txt_words.extend(nltk.word_tokenize(s))\n",
    "\n",
    "    return txt_words\n",
    "\n",
    "words_tweet1 = tokenize_text(tweets[0])\n",
    "\n",
    "freq_tweet1 = nltk.FreqDist(words_tweet1)\n",
    "freq_tweet1.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.11.201610pm',\n",
       " \"''\",\n",
       " 'littl',\n",
       " 'girl',\n",
       " 'watch',\n",
       " '...',\n",
       " 'never',\n",
       " 'doubt',\n",
       " 'valuabl',\n",
       " 'power',\n",
       " 'deserv',\n",
       " 'everi',\n",
       " 'chanc',\n",
       " 'opportun',\n",
       " 'world',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text normalization\n",
    "\n",
    "## tokenize the tweets using the nltk.word_tokenize function.\n",
    "## remove the stopwords\n",
    "## applying the Porter stemming algorithm\n",
    "\n",
    "## We can wrap the sequence of transformations in a small function\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def normalize_text(txt):\n",
    "    ## compressing whitespace and removing leading and trailing whitespace\n",
    "    txt_next = txt.strip()\n",
    "    txt_next = re.sub(\"\\s+\", \" \", txt_next)\n",
    "\n",
    "    ## Remove stopwords\n",
    "    ## First we convert the string to lowercase, because the stopwords\n",
    "    ## in nltk are also in lowercase\n",
    "    txt_next = txt_next.lower()\n",
    "\n",
    "    ## Next, get the words and\n",
    "    txt_words = tokenize_text(txt_next)\n",
    "\n",
    "    ## Filter the stopwords\n",
    "    txt_words = [w for w in txt_words if w not in stop_words]\n",
    "    ## Remove the punctuation\n",
    "    txt_words = [w for w in txt_words if w not in string.punctuation]\n",
    "    ## Apply the stemmer\n",
    "    stems = [stemmer.stem(w) for w in txt_words]\n",
    "\n",
    "    return stems\n",
    "\n",
    "normalize_text(tweets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.11.201610pm\"To all the little girls watching...</td>\n",
       "      <td>9.11.201610pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.11.20169amWe're delighted to have spent the ...</td>\n",
       "      <td>9.11.20169am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.11.20164pmMeet the people who are suing the...</td>\n",
       "      <td>11.11.20164pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.11.20163pmHillary Clinton at Wednesday's @d...</td>\n",
       "      <td>14.11.20163pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.11.20168pmI'm so proud of @OnwardTogether p...</td>\n",
       "      <td>10.11.20168pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet      timestamp\n",
       "0  9.11.201610pm\"To all the little girls watching...  9.11.201610pm\n",
       "1  9.11.20169amWe're delighted to have spent the ...   9.11.20169am\n",
       "2  11.11.20164pmMeet the people who are suing the...  11.11.20164pm\n",
       "3  14.11.20163pmHillary Clinton at Wednesday's @d...  14.11.20163pm\n",
       "4  10.11.20168pmI'm so proud of @OnwardTogether p...  10.11.20168pm\n",
       "5                                                               NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas dataframe from the list of tweets\n",
    "dt = pd.DataFrame(tweets, columns = [\"tweet\"])\n",
    "\n",
    "## Count the number of characters\n",
    "dt[\"num_chars\"] = dt.tweet.str.len()\n",
    "\n",
    "## Count the number of sentences\n",
    "dt[\"num_sents\"] = dt.tweet.apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "\n",
    "## Number of hashtags\n",
    "dt[\"num_hashtags\"] = dt.tweet.str.count(\"#\")\n",
    "\n",
    "## First mention\n",
    "## The regular expression matches anyting starting with @ and followed by\n",
    "## any number of word-character (but at least one).\n",
    "dt[\"first_mention\"] = dt.tweet.str.extract(r\"@(\\w+)\")\n",
    "\n",
    "## Extract the datetime\n",
    "## The regexp to match the dates in this format\n",
    "## matches one or two numbers (day) followed by a dot\n",
    "## followed by one or two numbers (month) followed by a dot\n",
    "## followed by four numbers (year)\n",
    "## followed by one or two numbers (hour)\n",
    "## followed by two characters that can be \"a\", \"m\" or \"p\"\n",
    "dt[\"timestamp\"] = dt.tweet.str.extract(r\"(\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\d{1,2}[amp]{2})\")\n",
    "dt[[\"tweet\", \"timestamp\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "The following data frame contains 5 appointment records.\n",
    "\n",
    "1. Replace each weekday mentioned in the records with its 3 letter abbreviation.\n",
    "2. Extract the time of each appointment (keep the am/pm suffix). Use named capture groups and the\n",
    "    `str.extractall` method of the `text` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday: The doctor's appointment is at 2:45pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday: The dentist's appointment is at 11:30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday: Be back home by 11:15 pm at the latest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0     Monday: The doctor's appointment is at 2:45pm.\n",
       "1  Tuesday: The dentist's appointment is at 11:30...\n",
       "2  Wednesday: At 7:00pm, there is a basketball game!\n",
       "3  Thursday: Be back home by 11:15 pm at the latest.\n",
       "4  Friday: Take the train at 08:10 am, arrive at ..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\",\n",
    "                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n",
    "                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>ampm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hour minute ampm\n",
       "  match                 \n",
       "0 0        2     45   pm\n",
       "1 0       11     30   am\n",
       "2 0        7     00   pm\n",
       "3 0       11     15   pm\n",
       "4 0       08     10   am\n",
       "  1       09     00   am"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract the times\n",
    "\n",
    "df.text.str.extractall(r\"(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})\\s*(?P<ampm>[amp]{2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amarov/anaconda3/envs/text-analytics-script/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          Mon: The doctor's appointment is at 2:45pm.\n",
       "1       Tue: The dentist's appointment is at 11:30 am.\n",
       "2          Wed: At 7:00pm, there is a basketball game!\n",
       "3         Thu: Be back home by 11:15 pm at the latest.\n",
       "4    Fri: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Replace the weekdays (Monday, Tuesday, etc.) with their three letter abbreviation\n",
    "## This exercise demonstrates the use of functions when replacing text\n",
    "\n",
    "## The functions received a match as its argument\n",
    "def abbreviate_daynames(m):\n",
    "    matched_day =  m.groups()[0]\n",
    "    return matched_day[:3]\n",
    "\n",
    "## It is important to notice, that the whole function \"abbreviate_daynames\"\n",
    "## is passed to the replacement argument of \"replace\". The \"abbreviate_daynames\"\n",
    "## function is called when replace finds a match\n",
    "\n",
    "df.text.str.replace(\"(\\w+day)\", abbreviate_daynames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
