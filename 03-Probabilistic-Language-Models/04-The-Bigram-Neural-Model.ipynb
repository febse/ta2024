{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bi-gram Neural Model\n",
    "\n",
    "[Open in Colab](https://colab.research.google.com/github/febse/ta2024/blob/main/03-Probabilistic-Language-Models/04-The-Bigram-Neural-Model.ipynb)\n",
    "\n",
    "The logistic regression model that we employed to estimate the probability of a word given its preceding word produced a vector representation of each word in the vocabulary (the weights of the model). However, these vectors suffer from some drawbacks. First of all, the length of these vectors is equal to the size of the vocabulary, which can be very large. This makes the model computationally expensive. The number of parameters in the model is equal to the square of the size of the vocabulary.\n",
    "\n",
    "We can try to alleviate this problem by using a neural network model with one hidden layer where the dimension of the hidden layer is much smaller than the size of the vocabulary. For a vocabulary of 3 words and a hidden layer of size 2, the model would look like this:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A1[1] -->|wh11| H1[h1]\n",
    "    A1 -->|wh12| H2[h2]\n",
    "    A2[0] -->|wh21| H1 \n",
    "    A2 -->|wh22| H2\n",
    "    A3[0] -->|wh31| H1\n",
    "    A3 -->|wh32| H2\n",
    "    H1 -->|wo11| O1[z1]\n",
    "    H1 -->|wo21| O2[z2]\n",
    "    H1 -->|wo31| O3[z3]\n",
    "    H2 -->|wo12| O1\n",
    "    H2 -->|wo22| O2\n",
    "    H2 -->|wo32| O3\n",
    "    O1 --> SM\n",
    "    O2 --> SM\n",
    "    O3 --> SM\n",
    "    SM[Softmax]  --> P1[P1] --> Y1[0]\n",
    "    SM --> P2[P2] --> Y2[1]\n",
    "    SM --> P3[P3] --> Y3[0]\n",
    "```\n",
    "\n",
    "Notice that the number of parameters in this model is equal to the number of weights connecting the input layer to the hidden layer plus the number of weights connecting the hidden layer to the output layer. Here we have 6 weights connecting the input layer to the hidden layer and 6 weights connecting the hidden layer to the output layer, for a total of 12 weights. In this case this even appears to worsen the problem of the number of parameters as the logistic regression would have only $3 \\cdot 3 = 9$ parameters.\n",
    "\n",
    "However, with a vocabulary of 1000 words and a hidden layer of size 100, the number of parameters in the neural network model is $2 \\cdot 100000$, which is much smaller than the one million parameters in the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the model we must specify an activation function for the hidden layer. As an exercise, let us choose the `tanh` function. The `tanh` function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "## The Forward Pass\n",
    "\n",
    "For a single bi-gram with input $x$ and output $y$, the forward pass of the model is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a = W^{(1) T} \\cdot x \\\\\n",
    "h = \\text{tanh}(a) \\\\\n",
    "z = W^{(2) T} \\cdot h \\\\\n",
    "\\hat{y} = \\text{Softmax}(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $W^{(1)}$ is the matrix of weights connecting the input layer to the hidden layer, $W^{(2)}$ is the matrix of weights connecting the hidden layer to the output layer, and $\\hat{y}$ is the predicted probability distribution over the vocabulary.\n",
    "\n",
    "## The Backward Pass\n",
    "\n",
    "The loss function for the model is the cross-entropy loss function just as in the logistic regression model.\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "This time we have two sets of weights to update, $W^{(1)}$ and $W^{(2)}$. The gradients of the loss function with respect to the weights are given by:\n",
    "\n",
    "The gradient of the loss function with respect to the output layer weights is the same as in the logistic regression model, only this time the input to the softmax function is the output of the hidden layer instead of the input layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}} = (\\hat{y} - y) \\cdot h^T\n",
    "$$\n",
    "\n",
    "We can find the gradient of the loss function with respect to the hidden layer weights by applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} \\cdot \\frac{\\partial a}{\\partial W^{(1)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We already know the derivative of the cross-entropy loss function with respect to the output layer (z). This was a vector of size $V$ where $V$ is the size of the vocabulary giving the prediction errors.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{z}} = \\hat{y} - y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next derivative is the derivative of the output layer with respect to activation of the hidden layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{z}}{\\partial h} = W^{(2)}\n",
    "$$\n",
    "\n",
    "The next derivative is the derivative of the hidden layer with respect to its activation function. This is the derivative of the `tanh` function.\n",
    "\n",
    "Note that because we are taking the derivative of a vector with respect to itself, the derivative is a diagonal matrix of dimension $H \\times H$ where $H$ is the size of the hidden layer. See @exr-tanh-derivative for the derivative of the `tanh` function.\n",
    "\n",
    "$$\n",
    "h = \\text{tanh}(a) = \\begin{bmatrix} \\text{tanh}(a_1) \\\\ \\text{tanh}(a_2) \\\\ \\vdots \\\\ \\text{tanh}(a_H) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial a} = \\begin{bmatrix} 1 - \\text{tanh}^2(a_1) & 0 & \\cdots & 0 \\\\ 0 & 1 - \\text{tanh}^2(a_2) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 - \\text{tanh}^2(a_H) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying the derivatives up to the last term we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} & = (\\hat{y} - y) W^{(2)} \\cdot \\text{diag}(1 - \\text{tanh}^2(a))\\\\\n",
    "& = \\underbrace{(\\hat{y} - y) W^{(2)} \\odot (1 - h^2)}_{1 \\times H} \\\\\n",
    "\\end{align}\n",
    "$${#eq-bi-gram-hidden-weights-partial}\n",
    "\n",
    "For the last derivative we need to follow the same step as in the logistic regression model, so the full derivative is the outer product of @eq-bi-gram-hidden-weights-partial and the input vector.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial W^{(1)}}}_{V \\times H} = \\underbrace{x^T}_{V \\times 1} \\underbrace{(\\hat{y} - y) W^{(2)} \\odot (1 - h^2)}_{1 \\times H}\n",
    "$$\n",
    "\n",
    "\n",
    "For a matrix of input vectors $X$ and output vectors $Y$, the gradients of the loss function with respect to the weights are given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}} & = \\underbrace{H^T}_{H \\times n} \\underbrace{(\\hat{Y} - Y)}_{n \\times V} \\\\\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} & = \\underbrace{X^T}_{V \\times n} \\underbrace{(\\hat{Y} - Y) \\cdot W^{(2)} \\odot (1 - H^2)}_{n \\times H}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ":::{#exr-tanh-derivative}\n",
    "## Derivative of the tanh Function\n",
    "\n",
    "Calculate the derivative of the `tanh` function with respect to its input $x$.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "## Solution (click to expand)\n",
    "\n",
    "We just need to apply the quotient rule to the `tanh` function. The derivative of the `tanh` function is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx} \\text{tanh}(x) & = \\frac{d}{dx} \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right) \\\\\n",
    "&  = \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2}\n",
    "& = \\frac{(e^x + e^{-x})^2}{(e^x + e^{-x})^2} - \\frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} \\\\\n",
    "& = \\frac{\\cancel{(e^x + e^{-x})^2}}{\\cancel{(e^x + e^{-x})^2}} - \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right)^2 \\\\\n",
    "& = 1 - \\text{tanh}^2(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Note that the neural bi-gram model is just a minor modification of the logistic regression model. We can literally copy the code from the logistic regression model and modify it to include the hidden layer. We need to change two things:\n",
    "\n",
    "- Instead of a single large matrix of weights connecting the input layer to the output layer, we now have two matrices of weights, one connecting the input layer to the hidden layer and one connecting the hidden layer to the output layer.\n",
    "- We need to add the `tanh` function to the forward pass and its derivative to the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy the helper functions from the previous notebook here\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_doc(text: str) -> list:\n",
    "    sentences = []\n",
    "    text_doc = nlp(text)\n",
    "    word2idx = {\n",
    "        \"BEGINNING\": 0,\n",
    "        \"END\": 1\n",
    "    }\n",
    "    idx2word = {\n",
    "        0: \"BEGINNING\",\n",
    "        1: \"END\"\n",
    "    }\n",
    "    \n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        tokens = [\"BEGINNING\"]\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "\n",
    "            token_normalized = token.lower_ \n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            if token_normalized not in word2idx:            \n",
    "                idx = len(word2idx)\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        tokens.append(\"END\")\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word\n",
    "\n",
    "def tokens_to_index(tokens: list, word2idx: dict):\n",
    "    indexes_list = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        indexes_list.append(word2idx[token])\n",
    "        \n",
    "    return indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, define the softmax function and the training function\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def train_neural_bi_gram_model(\n",
    "        sentences: list[list[int]],\n",
    "        D: int,\n",
    "        V: int,\n",
    "        learning_rate: float = 0.01,\n",
    "        epochs: int = 100\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Train a neural bigram model using the gradient descent algorithm.\n",
    "\n",
    "    Args:\n",
    "    sentences: list of lists of integers representing the sentences in the corpus\n",
    "    D: the size of the hidden layer\n",
    "    V: the size of the vocabulary\n",
    "    learning_rate: the learning rate of the gradient descent algorithm\n",
    "    epochs: the number of epochs to train the model    \n",
    "    \"\"\"\n",
    "\n",
    "    # As before, we will store the loss values in a list for plotting later\n",
    "    losses = []\n",
    "\n",
    "    # Initialize the weights to small random values\n",
    "    # We divide by sqrt(V) to make the weights smaller\n",
    "\n",
    "    W1 = np.random.randn(V, D) / np.sqrt(V)\n",
    "    W2 = np.random.randn(D, V) / np.sqrt(D)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch so that the model sees the sentences in a different order\n",
    "        # at each epoch\n",
    "\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        # Set up a counter to keep track of the number of sentences processed\n",
    "        i = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            \n",
    "            # An example sentence has the form [\"BEGINNING\", \"hello\", \"my\", \"name\", \"is\", \"john\", \"END\"]\n",
    "            # Only with the word indices instead of the words\n",
    "            # It has n = 7 words and therefore n - 1 = 6 bi-grams\n",
    "            # So each row of the inputs and targets matrices will have the shape (1, vocab_size)\n",
    "            \n",
    "            # This is the same as in the logistic regression model\n",
    "            n = len(sentence)\n",
    "            \n",
    "            X = np.zeros((n - 1, V))\n",
    "            Y = np.zeros((n - 1, V))\n",
    "            X[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            Y[np.arange(n - 1), sentence[1:]] = 1\n",
    "            \n",
    "            # Compute the predicted probabilities (forward pass)\n",
    "            \n",
    "            # DIFFERENT FROM LOGISTIC REGRESSION\n",
    "            # Now the prediction is done in two steps\n",
    "\n",
    "            # First, we compute the hidden layer representation\n",
    "            A = X.dot(W1)\n",
    "            \n",
    "            # Then the hidden layer activation\n",
    "            H = np.tanh(A)\n",
    "\n",
    "            # We map the hidden layer activation to the output layer\n",
    "            z = H.dot(W2)\n",
    "\n",
    "            # We pass the output layer through the softmax function\n",
    "            # to produce the predicted probabilities\n",
    "            Y_hat = softmax(z)\n",
    "            \n",
    "\n",
    "            # AGAIN, DIFFERENT FROM LOGISTIC REGRESSION\n",
    "\n",
    "            # Compute the gradients of the loss with respect to the weights\n",
    "\n",
    "            # The gradient of the loss with respect to the output layer weights\n",
    "            # is the difference between the predicted probabilities and the actual targets\n",
    "            # multiplied by the hidden layer activation\n",
    "\n",
    "            gW2 = H.T.dot(Y_hat - Y)\n",
    "            gW1 = X.T.dot((Y_hat - Y).dot(W2.T) * (1 - H * H))\n",
    "\n",
    "            # Once that we have the gradients, we update the weights\n",
    "\n",
    "            W2 = W2 - learning_rate * gW2\n",
    "            W1 = W1 - learning_rate * gW1\n",
    "                        \n",
    "            # THE LOSS COMPUTATION IS THE SAME AS IN THE LOGISTIC REGRESSION MODEL\n",
    "\n",
    "            # Compute the loss and store it so that we can plot it later\n",
    "            loss = -np.sum(Y * np.log(Y_hat)) / (n - 1)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Plot some information about the training process every 10 sentences            \n",
    "            if i % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"sentence: %s/%s\" % (i, len(sentences)), \"loss:\", loss)\n",
    "            \n",
    "            # Increment the sentence counter\n",
    "            i += 1\n",
    "    \n",
    "    return W1, W2, losses"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
