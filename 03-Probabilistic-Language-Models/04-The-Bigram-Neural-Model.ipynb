{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bi-gram Neural Model\n",
    "\n",
    "[Open in Colab](https://colab.research.google.com/github/febse/ta2024/blob/main/03-Probabilistic-Language-Models/04-The-Bigram-Neural-Model.ipynb)\n",
    "\n",
    "The logistic regression model that we employed to estimate the probability of a word given its preceding word produced a vector representation of each word in the vocabulary (the weights of the model). However, these vectors suffer from some drawbacks. First of all, the length of these vectors is equal to the size of the vocabulary, which can be very large. This makes the model computationally expensive. The number of parameters in the model is equal to the square of the size of the vocabulary.\n",
    "\n",
    "We can try to alleviate this problem by using a neural network model with one hidden layer where the dimension of the hidden layer is much smaller than the size of the vocabulary. For a vocabulary of 3 words and a hidden layer of size 2, the model would look like this:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A1[1] -->|wh11| H1[h1]\n",
    "    A1 -->|wh12| H2[h2]\n",
    "    A2[0] -->|wh21| H1 \n",
    "    A2 -->|wh22| H2\n",
    "    A3[0] -->|wh31| H1\n",
    "    A3 -->|wh32| H2\n",
    "    H1 -->|wo11| O1[z1]\n",
    "    H1 -->|wo21| O2[z2]\n",
    "    H1 -->|wo31| O3[z3]\n",
    "    H2 -->|wo12| O1\n",
    "    H2 -->|wo22| O2\n",
    "    H2 -->|wo32| O3\n",
    "    O1 --> SM\n",
    "    O2 --> SM\n",
    "    O3 --> SM\n",
    "    SM[Softmax]  --> P1[P1] --> Y1[0]\n",
    "    SM --> P2[P2] --> Y2[1]\n",
    "    SM --> P3[P3] --> Y3[0]\n",
    "```\n",
    "\n",
    "Notice that the number of parameters in this model is equal to the number of weights connecting the input layer to the hidden layer plus the number of weights connecting the hidden layer to the output layer. Here we have 6 weights connecting the input layer to the hidden layer and 6 weights connecting the hidden layer to the output layer, for a total of 12 weights. In this case this even appears to worsen the problem of the number of parameters as the logistic regression would have only $3 \\cdot 3 = 9$ parameters.\n",
    "\n",
    "However, with a vocabulary of 1000 words and a hidden layer of size 100, the number of parameters in the neural network model is $2 \\cdot 100000$, which is much smaller than the one million parameters in the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the model we must specify an activation function for the hidden layer. As an exercise, let us choose the `tanh` function. The `tanh` function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "## The Forward Pass\n",
    "\n",
    "For a single bi-gram with input $x$ and output $y$, the forward pass of the model is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a = W_{h}^T \\cdot x \\\\\n",
    "h = \\text{tanh}(a) \\\\\n",
    "z = W_{o}^T \\cdot h \\\\\n",
    "\\hat{y} = \\text{Softmax}(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $W_{h}$ is the matrix of weights connecting the input layer to the hidden layer, $W_{o}$ is the matrix of weights connecting the hidden layer to the output layer, and $\\hat{y}$ is the predicted probability distribution over the vocabulary.\n",
    "\n",
    "## The Backward Pass\n",
    "\n",
    "The loss function for the model is the cross-entropy loss function just as in the logistic regression model.\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "This time we have two sets of weights to update, $W_{h}$ and $W_{o}$. The gradients of the loss function with respect to the weights are given by:\n",
    "\n",
    "The gradient of the loss function with respect to the output layer weights is the same as in the logistic regression model, only this time the input to the softmax function is the output of the hidden layer instead of the input layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{o}} = (\\hat{y} - y) \\cdot h^T\n",
    "$$\n",
    "\n",
    "We can find the gradient of the loss function with respect to the hidden layer weights by applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{h}} & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} \\cdot \\frac{\\partial a}{\\partial W_{h}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We already know the derivative of the cross-entropy loss function with respect to the output layer (z). This was a vector of size $V$ where $V$ is the size of the vocabulary giving the prediction errors.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{z}} = \\hat{y} - y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next derivative is the derivative of the output layer with respect to activation of the hidden layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{z}}{\\partial h} = W_{o}\n",
    "$$\n",
    "\n",
    "The next derivative is the derivative of the hidden layer with respect to its activation function. This is the derivative of the `tanh` function.\n",
    "\n",
    "Note that because we are taking the derivative of a vector with respect to itself, the derivative is a diagonal matrix of dimension $H \\times H$ where $H$ is the size of the hidden layer. See @exr-tanh-derivative for the derivative of the `tanh` function.\n",
    "\n",
    "$$\n",
    "h = \\text{tanh}(a) = \\begin{bmatrix} \\text{tanh}(a_1) \\\\ \\text{tanh}(a_2) \\\\ \\vdots \\\\ \\text{tanh}(a_H) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial a} = \\begin{bmatrix} 1 - \\text{tanh}^2(a_1) & 0 & \\cdots & 0 \\\\ 0 & 1 - \\text{tanh}^2(a_2) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 - \\text{tanh}^2(a_H) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying the derivatives up to the last term we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} & = (\\hat{y} - y) W_{o} \\cdot \\text{diag}(1 - \\text{tanh}^2(a))\\\\\n",
    "& = \\underbrace{(\\hat{y} - y) W_{o} \\odot (1 - h^2)}_{1 \\times H} \\\\\n",
    "\\end{align}\n",
    "$${#eq-bi-gram-hidden-weights-partial}\n",
    "\n",
    "For the last derivative we need to follow the same step as in the logistic regression model, so the full derivative is the outer product of @eq-bi-gram-hidden-weights-partial and the input vector.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial W_{h}}}_{V \\times H} = \\underbrace{x^T}_{V \\times 1} \\underbrace{(\\hat{y} - y) W_{o} \\odot (1 - h^2)}_{1 \\times H}\n",
    "$$\n",
    "\n",
    "\n",
    "For a matrix of input vectors $X$ and output vectors $Y$, the gradients of the loss function with respect to the weights are given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{o}} & = \\underbrace{H^T}_{H \\times n} \\underbrace{(\\hat{Y} - Y)}_{n \\times V} \\\\\n",
    "\\frac{\\partial L}{\\partial W_{h}} & = \\underbrace{X^T}_{V \\times n} \\underbrace{(\\hat{Y} - Y) \\cdot W_{o} \\odot (1 - H^2)}_{n \\times H}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ":::{#exr-tanh-derivative}\n",
    "## Derivative of the tanh Function\n",
    "\n",
    "Calculate the derivative of the `tanh` function with respect to its input $x$.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "## Solution (click to expand)\n",
    "\n",
    "We just need to apply the quotient rule to the `tanh` function. The derivative of the `tanh` function is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx} \\text{tanh}(x) & = \\frac{d}{dx} \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right) \\\\\n",
    "&  = \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2}\n",
    "& = \\frac{(e^x + e^{-x})^2}{(e^x + e^{-x})^2} - \\frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} \\\\\n",
    "& = \\frac{\\cancel{(e^x + e^{-x})^2}}{\\cancel{(e^x + e^{-x})^2}} - \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right)^2 \\\\\n",
    "& = 1 - \\text{tanh}^2(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-info}\n",
    "## Assignment\n",
    "\n",
    "Use the functions in the logistic regression model and adapt them to train the neural bi-gram model.\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
