{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24fded7db0e8f68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "## Skip-gram Model\n",
    "\n",
    "The neural bi-gram model that we have discussed so far does produce word embeddings (word vectors of fixed size, the size of the hidden layer). It does a poor job at capturing semantic relationships between words, because it is just trained to predict the next word given a preceding word. \n",
    "\n",
    "The skip-gram models extends the bi-gram model with the goal of capturing a larger context around each word.\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "The bi-gram model would produce the following training sample for the word \"tired\":\n",
    "\n",
    "- (tired, of)\n",
    "\n",
    "The skip-gram model looks at a set of nearby words (context window $m$) and produces the following training data for the word tired $m = 2$\n",
    "\n",
    "- (tired, get)\n",
    "- (tired, very)\n",
    "- (tired, of)\n",
    "- (tired, sitting)\n",
    "\n",
    "In this sense it looks very much like the bi-gram model, except that it skips a few words between the words in the training samples. This is why it is called the skip-gram model.\n",
    "\n",
    "You can think about the model in two ways which are equivalent:\n",
    "\n",
    "1. One training sample with four targets (and therefore four softmax classifiers)\n",
    "    - tired -> (very, get, of, sitting)\n",
    "2. Four training samples with the same input word and four different target words\n",
    "    - tired -> very\n",
    "    - tired -> get\n",
    "    - tired -> of\n",
    "    - tired -> sitting\n",
    "\n",
    "\n",
    "![Skip-gram Model](../figures/skip-gram-architecture.webp)\n",
    "\n",
    "Different from the bi-gram model, the skip-gram drops the non-linear activation function (tanh) and uses the identity function instead. Apart from making the model simpler, this change makes the dot-product between the word-vector of the center word and the word-vectors of the context words the only factor that determines the probability of the context words given the center word. Remember that the dot-product is a measure of similarity between two vectors, so the skip-gram model will produce word vectors that are closer in the vector space for words that are commonly used in similar contexts.\n",
    "\n",
    "Let's denote the weight matrices of the skip-gram model as $\\mathbf{W}^{1}$ and $\\mathbf{W}^{2}$ for the input and output layers, respectively. The hidden layer is given by:\n",
    "\n",
    "$$\n",
    "h = \\mathbf{W}^{(1)T} x\n",
    "$$\n",
    "\n",
    "where $x$ is the one-hot encoded input vector of length $V$ (the size of the vocabulary) and $h$ is the hidden layer of size $D$ (the size of the word embedding).\n",
    "\n",
    "Note that because the input vector is one-hot encoded, the dot-product is equivalent to selecting the row of the weight matrix that corresponds to the index of the input word.\n",
    "\n",
    "The output layer is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{P}(y | \\text{input}) = \\text{softmax}(\\mathbf{W}^{(2)T} h)\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability distribution over the vocabulary and is a vector of length $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190844f20ef41886",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Negative Sampling\n",
    "\n",
    "The practical application of the skip-gram model is computationally expensive. Consider what will happen if we try to train the model on a large corpus with a vocabulary size e.g., $V = 250000$ words and word vector size $D = 300$. Each of the two sets of weights $W^h$ and $W^o$ will be $250000 \\times 300 = 7.5 \\times 10^7$. In order to train the model, we will need a huge amount of training data to avoid overfitting because of the high number of parameters. This means that the model will take a long time to train. Furthermore, the model will have to compute the softmax function for each of the $V$ words in the vocabulary, which is also computationally expensive.\n",
    "\n",
    "There are several approaches to work around the softmax computational problem.\n",
    "\n",
    "- Hierarchical Softmax (not covered here)\n",
    "- Negative Sampling\n",
    "\n",
    "The idea behind negative sampling is to just throw away the softmax function and replace it with binary classification problems. Instead of predicting the probability of each word in the vocabulary, we predict the probability of the correct context word and the probability of a set of words that are not in the context of the center word. These words are called negative samples.\n",
    "\n",
    "A natural question is how to choose the negative examples. The authors of the skip-gram model suggest using the uni-gram distribution raised to the 3/4 power. This is equivalent to sampling from the distribution.\n",
    " \n",
    "The uni-gram distribution is simply the frequency of each word in the corpus. \n",
    "\n",
    "$$\n",
    "p(\"tiger\") = \\frac{\\text{frequency of \"tiger\"}}{\\text{total number of words}}\n",
    "$$\n",
    "\n",
    "The uni-gram distribution raised to the 3/4 power is:\n",
    "\n",
    "$$\n",
    "p^1(\"tiger\") = \\frac{p(\\text{\"tiger\"})^{3/4}}{\n",
    "\\sum}\n",
    "$$\n",
    "\n",
    "where the sum in the denominator is the sum of the frequency of each word in the corpus raised to the 3/4 power so that $p^1$ is a probability distribution over the vocabulary. The 3/4 power is used to reduce the probability of sampling very frequent words and is a hyperparameter of the model.\n",
    "\n",
    "## Training the Skip-gram Model with Negative Sampling\n",
    "\n",
    "Consider again the training sample:\n",
    "\n",
    "- (tired, get)\n",
    "- (tired, very)\n",
    "- (tired, of)\n",
    "- (tired, sitting)\n",
    "\n",
    "The skip-gram model with negative sampling will produce the following training samples:\n",
    "\n",
    "\n",
    "- (tired, get, 1)\n",
    "- (tired, very, 1)\n",
    "- (tired, of, 1)\n",
    "- (tired, sitting, 1)\n",
    "- (tired, cat, 0)\n",
    "- (tired, egg, 0)\n",
    "- (tired, sky, 0)\n",
    "- (tired, smile, 0)\n",
    "\n",
    "where the third element in the tuple is the label of the training sample. The label is 1 if the word is in the context of the center word and 0 if it is not. Now we can train the model using binary cross-entropy loss instead of the softmax loss.\n",
    "\n",
    "We will have to predict the following probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\text{get} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{get}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{very} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{very}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{of} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{of}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{sitting} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{sitting}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{cat} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{cat}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{egg} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{egg}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{sky} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{sky}} W^{(1)}_{\\text{tired}}) \\\\\n",
    "P(\\text{smile} | \\text{tired}) & = \\sigma(W^{(2)T}_{\\text{smile}} W^{(1)}_{\\text{tired}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61fd46",
   "metadata": {},
   "source": [
    "Given these predicted probabilities, we can compute the binary cross-entropy loss. Remember\n",
    "that the binary cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "L = y_{\\text{out}} \\log P(y_{\\text{out}} = 1 | x_{\\text{in}}) + (1 - y_{\\text{out}}) \\log (1 - P(y_{\\text{out}} = 1 | x_{\\text{in}}))\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\text{get}) & = -\\log(P(\\text{get} | \\text{tired})) \\\\\n",
    "L(\\text{very}) & = -\\log(P(\\text{very} | \\text{tired})) \\\\\n",
    "L(\\text{of}) & = -\\log(P(\\text{of} | \\text{tired})) \\\\\n",
    "L(\\text{sitting}) & = -\\log(P(\\text{sitting} | \\text{tired})) \\\\\n",
    "L(\\text{cat}) & = -\\log(1 - P(\\text{cat} | \\text{tired})) \\\\\n",
    "L(\\text{egg}) & = -\\log(1 - P(\\text{egg} | \\text{tired})) \\\\\n",
    "L(\\text{sky}) & = -\\log(1 - P(\\text{sky} | \\text{tired})) \\\\\n",
    "L(\\text{smile}) & = -\\log(1 - P(\\text{smile} | \\text{tired}))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The total loss is the sum of the losses:\n",
    "\n",
    "$$\n",
    "L = L(\\text{get}) + L(\\text{very}) + L(\\text{of}) + L(\\text{sitting}) + L(\\text{cat}) + L(\\text{egg}) + L(\\text{sky}) + L(\\text{smile})\n",
    "$$\n",
    "\n",
    "We can write the loss more generally as:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{w \\in \\text{Context}} \\log(P(y_{\\text{w}} = 1 | x_{\\text{in}})) - \\sum_{w \\in \\text{Neg. samples}} \\log(1 - P(y_{\\text{w}} = 1 | x_{\\text{in}}))\n",
    "$$\n",
    "\n",
    "where $k$ is the number of negative samples.\n",
    "\n",
    "Now that we have the loss function, we can derive the gradients so that we can update the weights of the model using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f15256",
   "metadata": {},
   "source": [
    "Let's first consider the updates for the output layer weights $\\mathbf{W}^{(2)}$. For a single training sample $(x_{\\text{in}}, y_{\\text{out}})$, the gradient of the loss with respect to the output layer weights is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}_{\\text{out}}} & = W^{(1)}_{\\text{in}} (P(y_{\\text{out}} = 1 | x_{\\text{in}}) - y_{\\text{out}}) \\end{align*}\n",
    "$$\n",
    "\n",
    "The multiplication here is allowed, because for a single training sample $p - y$ is a scalar. Let's now consider a batch of training samples.\n",
    "\n",
    ":::{.callout-important}\n",
    "## Weight Update for the Output Layer\n",
    "\n",
    "Note that the weight update for the output layer only affects a single row of the weight matrix $\\mathbf{W}^{(2)}$. This is because the predicted probability only depends on the word vector of the output word.\n",
    "\n",
    ":::\n",
    "\n",
    "Now let's consider a batch of three training samples:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}_{\\text{of}}} & = W^{(1)}_{\\text{tired}} (P(y_{\\text{of}} = 1 | x_{\\text{tired}}) - y_{\\text{of}}) \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}_{\\text{sitting}}} & = W^{(1)}_{\\text{tired}} (P(y_{\\text{sitting}} = 1 | x_{\\text{tired}}) - y_{\\text{sitting}}) \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}_{\\text{cat}}} & = W^{(1)}_{\\text{tired}} (P(y_{\\text{cat}} = 1 | x_{\\text{tired}}) - y_{\\text{cat}}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The weight updates only affect the rows of the weight matrix that correspond to the output words in the training samples. In order to vectorize the weight updates, consider the vector of prediction errors:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(P - Y) & = \\begin{bmatrix} P(y_{\\text{of}} = 1 | x_{\\text{tired}}) - y_{\\text{of}} \\\\ P(y_{\\text{sitting}} = 1 | x_{\\text{tired}}) - y_{\\text{sitting}} \\\\ P(y_{\\text{cat}} = 1 | x_{\\text{tired}}) - y_{\\text{cat}} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You can obtain the $3 \\times V$ (sub)-matrix of gradients for the output layer weights by taking the _outer_ product of the word vector of the input word (with $D = 100$ embedding size) and the prediction errors:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = W^{(1)}_{\\text{tired}} \\otimes (P - Y)\n",
    "$$\n",
    "\n",
    "where $\\otimes$ is the outer product.\n",
    "\n",
    "We can derive the gradient for the hidden layer weights $\\mathbf{W}^{(1)}$ by using the chain rule. We must also keep in mind that for a single batch (center word, context words and negative samples) we only need to update one row of the weight matrix $\\mathbf{W}^{(1)}$ corresponding to the center (input) word.\n",
    "\n",
    "Let us review how we derived the gradient in the logistic regression model. Here the derivation is even simpler, because the hidden layer is just a linear transformation of the input layer.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_n} & = - \\frac{\\partial}{\\partial z_n} (y_n \\log(\\sigma(z_n)) + (1 - y_n)(1 - \\sigma(z_n))) \\\\\n",
    "& = - \\frac{y_n}{\\sigma(z_n)} \\frac{\\partial \\sigma(z_n)}{\\partial z_n} + \\frac{1 - y_n}{1 - \\sigma(z_n)} \\frac{\\partial \\sigma(z_n)}{\\partial z_n} \\\\\n",
    "& = - \\frac{y_n}{\\sigma(z_n)} \\sigma(z_n)(1 - \\sigma(z_n)) + \\frac{1 - y_n}{1 - \\sigma(z_n)} \\sigma(z_n)(1 - \\sigma(z_n)) \\\\\n",
    "& = - y_n (1 - \\sigma(z_n)) + (1 - y_n) \\sigma(z_n) \\\\\n",
    "& = - y_n + y_n \\sigma(z_n) + \\sigma(z_n) - y_n \\sigma(z_n) \\\\\n",
    "& = \\sigma(z_n) - y_n \\\\\n",
    "& = P(y_n = 1 | x_{\\text{in}}) - y_n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The output layer value $z_n$ is just the dot product of the hidden layer and the output layer weights:\n",
    "\n",
    "$$\n",
    "z_n = W^{(2)}_{\\text{out}(n)} W^{(1)}_{\\text{in}}\n",
    "$$\n",
    "\n",
    "One thing that we need to keep in mind is that the gradient of the loss with respect to the hidden layer weights is the sum of the gradients of the loss with respect to the output layer weights, because the hidden layer weights are shared across all output words and therefore the probabilities of all output words depend on the hidden layer weights.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}_{\\text{in}}} & = \\sum_{n = 1}^{N} \\frac{\\partial L}{\\partial a_n} \\frac{\\partial a_n}{\\partial W^{1}_{\\text{in}}} \\\\\n",
    "& = \\sum_{n = 1}^{N} (P(y_n = 1 | x_{\\text{in}}) - y_n) \\frac{\\partial W^{(2)T}_{\\text{out}(n)}W^{(1)_{\\text{in}}}}{\\partial W^{(1)_{\\text{in}}}} \\\\\n",
    "& = \\sum_{n = 1}^{N} (P(y_n = 1 | x_{\\text{in}}) - y_n) W^{(2)}_{\\text{out}(n)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439c927",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Subsampling Frequent Words\n",
    "\n",
    "A pattern occurring in most human language texts is that some words are much more frequent than others. There is an empirical finding known as Zipf's law that states that the frequency of a word is inversely proportional to its rank in the frequency table. For example, the most frequent word will occur twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "$$\n",
    "\\text{frequency} \\propto \\frac{1}{\\text{rank}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6a78a3b8e23f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:08.141505642Z",
     "start_time": "2023-12-20T16:04:04.741182309Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "gutenberg.fileids()\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc633315d76f0cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:08.280391534Z",
     "start_time": "2023-12-20T16:04:08.146221431Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1616),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 620),\n",
       " ('she', 544),\n",
       " ('it', 539),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 396),\n",
       " ('was', 366),\n",
       " ('i', 364),\n",
       " ('in', 359),\n",
       " ('you', 356),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " ('at', 209),\n",
       " ('on', 191),\n",
       " ('had', 184),\n",
       " ('with', 179)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "words = word_tokenize(alice)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "FreqDist(words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0c29e92d2debc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because some words are used very often (e.g., the), they provide little information about the context. Furthermore, the training will spend a lot of time learning the word vectors for these words.\n",
    "\n",
    "A solution is to drop some of the frequent words from the training data. The authors of the skip-gram model suggest dropping words with a probability of:\n",
    "\n",
    "$$\n",
    "P_{drop}(w) = 1 - \\sqrt{\\frac{\\text{threshold}}{p^1(w)}}\n",
    "$$\n",
    "\n",
    "with a threshold of $t = 10^{-5}$ and $p^1(w)$ is modified uni-gram distribution.\n",
    "\n",
    "Consider our running example:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "and suppose that we drop the words \"get\" and \"of\" of the words in the sentence. The sentence will become:\n",
    "\n",
    "```\n",
    "Alice was beginning to very **tired** sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "What is the effect of this technique on the context window size?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
