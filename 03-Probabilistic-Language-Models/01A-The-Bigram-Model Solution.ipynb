{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e65b70ca58cf6a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# A Probabilistic Language Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc208377",
   "metadata": {},
   "source": [
    "A very simple model for a natural language is the Markov bi-gram model.\n",
    "\n",
    "Bi-grams are sequences of two consecutive words.\n",
    "\n",
    "```\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again. \n",
    "```\n",
    "\n",
    "The bi-grams of the sentence are\n",
    "\n",
    "1. \"In another\"\n",
    "2. \"another moment\"\n",
    "3. \"moment down\"\n",
    "4. ...\n",
    "\n",
    "The model bi-gram model is a probabilistic language model based on the probabilities of bi-grams. The probability of a bi-gram is the conditional probability of the second word given the first word.\n",
    "\n",
    "$$\n",
    "P(\\text{another} | \\text{in}), \\\\\n",
    "P(\\text{moment} | \\text{another}) \\\\\n",
    "P(\\text{down} | \\text{moment})\n",
    "$$\n",
    "\n",
    "Applications of the bi-gram model include\n",
    "- Speech recognition\n",
    "- Optical character recognition\n",
    "- Spelling and grammar correction, nonsense detection (low probability sequences)\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cff452388432fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# The Bi-gram model\n",
    "\n",
    "In most languages, the units of meaning are sentences (a sequence of words and punctuation). An interesting question would be to ask: what is the probability to see a specific sentence in a corpus of text?\n",
    "\n",
    "Take the following quote from Oscar Wilde as an example:\n",
    "\n",
    "```\n",
    "Be yourself, everyone else is already taken.\n",
    "```\n",
    "\n",
    "Ignoring the punctuation, there are 7 words in this sentence: $w_1$ to $w_7$.\n",
    "\n",
    "$w_1$ = \"Be\"\n",
    "$w_2$ = \"yourself\"\n",
    "$w_3$ = \"everyone\"\n",
    "...\n",
    "$w_7$ = \"taken\"\n",
    "\n",
    "We could write the probability of this sentence as\n",
    "\n",
    "$$\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1)\n",
    "$$\n",
    "\n",
    "and (in principle) we could estimate this probability by counting the number of times this sentence occurs in a large corpus of text. However, this may not work well because the number of times a whole sentence (or any other long sequence of tokens) occurs in a corpus is likely to be very small or zero.\n",
    "\n",
    "Let's look into a way to model this probability in a more tractable way.\n",
    "\n",
    "Using the chain rule of probability, we can write the probability of a sentence as the product of the probabilities of each word given the preceding words. With the 7 words in the sentence, we can write\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1) = & P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6, w_5, w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5, w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, all these probabilities are not much more tractable than the probability of the whole sentence, so we need to make an assumption to simplify this model. The most strict assumption that we can make is that the conditional probability of a word only depends on the preceding word. This is called the **Markov assumption**. With this assumption, the probability of the 7-th word given the preceding words can be written as\n",
    "\n",
    "$$\n",
    "P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) = P(w_7 | w_6)\n",
    "$$\n",
    "\n",
    "Analogously, we can write the probability of the sentence as\n",
    "\n",
    "$$\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1) = P(w_7 | w_6) \\cdot P(w_6 | w_5) \\cdot P(w_5 | w_4) \\cdot P(w_4 | w_3) \\cdot P(w_3 | w_2) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "$$\n",
    "\n",
    "The probabilities of the bi-grams (the probability so see the second word given the first word) are much easier to estimate than the probability of a whole sentence. We can estimate these probabilities by counting the number of times a word $w_t$ follows a word $w_{t - 1}$ in a corpus of text.\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_t | w_{t - 1}) = \\frac{\\text{Count}(w_{t - 1}, w_t)}{\\text{Count}(w_{t - 1})}\n",
    "$$\n",
    "\n",
    "More generally, the probability of a sentence with $T$ words can be written as\n",
    "\n",
    "$$\n",
    "P(w_T, w_{T - 1}, \\ldots, w_1) = P(w_1) \\prod_{t = 2}^{T} P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "As these probabilities would tend to be small, multiplying them in long sequences may lead to underflow problems because computer precision is limited. A common solution to this problem is to use the log-probabilities instead of the probabilities.\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P(w_T, w_{T - 1}, \\ldots, w_1) = \\log P(w_1) +  \\sum_{t = 2}^{T} \\log P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "\n",
    "Another problem is that the log probabilities of sentences will be biased toward shorter sentences, simply because there are less terms in a short sentence. To solve this problem, we can normalize the log probabilities by dividing by the number of words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:58:38.929473979Z",
     "start_time": "2023-12-18T15:58:35.901907219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917d7e9f8c053444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:03.150423303Z",
     "start_time": "2023-12-18T15:30:03.142622821Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\\nOh dear! I shall be late!' (when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "\n",
    "alice[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5289a38f6498fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:06.858917781Z",
     "start_time": "2023-12-18T15:30:03.149423919Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we will pass the whole text through spacy's pipeline\n",
    "\n",
    "doc = nlp(alice[:200])\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187e4541-8af1-46f6-bbe6-ed89d44c429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[          X\n",
      "alice      PROPN\n",
      "'s         PART\n",
      "adventures PROPN\n",
      "in         ADP\n",
      "wonderland PROPN\n",
      "by         ADP\n",
      "lewis      PROPN\n",
      "carroll    PROPN\n",
      "1865       NUM\n",
      "]          PUNCT\n",
      "\n",
      "\n",
      "         SPACE\n",
      "chapter    NOUN\n",
      "i.         PROPN\n",
      "down       ADP\n",
      "the        DET\n",
      "rabbit     PROPN\n",
      "-          PUNCT\n",
      "hole       PROPN\n",
      "\n",
      "\n",
      "         SPACE\n",
      "alice      PROPN\n",
      "was        AUX\n",
      "beginning  VERB\n",
      "to         PART\n",
      "get        VERB\n",
      "very       ADV\n",
      "tired      ADJ\n",
      "of         ADP\n",
      "sitting    VERB\n",
      "by         ADP\n",
      "her        PRON\n",
      "sister     NOUN\n",
      "on         ADP\n",
      "the        DET\n",
      "\n",
      "          SPACE\n",
      "bank       NOUN\n",
      ",          PUNCT\n",
      "and        CCONJ\n",
      "of         ADP\n",
      "having     VERB\n",
      "nothing    PRON\n",
      "to         PART\n",
      "do         VERB\n",
      ":          PUNCT\n",
      "once       ADV\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.lower_:10} {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee929852-223b-42c4-a3d7-51182aa7b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16549f41b17ec43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The result is an object of class `Doc` that we can use to a sequence of sentences and tokens. We will use the `sent()` generator to iterate over the first few sentences in the book and save them in a list. Next, we will create a small function to tokenize the sentences and remove the punctuation and spaces tokens. It wil also create a word to index dictionary and an index to word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2774138c46bc034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:07.850090121Z",
     "start_time": "2023-12-18T15:30:07.806243633Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_doc(text: str):\n",
    "    sentences = []\n",
    "    text_doc = nlp(text)\n",
    "    \n",
    "    word2idx = {\n",
    "        \"BEGINNING\": 0,\n",
    "        \"END\": 1\n",
    "    }\n",
    "    idx2word = {\n",
    "        0: \"BEGINNING\",\n",
    "        1: \"END\"\n",
    "    }\n",
    "    \n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        tokens = [\"BEGINNING\"]\n",
    "        \n",
    "        for token in sentence:            \n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "            token_normalized = token.lower_ \n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            if token_normalized not in word2idx:\n",
    "                idx = len(word2idx)\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        tokens.append(\"END\")\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word\n",
    "\n",
    "tmp_, tmp_word2idx, tmp_idx2word = tokenize_doc(alice[0:1111])\n",
    "len(tmp_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1fc7bfbbe3609a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:15.998848401Z",
     "start_time": "2023-12-18T15:30:15.996349040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we can create a V x V matrix where V is the size of the vocabulary\n",
    "\n",
    "def compute_bigrams_prob_mtx(sentences, word2idx: dict, smoothing: float = 1.0):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = len(word2idx)\n",
    "    \n",
    "    # Let's first create a matrix of counts\n",
    "    BGC = np.ones((vocab_size, vocab_size)) * smoothing\n",
    "\n",
    "    # Now let us loop over all sentences, extract the bi-grams and count their occurrences\n",
    "    # Each time we encounter the sequence \"is strong\" for example, we will increment the count of the\n",
    "    # Row index of the first word and the column index of the second word\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            first_word = sent[i - 1]\n",
    "            \n",
    "            # We will use the word2idx dictionary to get the index of the word\n",
    "            first_word_idx = word2idx[first_word]\n",
    "            second_word_idx = word2idx[word]\n",
    "            # We will use the index to increment the count of the word\n",
    "            BGC[first_word_idx, second_word_idx] += 1\n",
    "    \n",
    "    # Now we can normalize the counts to get the probabilities\n",
    "    \n",
    "    BGP = BGC / BGC.sum(axis=1, keepdims=True)\n",
    "    return BGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dce8337c18f5e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:18.970391193Z",
     "start_time": "2023-12-18T15:30:18.967392052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def score_sentence_str(sentence_str: str, word2idx: dict, bigram_probs: np.ndarray):\n",
    "    # First we tokenize the sentence and remove the punctuation and spaces tokens\n",
    "    sents, _, _ = tokenize_doc(sentence_str)\n",
    "    \n",
    "    sentence_score = 0\n",
    "    \n",
    "    words = sents[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            first_word_idx = word2idx[words[i - 1]]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {words[i - 1]} not in vocabulary\")\n",
    "        \n",
    "        try:\n",
    "            second_word_idx = word2idx[word]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {word} not in vocabulary\")\n",
    "                \n",
    "        sentence_score += np.log(bigram_probs[first_word_idx, second_word_idx])\n",
    "        \n",
    "    return sentence_score / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d3f46d341e62ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:26.193805458Z",
     "start_time": "2023-12-18T15:30:23.055131383Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let us run the whole thing\n",
    "\n",
    "sentences, word2idx, idx2word = tokenize_doc(alice)\n",
    "BGP = compute_bigrams_prob_mtx(sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ed2001369513c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:28.350304554Z",
     "start_time": "2023-12-18T15:30:28.344750384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00023596, 0.0261916 ],\n",
       "       [0.00037313, 0.00037313]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGP[0:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4485438d3cbc7465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:32.196484124Z",
     "start_time": "2023-12-18T15:30:32.151874863Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.7696548515779575"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence = sentences[32]\n",
    "# second_sentence\n",
    "score_sentence_str(\" \".join(second_sentence), word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2918b438-009a-4616-8c27-5ca15535cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGINNING',\n",
       " 'no',\n",
       " 'it',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'do',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " 'shall',\n",
       " 'see',\n",
       " 'it',\n",
       " 'written',\n",
       " 'up',\n",
       " 'somewhere',\n",
       " 'END']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a928e1924e808af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:34.845609857Z",
     "start_time": "2023-12-18T15:30:34.817295475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.0020184778362875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it out with a valid sentence\n",
    "score_sentence_str(\"Be nice, everybody else is already taken.\", word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae281944685eaf35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:31:04.060613419Z",
     "start_time": "2023-12-18T15:31:04.013001546Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.539825013894565"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentence_str(\"Rude foot fun egg.\", word2idx, BGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904762ea8eedc39",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "Instead of counting the number of times a word occurs in the corpus, we can use a logistic regression model to estimate the probability of a word given the preceding word. The logistic regression model will learn a vector representation for each word in the vocabulary. The probability of a word given the preceding word will be the dot product of the vector representations of the two words.\n",
    "\n",
    "First we need to map the words to numbers, because the logistic regression model operates on matrices of numbers. What we can do is create a vocabulary (all unique words in our corpus) and represent each word with $V$ dimensional vector where $V$ is the size of the vocabulary. The vector will have a 1 at the index of the word and zeros everywhere else. This is called a one-hot encoding.\n",
    "\n",
    "For example, if our vocabulary is\n",
    "\n",
    "```\n",
    "[\"be\", \"yourself\", \"everyone\", \"else\", \"is\", \"already\", \"taken\"]\n",
    "```\n",
    "\n",
    "then the one-hot encoding of \"yourself\" will be\n",
    "\n",
    "```\n",
    "[0, 1, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "the one-hot encoding of \"is\" will be\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "Let's create a function to create these one-hot encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff839d4b9665eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T14:52:01.731480745Z",
     "start_time": "2023-12-17T14:52:01.726790237Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This will return a one-hot encoded vector with 1 at the index of idx\n",
    "def one_hot_encode_word(idx: int, vocab_size: int):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[idx] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1addb37c64ac3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:34:45.156404475Z",
     "start_time": "2023-12-18T15:34:45.134771469Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# It is convenient to have a function that processes the raw text and returns the word indices\n",
    "def text_to_indexed_sentences(sentences: list, word2idx: dict):\n",
    "    sentences_with_idx = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_with_idx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            idx = word2idx[word]    \n",
    "            sentence_with_idx.append(idx)\n",
    "            \n",
    "        sentences_with_idx.append(sentence_with_idx)\n",
    "    \n",
    "    return sentences_with_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77167fa020d6dd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:34:52.599776649Z",
     "start_time": "2023-12-18T15:34:52.588587258Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGINNING', 'hello', 'my', 'name', 'is', 'john', 'END']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences, sample_word2idx, sample_idx2word = tokenize_doc(\"Hello, my name is John. What is your name?\")\n",
    "sample_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84617605544ce6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:35:05.972585057Z",
     "start_time": "2023-12-18T15:35:05.928908797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 6, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_alice_sentences_idx = text_to_indexed_sentences(sample_sentences, sample_word2idx)\n",
    "sample_alice_sentences_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda35b72d7ebb70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we need to consider how the training data for our problem should look like. In a classification problem we normally a have $N \\times K$ matrix $\\mathbf{X}$, representing the $K$ features of $N$ observations.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\ldots & x_{1K} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{2K} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & x_{N2} & \\ldots & x_{NK} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and a $N$ dimensional vector $\\mathbf{y}$ representing the labels of the $N$ observations. It is convenient to represent the labels as one-hot encoded vectors. For example, with $K = 3$ classes, the labels will be $N \\times K$ matrix $\\mathbf{Y}$ of one-hot encoded vectors.\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the example above, the first observation belongs to the first class, the second observation belongs to the second class, the third observation belongs to the third class and the last observation belongs to the second class.\n",
    "\n",
    "In our specific case the labels are the second words in the bi-grams and each word is represented by a one-hot encoded vector. So the labels will be an $N \\times V$ matrix $\\mathbf{Y}$ where $V$ is the size of the vocabulary.\n",
    "\n",
    "The predictor matrix $\\mathbf{X}$ will also be an $N \\times V$ matrix. The $i$-th row of $\\mathbf{X}$ will be the one-hot encoded vector of the first word in the $i$-th bi-gram.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & 0 & \\ldots & 1 \\\\\n",
    "0 & 0 & 0 & \\ldots & 1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 1 & 0 & \\ldots & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can read this example matrix as: the first bi-gram is \"be yourself\", the second bi-gram is \"yourself everyone\", the third bi-gram is \"everyone else\" and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e8702f29bfc2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "$$\n",
    "p(y | x) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T \\mathbf{x})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb606ce280bdb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " Using the cross-entropy loss\n",
    " \n",
    "$$\n",
    "J(w) = -\\frac{1}{N}\\sum_{i = 1}^{N} \\sum_{j = 1}^{V} y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "the gradient descent update rule for the weights is\n",
    "\n",
    "$$\n",
    "W^{\\text{new}} = W^{\\text{old}} - \\eta \\nabla_{w} J(w) \\\\\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate and the gradient of the loss with respect to the weights is\n",
    "\n",
    "$$\n",
    "\\nabla J = X^T (\\hat{Y} - Y)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669e01d6e6b5e745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:37:17.418953402Z",
     "start_time": "2023-12-18T15:37:17.417319502Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Next we will define the softmax function that will take a np.array of shape (N, D) and return a np.array of shape (N, D) where each row is\n",
    "# the softmax of the corresponding row in the input array\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def train_logistic(sentences: list[list[int]], vocab_size: int, learning_rate: float = 0.01, epochs: int = 100):    \n",
    "    losses = []\n",
    "    \n",
    "    # Initialize weights\n",
    "    W = np.random.randn(vocab_size, vocab_size) / np.sqrt(vocab_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        j = 0 # keep track of iterations\n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            \n",
    "            # An example sentence has the form [\"BEGINNING\", \"hello\", \"my\", \"name\", \"is\", \"john\", \"END\"]\n",
    "            # Only with the word indices instead of the words\n",
    "            # It has n = 7 words and therefore n - 1 = 6 bi-grams\n",
    "            # So each row of the inputs and targets matrices will have the shape (1, vocab_size)\n",
    "            \n",
    "            n = len(sentence)\n",
    "            \n",
    "            inputs = np.zeros((n - 1, vocab_size))\n",
    "            targets = np.zeros((n - 1, vocab_size))\n",
    "            inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "\n",
    "            #print(\"Inputs matrix\")\n",
    "            #print(inputs)\n",
    "\n",
    "            #print(\"Targets matrix\")\n",
    "            #print(targets)\n",
    "            \n",
    "            # Compute the predictions\n",
    "            predictions = softmax(inputs.dot(W))\n",
    "            \n",
    "            # Perform a gradient descent update\n",
    "            W = W - learning_rate * inputs.T.dot(predictions - targets)\n",
    "            \n",
    "            # Save the loss at each iteration (we don't use it here, but you may want to plot it later)\n",
    "            loss = - np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "            losses.append(loss)     \n",
    "            \n",
    "            if j % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "            j += 1\n",
    "    \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb92e37d67cb69bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:38:39.782170003Z",
     "start_time": "2023-12-18T15:38:36.747391340Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "full_sentences, full_word2idx, full_idx2word = tokenize_doc(alice)\n",
    "full_sentences_idx = text_to_indexed_sentences(full_sentences, full_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b90d13ab89bf10f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:38:41.221576416Z",
     "start_time": "2023-12-18T15:38:41.051727734Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 0/1558 loss: 7.897522305635123\n",
      "epoch: 0 sentence: 10/1558 loss: 7.881285444011558\n",
      "epoch: 0 sentence: 20/1558 loss: 7.894268054175027\n",
      "epoch: 0 sentence: 30/1558 loss: 7.9014236352812315\n",
      "epoch: 0 sentence: 40/1558 loss: 7.885451681302905\n",
      "epoch: 0 sentence: 50/1558 loss: 7.850217431299732\n",
      "epoch: 0 sentence: 60/1558 loss: 7.888500423235992\n",
      "epoch: 0 sentence: 70/1558 loss: 7.8958181550691915\n",
      "epoch: 0 sentence: 80/1558 loss: 7.898439318337972\n",
      "epoch: 0 sentence: 90/1558 loss: 7.886170716644597\n",
      "epoch: 0 sentence: 100/1558 loss: 7.9001266550996965\n",
      "epoch: 0 sentence: 110/1558 loss: 7.897777072212502\n",
      "epoch: 0 sentence: 120/1558 loss: 7.874714513968414\n",
      "epoch: 0 sentence: 130/1558 loss: 7.895134820084948\n",
      "epoch: 0 sentence: 140/1558 loss: 7.88157574950131\n",
      "epoch: 0 sentence: 150/1558 loss: 7.9002320783536275\n",
      "epoch: 0 sentence: 160/1558 loss: 7.879115143252103\n",
      "epoch: 0 sentence: 170/1558 loss: 7.897435626651721\n",
      "epoch: 0 sentence: 180/1558 loss: 7.880925049921342\n",
      "epoch: 0 sentence: 190/1558 loss: 7.883889834147675\n",
      "epoch: 0 sentence: 200/1558 loss: 7.856816467383628\n",
      "epoch: 0 sentence: 210/1558 loss: 7.881033718608655\n",
      "epoch: 0 sentence: 220/1558 loss: 7.883479350693438\n",
      "epoch: 0 sentence: 230/1558 loss: 7.888316788480739\n",
      "epoch: 0 sentence: 240/1558 loss: 7.839223159385199\n",
      "epoch: 0 sentence: 250/1558 loss: 7.868812112220604\n",
      "epoch: 0 sentence: 260/1558 loss: 7.867139849895932\n",
      "epoch: 0 sentence: 270/1558 loss: 7.880085541003567\n",
      "epoch: 0 sentence: 280/1558 loss: 7.876578286278124\n",
      "epoch: 0 sentence: 290/1558 loss: 7.849184114475819\n",
      "epoch: 0 sentence: 300/1558 loss: 7.888984542813706\n",
      "epoch: 0 sentence: 310/1558 loss: 7.900067632126459\n",
      "epoch: 0 sentence: 320/1558 loss: 7.830111848319549\n",
      "epoch: 0 sentence: 330/1558 loss: 7.865408931386386\n",
      "epoch: 0 sentence: 340/1558 loss: 7.8924417860243405\n",
      "epoch: 0 sentence: 350/1558 loss: 7.873145614862478\n",
      "epoch: 0 sentence: 360/1558 loss: 7.898054587856964\n",
      "epoch: 0 sentence: 370/1558 loss: 7.84147172434376\n",
      "epoch: 0 sentence: 380/1558 loss: 7.848854860211092\n",
      "epoch: 0 sentence: 390/1558 loss: 7.871999704491175\n",
      "epoch: 0 sentence: 400/1558 loss: 7.875817323774255\n",
      "epoch: 0 sentence: 410/1558 loss: 7.829394625912576\n",
      "epoch: 0 sentence: 420/1558 loss: 7.874698962682061\n",
      "epoch: 0 sentence: 430/1558 loss: 7.863630755593621\n",
      "epoch: 0 sentence: 440/1558 loss: 7.883177666478442\n",
      "epoch: 0 sentence: 450/1558 loss: 7.8754335915475\n",
      "epoch: 0 sentence: 460/1558 loss: 7.848482915596636\n",
      "epoch: 0 sentence: 470/1558 loss: 7.844126529725877\n",
      "epoch: 0 sentence: 480/1558 loss: 7.5823419307701245\n",
      "epoch: 0 sentence: 490/1558 loss: 7.88898484634627\n",
      "epoch: 0 sentence: 500/1558 loss: 7.875776910809234\n",
      "epoch: 0 sentence: 510/1558 loss: 7.789222361619881\n",
      "epoch: 0 sentence: 520/1558 loss: 7.897150941172212\n",
      "epoch: 0 sentence: 530/1558 loss: 7.809842828307078\n",
      "epoch: 0 sentence: 540/1558 loss: 7.883535667676212\n",
      "epoch: 0 sentence: 550/1558 loss: 7.849024787933393\n",
      "epoch: 0 sentence: 560/1558 loss: 7.882279962558103\n",
      "epoch: 0 sentence: 570/1558 loss: 7.865761653887672\n",
      "epoch: 0 sentence: 580/1558 loss: 7.833471730087719\n",
      "epoch: 0 sentence: 590/1558 loss: 7.885960377372404\n",
      "epoch: 0 sentence: 600/1558 loss: 7.796236907899021\n",
      "epoch: 0 sentence: 610/1558 loss: 7.795965536705401\n",
      "epoch: 0 sentence: 620/1558 loss: 7.823194582747126\n",
      "epoch: 0 sentence: 630/1558 loss: 7.866268780712305\n",
      "epoch: 0 sentence: 640/1558 loss: 7.838419359727387\n",
      "epoch: 0 sentence: 650/1558 loss: 7.743852503867271\n",
      "epoch: 0 sentence: 660/1558 loss: 7.816356810379455\n",
      "epoch: 0 sentence: 670/1558 loss: 7.865968039103034\n",
      "epoch: 0 sentence: 680/1558 loss: 7.83667242038573\n",
      "epoch: 0 sentence: 690/1558 loss: 7.83012236881788\n",
      "epoch: 0 sentence: 700/1558 loss: 7.839351315199698\n",
      "epoch: 0 sentence: 710/1558 loss: 7.863263498965019\n",
      "epoch: 0 sentence: 720/1558 loss: 7.675301920873356\n",
      "epoch: 0 sentence: 730/1558 loss: 7.755503456338618\n",
      "epoch: 0 sentence: 740/1558 loss: 7.8545988075248205\n",
      "epoch: 0 sentence: 750/1558 loss: 7.826387816114259\n",
      "epoch: 0 sentence: 760/1558 loss: 7.825654242192527\n",
      "epoch: 0 sentence: 770/1558 loss: 7.374281090724847\n",
      "epoch: 0 sentence: 780/1558 loss: 7.800322016178828\n",
      "epoch: 0 sentence: 790/1558 loss: 7.829360734151883\n",
      "epoch: 0 sentence: 800/1558 loss: 7.78605045891421\n",
      "epoch: 0 sentence: 810/1558 loss: 7.702344511368267\n",
      "epoch: 0 sentence: 820/1558 loss: 7.83806802226316\n",
      "epoch: 0 sentence: 830/1558 loss: 7.809886002559224\n",
      "epoch: 0 sentence: 840/1558 loss: 7.7827820102047935\n",
      "epoch: 0 sentence: 850/1558 loss: 7.334887824533313\n",
      "epoch: 0 sentence: 860/1558 loss: 7.86143975206374\n",
      "epoch: 0 sentence: 870/1558 loss: 7.768486567768018\n",
      "epoch: 0 sentence: 880/1558 loss: 7.808293306895486\n",
      "epoch: 0 sentence: 890/1558 loss: 7.884200767424577\n",
      "epoch: 0 sentence: 900/1558 loss: 7.840282085991901\n",
      "epoch: 0 sentence: 910/1558 loss: 7.814838030202487\n",
      "epoch: 0 sentence: 920/1558 loss: 7.743342687275549\n",
      "epoch: 0 sentence: 930/1558 loss: 7.738333960888788\n",
      "epoch: 0 sentence: 940/1558 loss: 7.809681500587154\n",
      "epoch: 0 sentence: 950/1558 loss: 7.8435982478489485\n",
      "epoch: 0 sentence: 960/1558 loss: 7.789917756106697\n",
      "epoch: 0 sentence: 970/1558 loss: 7.235916534061662\n",
      "epoch: 0 sentence: 980/1558 loss: 7.84670729794151\n",
      "epoch: 0 sentence: 990/1558 loss: 7.871634225221067\n",
      "epoch: 0 sentence: 1000/1558 loss: 7.707128634977039\n",
      "epoch: 0 sentence: 1010/1558 loss: 7.803377843365631\n",
      "epoch: 0 sentence: 1020/1558 loss: 7.8953864787821395\n",
      "epoch: 0 sentence: 1030/1558 loss: 7.833886448151816\n",
      "epoch: 0 sentence: 1040/1558 loss: 7.8995750449356885\n",
      "epoch: 0 sentence: 1050/1558 loss: 7.6925283903321775\n",
      "epoch: 0 sentence: 1060/1558 loss: 7.794668335929695\n",
      "epoch: 0 sentence: 1070/1558 loss: 7.809200193534728\n",
      "epoch: 0 sentence: 1080/1558 loss: 7.788063547722768\n",
      "epoch: 0 sentence: 1090/1558 loss: 7.842155980286232\n",
      "epoch: 0 sentence: 1100/1558 loss: 7.818639732431106\n",
      "epoch: 0 sentence: 1110/1558 loss: 7.890405660438471\n",
      "epoch: 0 sentence: 1120/1558 loss: 7.797727859130959\n",
      "epoch: 0 sentence: 1130/1558 loss: 7.781936483533518\n",
      "epoch: 0 sentence: 1140/1558 loss: 7.711749991260447\n",
      "epoch: 0 sentence: 1150/1558 loss: 7.866316208114733\n",
      "epoch: 0 sentence: 1160/1558 loss: 7.709765866703105\n",
      "epoch: 0 sentence: 1170/1558 loss: 7.54840797305385\n",
      "epoch: 0 sentence: 1180/1558 loss: 7.097926059011281\n",
      "epoch: 0 sentence: 1190/1558 loss: 7.846272707882934\n",
      "epoch: 0 sentence: 1200/1558 loss: 7.800771621583965\n",
      "epoch: 0 sentence: 1210/1558 loss: 7.07823096137255\n",
      "epoch: 0 sentence: 1220/1558 loss: 7.766609696926156\n",
      "epoch: 0 sentence: 1230/1558 loss: 7.743968385084562\n",
      "epoch: 0 sentence: 1240/1558 loss: 7.828436575276686\n",
      "epoch: 0 sentence: 1250/1558 loss: 7.809796410770317\n",
      "epoch: 0 sentence: 1260/1558 loss: 7.450435878457825\n",
      "epoch: 0 sentence: 1270/1558 loss: 7.687903316489692\n",
      "epoch: 0 sentence: 1280/1558 loss: 7.819277857902937\n",
      "epoch: 0 sentence: 1290/1558 loss: 7.50594772527254\n",
      "epoch: 0 sentence: 1300/1558 loss: 7.822153927117816\n",
      "epoch: 0 sentence: 1310/1558 loss: 7.67774541402562\n",
      "epoch: 0 sentence: 1320/1558 loss: 7.825580338800696\n",
      "epoch: 0 sentence: 1330/1558 loss: 7.787327295730029\n",
      "epoch: 0 sentence: 1340/1558 loss: 7.689861484743237\n",
      "epoch: 0 sentence: 1350/1558 loss: 7.691068256744553\n",
      "epoch: 0 sentence: 1360/1558 loss: 7.806184451951579\n",
      "epoch: 0 sentence: 1370/1558 loss: 7.793845627433472\n",
      "epoch: 0 sentence: 1380/1558 loss: 7.737193770461243\n",
      "epoch: 0 sentence: 1390/1558 loss: 7.870809509325656\n",
      "epoch: 0 sentence: 1400/1558 loss: 7.846461111617362\n",
      "epoch: 0 sentence: 1410/1558 loss: 7.6976353782024605\n",
      "epoch: 0 sentence: 1420/1558 loss: 7.256638468339614\n",
      "epoch: 0 sentence: 1430/1558 loss: 7.739190926686869\n",
      "epoch: 0 sentence: 1440/1558 loss: 6.9109088463799955\n",
      "epoch: 0 sentence: 1450/1558 loss: 7.783369594585448\n",
      "epoch: 0 sentence: 1460/1558 loss: 7.78956728457315\n",
      "epoch: 0 sentence: 1470/1558 loss: 7.547943683790011\n",
      "epoch: 0 sentence: 1480/1558 loss: 7.830759904301136\n",
      "epoch: 0 sentence: 1490/1558 loss: 7.752702175469421\n",
      "epoch: 0 sentence: 1500/1558 loss: 7.874993619762908\n",
      "epoch: 0 sentence: 1510/1558 loss: 7.819705587655126\n",
      "epoch: 0 sentence: 1520/1558 loss: 7.875903129483327\n",
      "epoch: 0 sentence: 1530/1558 loss: 6.822177488006659\n",
      "epoch: 0 sentence: 1540/1558 loss: 7.7811582652154465\n",
      "epoch: 0 sentence: 1550/1558 loss: 7.799125066835393\n",
      "epoch: 1 sentence: 0/1558 loss: 7.83606485279361\n",
      "epoch: 1 sentence: 10/1558 loss: 7.774894988144592\n",
      "epoch: 1 sentence: 20/1558 loss: 7.852886379097397\n",
      "epoch: 1 sentence: 30/1558 loss: 7.876048780928456\n",
      "epoch: 1 sentence: 40/1558 loss: 7.75523139028765\n",
      "epoch: 1 sentence: 50/1558 loss: 7.78929720210314\n",
      "epoch: 1 sentence: 60/1558 loss: 7.825863215217377\n",
      "epoch: 1 sentence: 70/1558 loss: 7.771278198517021\n",
      "epoch: 1 sentence: 80/1558 loss: 7.855259573391895\n",
      "epoch: 1 sentence: 90/1558 loss: 7.754176720713689\n",
      "epoch: 1 sentence: 100/1558 loss: 7.543522276734689\n",
      "epoch: 1 sentence: 110/1558 loss: 7.799277667127092\n",
      "epoch: 1 sentence: 120/1558 loss: 7.45302209499599\n",
      "epoch: 1 sentence: 130/1558 loss: 7.785314306178772\n",
      "epoch: 1 sentence: 140/1558 loss: 7.7416060163188325\n",
      "epoch: 1 sentence: 150/1558 loss: 6.694998516900108\n",
      "epoch: 1 sentence: 160/1558 loss: 7.693624945953255\n",
      "epoch: 1 sentence: 170/1558 loss: 7.650673273042306\n",
      "epoch: 1 sentence: 180/1558 loss: 7.848094013546475\n",
      "epoch: 1 sentence: 190/1558 loss: 7.825041826301954\n",
      "epoch: 1 sentence: 200/1558 loss: 7.821796556428968\n",
      "epoch: 1 sentence: 210/1558 loss: 7.709456341236987\n",
      "epoch: 1 sentence: 220/1558 loss: 7.848764986347859\n",
      "epoch: 1 sentence: 230/1558 loss: 7.799813450603126\n",
      "epoch: 1 sentence: 240/1558 loss: 7.835357615161601\n",
      "epoch: 1 sentence: 250/1558 loss: 7.747908819141346\n",
      "epoch: 1 sentence: 260/1558 loss: 7.747655305307927\n",
      "epoch: 1 sentence: 270/1558 loss: 7.604052746793017\n",
      "epoch: 1 sentence: 280/1558 loss: 7.8090424709702075\n",
      "epoch: 1 sentence: 290/1558 loss: 7.869653079119402\n",
      "epoch: 1 sentence: 300/1558 loss: 6.56757756033193\n",
      "epoch: 1 sentence: 310/1558 loss: 7.78231289319424\n",
      "epoch: 1 sentence: 320/1558 loss: 7.810959043626883\n",
      "epoch: 1 sentence: 330/1558 loss: 6.548111059282499\n",
      "epoch: 1 sentence: 340/1558 loss: 6.538303748361285\n",
      "epoch: 1 sentence: 350/1558 loss: 6.52849543232607\n",
      "epoch: 1 sentence: 360/1558 loss: 7.876651472543333\n",
      "epoch: 1 sentence: 370/1558 loss: 7.861150450103815\n",
      "epoch: 1 sentence: 380/1558 loss: 7.633595262452815\n",
      "epoch: 1 sentence: 390/1558 loss: 7.321024521134881\n",
      "epoch: 1 sentence: 400/1558 loss: 7.748129411704915\n",
      "epoch: 1 sentence: 410/1558 loss: 7.826499660913273\n",
      "epoch: 1 sentence: 420/1558 loss: 7.681095089039205\n",
      "epoch: 1 sentence: 430/1558 loss: 7.7615549343620165\n",
      "epoch: 1 sentence: 440/1558 loss: 7.76183133345698\n",
      "epoch: 1 sentence: 450/1558 loss: 7.846758365729509\n",
      "epoch: 1 sentence: 460/1558 loss: 7.728197081359399\n",
      "epoch: 1 sentence: 470/1558 loss: 7.762252537471535\n",
      "epoch: 1 sentence: 480/1558 loss: 7.687765294275527\n",
      "epoch: 1 sentence: 490/1558 loss: 6.401381030606648\n",
      "epoch: 1 sentence: 500/1558 loss: 7.692486369395908\n",
      "epoch: 1 sentence: 510/1558 loss: 7.710552276591272\n",
      "epoch: 1 sentence: 520/1558 loss: 7.791220979547103\n",
      "epoch: 1 sentence: 530/1558 loss: 7.81216396714902\n",
      "epoch: 1 sentence: 540/1558 loss: 7.536057011622256\n",
      "epoch: 1 sentence: 550/1558 loss: 7.699705060189926\n",
      "epoch: 1 sentence: 560/1558 loss: 7.7301415001699025\n",
      "epoch: 1 sentence: 570/1558 loss: 7.5894754856341375\n",
      "epoch: 1 sentence: 580/1558 loss: 7.749526791814259\n",
      "epoch: 1 sentence: 590/1558 loss: 7.792873342707527\n",
      "epoch: 1 sentence: 600/1558 loss: 7.827479580520475\n",
      "epoch: 1 sentence: 610/1558 loss: 7.692365583680893\n",
      "epoch: 1 sentence: 620/1558 loss: 7.77345559286689\n",
      "epoch: 1 sentence: 630/1558 loss: 7.380916476039012\n",
      "epoch: 1 sentence: 640/1558 loss: 7.69891697704126\n",
      "epoch: 1 sentence: 650/1558 loss: 7.834150358723235\n",
      "epoch: 1 sentence: 660/1558 loss: 7.6904380923882565\n",
      "epoch: 1 sentence: 670/1558 loss: 6.903012129419015\n",
      "epoch: 1 sentence: 680/1558 loss: 7.763021526674429\n",
      "epoch: 1 sentence: 690/1558 loss: 6.276245591868075\n",
      "epoch: 1 sentence: 700/1558 loss: 6.246542088624183\n",
      "epoch: 1 sentence: 710/1558 loss: 6.226813491568614\n",
      "epoch: 1 sentence: 720/1558 loss: 7.816291953074618\n",
      "epoch: 1 sentence: 730/1558 loss: 7.563938927928034\n",
      "epoch: 1 sentence: 740/1558 loss: 7.819385844973588\n",
      "epoch: 1 sentence: 750/1558 loss: 7.829492195796867\n",
      "epoch: 1 sentence: 760/1558 loss: 7.843399781992125\n",
      "epoch: 1 sentence: 770/1558 loss: 7.736048454567515\n",
      "epoch: 1 sentence: 780/1558 loss: 7.144069039383818\n",
      "epoch: 1 sentence: 790/1558 loss: 7.672844863379151\n",
      "epoch: 1 sentence: 800/1558 loss: 7.767958039751778\n",
      "epoch: 1 sentence: 810/1558 loss: 7.582310026562772\n",
      "epoch: 1 sentence: 820/1558 loss: 7.736277135939087\n",
      "epoch: 1 sentence: 830/1558 loss: 7.519308979930585\n",
      "epoch: 1 sentence: 840/1558 loss: 7.691566043379031\n",
      "epoch: 1 sentence: 850/1558 loss: 7.621352015300831\n",
      "epoch: 1 sentence: 860/1558 loss: 7.733967362790142\n",
      "epoch: 1 sentence: 870/1558 loss: 7.848827344421132\n",
      "epoch: 1 sentence: 880/1558 loss: 7.759242283685276\n",
      "epoch: 1 sentence: 890/1558 loss: 7.631135625817641\n",
      "epoch: 1 sentence: 900/1558 loss: 7.5146996177749\n",
      "epoch: 1 sentence: 910/1558 loss: 7.652678899536155\n",
      "epoch: 1 sentence: 920/1558 loss: 7.720929252018792\n",
      "epoch: 1 sentence: 930/1558 loss: 7.674592876581747\n",
      "epoch: 1 sentence: 940/1558 loss: 6.06365332855798\n",
      "epoch: 1 sentence: 950/1558 loss: 7.845033258150187\n",
      "epoch: 1 sentence: 960/1558 loss: 7.6760506361940015\n",
      "epoch: 1 sentence: 970/1558 loss: 7.599051552871491\n",
      "epoch: 1 sentence: 980/1558 loss: 7.6445476073655625\n",
      "epoch: 1 sentence: 990/1558 loss: 7.240867196732887\n",
      "epoch: 1 sentence: 1000/1558 loss: 7.734030717361078\n",
      "epoch: 1 sentence: 1010/1558 loss: 7.746483708363648\n",
      "epoch: 1 sentence: 1020/1558 loss: 7.592307866573004\n",
      "epoch: 1 sentence: 1030/1558 loss: 7.691585779532395\n",
      "epoch: 1 sentence: 1040/1558 loss: 7.691899072125198\n",
      "epoch: 1 sentence: 1050/1558 loss: 7.750581523755078\n",
      "epoch: 1 sentence: 1060/1558 loss: 7.576138545747521\n",
      "epoch: 1 sentence: 1070/1558 loss: 7.787578378555205\n",
      "epoch: 1 sentence: 1080/1558 loss: 7.390157782813702\n",
      "epoch: 1 sentence: 1090/1558 loss: 6.018640745063533\n",
      "epoch: 1 sentence: 1100/1558 loss: 7.740447415305912\n",
      "epoch: 1 sentence: 1110/1558 loss: 7.607344765856612\n",
      "epoch: 1 sentence: 1120/1558 loss: 7.720758359034802\n",
      "epoch: 1 sentence: 1130/1558 loss: 7.787121982306193\n",
      "epoch: 1 sentence: 1140/1558 loss: 7.168779887431675\n",
      "epoch: 1 sentence: 1150/1558 loss: 7.2944687324271955\n",
      "epoch: 1 sentence: 1160/1558 loss: 7.5818660724958304\n",
      "epoch: 1 sentence: 1170/1558 loss: 7.679512712386849\n",
      "epoch: 1 sentence: 1180/1558 loss: 7.7584424565384476\n",
      "epoch: 1 sentence: 1190/1558 loss: 7.380095630655423\n",
      "epoch: 1 sentence: 1200/1558 loss: 7.456402724199097\n",
      "epoch: 1 sentence: 1210/1558 loss: 7.448971265398694\n",
      "epoch: 1 sentence: 1220/1558 loss: 7.684889859071004\n",
      "epoch: 1 sentence: 1230/1558 loss: 7.552937594687375\n",
      "epoch: 1 sentence: 1240/1558 loss: 7.728263599159\n",
      "epoch: 1 sentence: 1250/1558 loss: 7.821709334039668\n",
      "epoch: 1 sentence: 1260/1558 loss: 7.654431939935718\n",
      "epoch: 1 sentence: 1270/1558 loss: 7.725961184142569\n",
      "epoch: 1 sentence: 1280/1558 loss: 7.83134570796008\n",
      "epoch: 1 sentence: 1290/1558 loss: 7.8755510970195095\n",
      "epoch: 1 sentence: 1300/1558 loss: 7.648282646199352\n",
      "epoch: 1 sentence: 1310/1558 loss: 7.597116023913007\n",
      "epoch: 1 sentence: 1320/1558 loss: 7.401196513867319\n",
      "epoch: 1 sentence: 1330/1558 loss: 7.679499119411225\n",
      "epoch: 1 sentence: 1340/1558 loss: 7.648340348034548\n",
      "epoch: 1 sentence: 1350/1558 loss: 7.501121507174535\n",
      "epoch: 1 sentence: 1360/1558 loss: 7.30329955626346\n",
      "epoch: 1 sentence: 1370/1558 loss: 7.802496922474739\n",
      "epoch: 1 sentence: 1380/1558 loss: 7.259488881766197\n",
      "epoch: 1 sentence: 1390/1558 loss: 7.75221236069297\n",
      "epoch: 1 sentence: 1400/1558 loss: 7.376593831722761\n",
      "epoch: 1 sentence: 1410/1558 loss: 7.677627297500463\n",
      "epoch: 1 sentence: 1420/1558 loss: 7.851741755946517\n",
      "epoch: 1 sentence: 1430/1558 loss: 7.624018361133539\n",
      "epoch: 1 sentence: 1440/1558 loss: 7.40413187998542\n",
      "epoch: 1 sentence: 1450/1558 loss: 7.590875566836364\n",
      "epoch: 1 sentence: 1460/1558 loss: 7.751504789823483\n",
      "epoch: 1 sentence: 1470/1558 loss: 7.77710707530321\n",
      "epoch: 1 sentence: 1480/1558 loss: 7.594009275211625\n",
      "epoch: 1 sentence: 1490/1558 loss: 7.8614590496945596\n",
      "epoch: 1 sentence: 1500/1558 loss: 7.198073693358886\n",
      "epoch: 1 sentence: 1510/1558 loss: 7.764157243151494\n",
      "epoch: 1 sentence: 1520/1558 loss: 7.716965718285185\n",
      "epoch: 1 sentence: 1530/1558 loss: 7.514696577875005\n",
      "epoch: 1 sentence: 1540/1558 loss: 7.54684295908173\n",
      "epoch: 1 sentence: 1550/1558 loss: 7.5168987084654795\n",
      "epoch: 2 sentence: 0/1558 loss: 7.2926812786143\n",
      "epoch: 2 sentence: 10/1558 loss: 7.737730491596852\n",
      "epoch: 2 sentence: 20/1558 loss: 7.651940187579448\n",
      "epoch: 2 sentence: 30/1558 loss: 5.707935760888095\n",
      "epoch: 2 sentence: 40/1558 loss: 7.589613993383023\n",
      "epoch: 2 sentence: 50/1558 loss: 7.640101725393942\n",
      "epoch: 2 sentence: 60/1558 loss: 7.619409344626623\n",
      "epoch: 2 sentence: 70/1558 loss: 7.715176128638037\n",
      "epoch: 2 sentence: 80/1558 loss: 7.828734118646426\n",
      "epoch: 2 sentence: 90/1558 loss: 7.346293190290308\n",
      "epoch: 2 sentence: 100/1558 loss: 7.769679129309049\n",
      "epoch: 2 sentence: 110/1558 loss: 7.781458495203428\n",
      "epoch: 2 sentence: 120/1558 loss: 7.688982034320638\n",
      "epoch: 2 sentence: 130/1558 loss: 7.168954817701577\n",
      "epoch: 2 sentence: 140/1558 loss: 7.14463434387144\n",
      "epoch: 2 sentence: 150/1558 loss: 7.787350161070759\n",
      "epoch: 2 sentence: 160/1558 loss: 7.850798177363838\n",
      "epoch: 2 sentence: 170/1558 loss: 7.767539549115706\n",
      "epoch: 2 sentence: 180/1558 loss: 7.186284976921947\n",
      "epoch: 2 sentence: 190/1558 loss: 7.232611623049678\n",
      "epoch: 2 sentence: 200/1558 loss: 7.596876030223774\n",
      "epoch: 2 sentence: 210/1558 loss: 7.478435584730249\n",
      "epoch: 2 sentence: 220/1558 loss: 7.599944212759787\n",
      "epoch: 2 sentence: 230/1558 loss: 7.760430093000678\n",
      "epoch: 2 sentence: 240/1558 loss: 7.6348981920262595\n",
      "epoch: 2 sentence: 250/1558 loss: 7.446583456304821\n",
      "epoch: 2 sentence: 260/1558 loss: 6.802922930277986\n",
      "epoch: 2 sentence: 270/1558 loss: 7.058236979731314\n",
      "epoch: 2 sentence: 280/1558 loss: 7.606095623314197\n",
      "epoch: 2 sentence: 290/1558 loss: 6.883924170978449\n",
      "epoch: 2 sentence: 300/1558 loss: 7.366621177668194\n",
      "epoch: 2 sentence: 310/1558 loss: 7.80600879864263\n",
      "epoch: 2 sentence: 320/1558 loss: 7.650948141213047\n",
      "epoch: 2 sentence: 330/1558 loss: 7.286208783904731\n",
      "epoch: 2 sentence: 340/1558 loss: 7.516205220088828\n",
      "epoch: 2 sentence: 350/1558 loss: 5.465223641856685\n",
      "epoch: 2 sentence: 360/1558 loss: 7.5003361993364805\n",
      "epoch: 2 sentence: 370/1558 loss: 7.72332615050794\n",
      "epoch: 2 sentence: 380/1558 loss: 7.848212271524428\n",
      "epoch: 2 sentence: 390/1558 loss: 7.100018800756154\n",
      "epoch: 2 sentence: 400/1558 loss: 6.9267238523057415\n",
      "epoch: 2 sentence: 410/1558 loss: 7.735297609219199\n",
      "epoch: 2 sentence: 420/1558 loss: 7.504130593818036\n",
      "epoch: 2 sentence: 430/1558 loss: 7.660189162094423\n",
      "epoch: 2 sentence: 440/1558 loss: 7.625492022982432\n",
      "epoch: 2 sentence: 450/1558 loss: 7.555232858179805\n",
      "epoch: 2 sentence: 460/1558 loss: 7.67211068886794\n",
      "epoch: 2 sentence: 470/1558 loss: 7.738561482771122\n",
      "epoch: 2 sentence: 480/1558 loss: 7.676922807739181\n",
      "epoch: 2 sentence: 490/1558 loss: 7.008894184765774\n",
      "epoch: 2 sentence: 500/1558 loss: 7.425341029321674\n",
      "epoch: 2 sentence: 510/1558 loss: 7.131302678895025\n",
      "epoch: 2 sentence: 520/1558 loss: 6.373726396143137\n",
      "epoch: 2 sentence: 530/1558 loss: 7.107010116724466\n",
      "epoch: 2 sentence: 540/1558 loss: 7.412391014863943\n",
      "epoch: 2 sentence: 550/1558 loss: 7.440463069583333\n",
      "epoch: 2 sentence: 560/1558 loss: 7.740353944163343\n",
      "epoch: 2 sentence: 570/1558 loss: 7.481287268559242\n",
      "epoch: 2 sentence: 580/1558 loss: 7.600200735953546\n",
      "epoch: 2 sentence: 590/1558 loss: 7.573403063027763\n",
      "epoch: 2 sentence: 600/1558 loss: 7.795573886545638\n",
      "epoch: 2 sentence: 610/1558 loss: 7.619611014432354\n",
      "epoch: 2 sentence: 620/1558 loss: 7.454498540464735\n",
      "epoch: 2 sentence: 630/1558 loss: 7.745855187259077\n",
      "epoch: 2 sentence: 640/1558 loss: 7.401453849480327\n",
      "epoch: 2 sentence: 650/1558 loss: 5.305395431828535\n",
      "epoch: 2 sentence: 660/1558 loss: 7.791490144802567\n",
      "epoch: 2 sentence: 670/1558 loss: 7.5629882690864365\n",
      "epoch: 2 sentence: 680/1558 loss: 7.643877053315291\n",
      "epoch: 2 sentence: 690/1558 loss: 7.7000903915902805\n",
      "epoch: 2 sentence: 700/1558 loss: 7.678491049126532\n",
      "epoch: 2 sentence: 710/1558 loss: 7.639553819071841\n",
      "epoch: 2 sentence: 720/1558 loss: 7.718149607423658\n",
      "epoch: 2 sentence: 730/1558 loss: 7.574220482530947\n",
      "epoch: 2 sentence: 740/1558 loss: 7.394142849981048\n",
      "epoch: 2 sentence: 750/1558 loss: 7.173736318329365\n",
      "epoch: 2 sentence: 760/1558 loss: 7.602722607127924\n",
      "epoch: 2 sentence: 770/1558 loss: 7.672147406108903\n",
      "epoch: 2 sentence: 780/1558 loss: 7.607673533891725\n",
      "epoch: 2 sentence: 790/1558 loss: 7.655145082607424\n",
      "epoch: 2 sentence: 800/1558 loss: 7.288599659860964\n",
      "epoch: 2 sentence: 810/1558 loss: 7.70138117674851\n",
      "epoch: 2 sentence: 820/1558 loss: 7.257450453921903\n",
      "epoch: 2 sentence: 830/1558 loss: 7.420596732314742\n",
      "epoch: 2 sentence: 840/1558 loss: 7.528430772878027\n",
      "epoch: 2 sentence: 850/1558 loss: 7.658468008720964\n",
      "epoch: 2 sentence: 860/1558 loss: 7.70067709203795\n",
      "epoch: 2 sentence: 870/1558 loss: 7.786547958461851\n",
      "epoch: 2 sentence: 880/1558 loss: 7.226575861250298\n",
      "epoch: 2 sentence: 890/1558 loss: 5.193631104391004\n",
      "epoch: 2 sentence: 900/1558 loss: 7.524217352425541\n",
      "epoch: 2 sentence: 910/1558 loss: 7.638585568691637\n",
      "epoch: 2 sentence: 920/1558 loss: 7.433540649061673\n",
      "epoch: 2 sentence: 930/1558 loss: 7.644158394229585\n",
      "epoch: 2 sentence: 940/1558 loss: 7.298353853735973\n",
      "epoch: 2 sentence: 950/1558 loss: 7.565592911215246\n",
      "epoch: 2 sentence: 960/1558 loss: 7.412526103889993\n",
      "epoch: 2 sentence: 970/1558 loss: 7.291781373473778\n",
      "epoch: 2 sentence: 980/1558 loss: 7.519721524383085\n",
      "epoch: 2 sentence: 990/1558 loss: 7.520023867201216\n",
      "epoch: 2 sentence: 1000/1558 loss: 7.728000373297482\n",
      "epoch: 2 sentence: 1010/1558 loss: 7.535306998765745\n",
      "epoch: 2 sentence: 1020/1558 loss: 7.272881510292813\n",
      "epoch: 2 sentence: 1030/1558 loss: 7.326624645775212\n",
      "epoch: 2 sentence: 1040/1558 loss: 7.513428272659622\n",
      "epoch: 2 sentence: 1050/1558 loss: 7.464553607468284\n",
      "epoch: 2 sentence: 1060/1558 loss: 7.370569165349598\n",
      "epoch: 2 sentence: 1070/1558 loss: 7.645029945377816\n",
      "epoch: 2 sentence: 1080/1558 loss: 7.7834979302637635\n",
      "epoch: 2 sentence: 1090/1558 loss: 7.5505435810990065\n",
      "epoch: 2 sentence: 1100/1558 loss: 7.236011559989573\n",
      "epoch: 2 sentence: 1110/1558 loss: 7.695719825102431\n",
      "epoch: 2 sentence: 1120/1558 loss: 7.826942225610696\n",
      "epoch: 2 sentence: 1130/1558 loss: 7.063162671282394\n",
      "epoch: 2 sentence: 1140/1558 loss: 7.5868062960088185\n",
      "epoch: 2 sentence: 1150/1558 loss: 6.403066760435551\n",
      "epoch: 2 sentence: 1160/1558 loss: 7.373831795438983\n",
      "epoch: 2 sentence: 1170/1558 loss: 7.603281219686563\n",
      "epoch: 2 sentence: 1180/1558 loss: 7.797994856198648\n",
      "epoch: 2 sentence: 1190/1558 loss: 6.890974238528635\n",
      "epoch: 2 sentence: 1200/1558 loss: 7.618618009628977\n",
      "epoch: 2 sentence: 1210/1558 loss: 7.21334769140918\n",
      "epoch: 2 sentence: 1220/1558 loss: 7.5241678712777444\n",
      "epoch: 2 sentence: 1230/1558 loss: 7.530339920661477\n",
      "epoch: 2 sentence: 1240/1558 loss: 7.478767403855283\n",
      "epoch: 2 sentence: 1250/1558 loss: 5.005886430066465\n",
      "epoch: 2 sentence: 1260/1558 loss: 7.772921804794941\n",
      "epoch: 2 sentence: 1270/1558 loss: 7.27126005499467\n",
      "epoch: 2 sentence: 1280/1558 loss: 7.6775069396314715\n",
      "epoch: 2 sentence: 1290/1558 loss: 7.654656285267788\n",
      "epoch: 2 sentence: 1300/1558 loss: 7.438717085959774\n",
      "epoch: 2 sentence: 1310/1558 loss: 7.602146657169479\n",
      "epoch: 2 sentence: 1320/1558 loss: 7.172735765460958\n",
      "epoch: 2 sentence: 1330/1558 loss: 7.450844320706245\n",
      "epoch: 2 sentence: 1340/1558 loss: 7.345396546371178\n",
      "epoch: 2 sentence: 1350/1558 loss: 7.714659709798248\n",
      "epoch: 2 sentence: 1360/1558 loss: 7.483771302085247\n",
      "epoch: 2 sentence: 1370/1558 loss: 7.683728095299872\n",
      "epoch: 2 sentence: 1380/1558 loss: 7.44352235867988\n",
      "epoch: 2 sentence: 1390/1558 loss: 7.571790188206956\n",
      "epoch: 2 sentence: 1400/1558 loss: 7.07041941319963\n",
      "epoch: 2 sentence: 1410/1558 loss: 7.828617264827462\n",
      "epoch: 2 sentence: 1420/1558 loss: 7.431463402540004\n",
      "epoch: 2 sentence: 1430/1558 loss: 7.650091361201078\n",
      "epoch: 2 sentence: 1440/1558 loss: 4.865295492392988\n",
      "epoch: 2 sentence: 1450/1558 loss: 4.856341793334989\n",
      "epoch: 2 sentence: 1460/1558 loss: 7.628508112102815\n",
      "epoch: 2 sentence: 1470/1558 loss: 7.587028507669299\n",
      "epoch: 2 sentence: 1480/1558 loss: 6.674574663302114\n",
      "epoch: 2 sentence: 1490/1558 loss: 7.394075018815471\n",
      "epoch: 2 sentence: 1500/1558 loss: 7.256782662901969\n",
      "epoch: 2 sentence: 1510/1558 loss: 4.802888285776108\n",
      "epoch: 2 sentence: 1520/1558 loss: 7.522386216426978\n",
      "epoch: 2 sentence: 1530/1558 loss: 7.419128597462958\n",
      "epoch: 2 sentence: 1540/1558 loss: 7.625648443385016\n",
      "epoch: 2 sentence: 1550/1558 loss: 7.634444919408161\n",
      "epoch: 3 sentence: 0/1558 loss: 4.758745431347827\n",
      "epoch: 3 sentence: 10/1558 loss: 7.507826241964608\n",
      "epoch: 3 sentence: 20/1558 loss: 7.676509012475585\n",
      "epoch: 3 sentence: 30/1558 loss: 7.648880490501584\n",
      "epoch: 3 sentence: 40/1558 loss: 7.30156318479417\n",
      "epoch: 3 sentence: 50/1558 loss: 7.771122222787682\n",
      "epoch: 3 sentence: 60/1558 loss: 7.3307860363103945\n",
      "epoch: 3 sentence: 70/1558 loss: 7.1036143047106295\n",
      "epoch: 3 sentence: 80/1558 loss: 7.792843697935194\n",
      "epoch: 3 sentence: 90/1558 loss: 7.406374693066698\n",
      "epoch: 3 sentence: 100/1558 loss: 7.7419647955725965\n",
      "epoch: 3 sentence: 110/1558 loss: 7.138649017658368\n",
      "epoch: 3 sentence: 120/1558 loss: 6.9940002012141855\n",
      "epoch: 3 sentence: 130/1558 loss: 7.553354326077022\n",
      "epoch: 3 sentence: 140/1558 loss: 6.762769902183031\n",
      "epoch: 3 sentence: 150/1558 loss: 4.666814621803248\n",
      "epoch: 3 sentence: 160/1558 loss: 7.462755917514516\n",
      "epoch: 3 sentence: 170/1558 loss: 7.309103356639759\n",
      "epoch: 3 sentence: 180/1558 loss: 7.582575435351729\n",
      "epoch: 3 sentence: 190/1558 loss: 7.474957581540732\n",
      "epoch: 3 sentence: 200/1558 loss: 7.398097522236693\n",
      "epoch: 3 sentence: 210/1558 loss: 7.8772796256686055\n",
      "epoch: 3 sentence: 220/1558 loss: 7.606879377145974\n",
      "epoch: 3 sentence: 230/1558 loss: 7.322833151747268\n",
      "epoch: 3 sentence: 240/1558 loss: 7.351955081747824\n",
      "epoch: 3 sentence: 250/1558 loss: 7.630656097687797\n",
      "epoch: 3 sentence: 260/1558 loss: 7.665026798589699\n",
      "epoch: 3 sentence: 270/1558 loss: 7.083115040129133\n",
      "epoch: 3 sentence: 280/1558 loss: 7.1400196776617175\n",
      "epoch: 3 sentence: 290/1558 loss: 7.432928119860088\n",
      "epoch: 3 sentence: 300/1558 loss: 7.373081968670006\n",
      "epoch: 3 sentence: 310/1558 loss: 7.229082620700756\n",
      "epoch: 3 sentence: 320/1558 loss: 7.702373219958405\n",
      "epoch: 3 sentence: 330/1558 loss: 7.701006751460957\n",
      "epoch: 3 sentence: 340/1558 loss: 7.446883714317979\n",
      "epoch: 3 sentence: 350/1558 loss: 7.694353656883692\n",
      "epoch: 3 sentence: 360/1558 loss: 7.571161230459729\n",
      "epoch: 3 sentence: 370/1558 loss: 7.534564917761349\n",
      "epoch: 3 sentence: 380/1558 loss: 7.529552982811691\n",
      "epoch: 3 sentence: 390/1558 loss: 6.564994561580619\n",
      "epoch: 3 sentence: 400/1558 loss: 7.433904650665086\n",
      "epoch: 3 sentence: 410/1558 loss: 7.508486883283158\n",
      "epoch: 3 sentence: 420/1558 loss: 7.444015062051706\n",
      "epoch: 3 sentence: 430/1558 loss: 7.204306107226878\n",
      "epoch: 3 sentence: 440/1558 loss: 7.336703654251661\n",
      "epoch: 3 sentence: 450/1558 loss: 6.425064536382501\n",
      "epoch: 3 sentence: 460/1558 loss: 7.669670277215011\n",
      "epoch: 3 sentence: 470/1558 loss: 6.883357021531802\n",
      "epoch: 3 sentence: 480/1558 loss: 6.854735967610698\n",
      "epoch: 3 sentence: 490/1558 loss: 6.890004544692705\n",
      "epoch: 3 sentence: 500/1558 loss: 7.421834765808116\n",
      "epoch: 3 sentence: 510/1558 loss: 7.410876173113535\n",
      "epoch: 3 sentence: 520/1558 loss: 6.968799790550422\n",
      "epoch: 3 sentence: 530/1558 loss: 7.418890892099534\n",
      "epoch: 3 sentence: 540/1558 loss: 7.472858859453365\n",
      "epoch: 3 sentence: 550/1558 loss: 7.597046525592381\n",
      "epoch: 3 sentence: 560/1558 loss: 6.693232222691424\n",
      "epoch: 3 sentence: 570/1558 loss: 7.6972888385754885\n",
      "epoch: 3 sentence: 580/1558 loss: 7.558826917633917\n",
      "epoch: 3 sentence: 590/1558 loss: 7.304705188381626\n",
      "epoch: 3 sentence: 600/1558 loss: 7.246274033100723\n",
      "epoch: 3 sentence: 610/1558 loss: 7.616682186780809\n",
      "epoch: 3 sentence: 620/1558 loss: 7.406380825344087\n",
      "epoch: 3 sentence: 630/1558 loss: 7.138214341930705\n",
      "epoch: 3 sentence: 640/1558 loss: 4.379674095464269\n",
      "epoch: 3 sentence: 650/1558 loss: 7.7799773771572704\n",
      "epoch: 3 sentence: 660/1558 loss: 7.502202580771807\n",
      "epoch: 3 sentence: 670/1558 loss: 7.492102857386105\n",
      "epoch: 3 sentence: 680/1558 loss: 7.326973238849869\n",
      "epoch: 3 sentence: 690/1558 loss: 7.715472500245932\n",
      "epoch: 3 sentence: 700/1558 loss: 7.298259921929714\n",
      "epoch: 3 sentence: 710/1558 loss: 7.541208523143511\n",
      "epoch: 3 sentence: 720/1558 loss: 7.310723368269608\n",
      "epoch: 3 sentence: 730/1558 loss: 7.3072137964684885\n",
      "epoch: 3 sentence: 740/1558 loss: 7.360406236904677\n",
      "epoch: 3 sentence: 750/1558 loss: 7.473161320255789\n",
      "epoch: 3 sentence: 760/1558 loss: 7.569765479799121\n",
      "epoch: 3 sentence: 770/1558 loss: 7.001337521511887\n",
      "epoch: 3 sentence: 780/1558 loss: 6.379223592124212\n",
      "epoch: 3 sentence: 790/1558 loss: 6.65746245185454\n",
      "epoch: 3 sentence: 800/1558 loss: 7.4770687719463425\n",
      "epoch: 3 sentence: 810/1558 loss: 7.636430627068743\n",
      "epoch: 3 sentence: 820/1558 loss: 7.825305911403037\n",
      "epoch: 3 sentence: 830/1558 loss: 7.300357645500446\n",
      "epoch: 3 sentence: 840/1558 loss: 7.678607415357359\n",
      "epoch: 3 sentence: 850/1558 loss: 7.501623511078478\n",
      "epoch: 3 sentence: 860/1558 loss: 7.337706040297891\n",
      "epoch: 3 sentence: 870/1558 loss: 7.569530527696361\n",
      "epoch: 3 sentence: 880/1558 loss: 7.039150532842154\n",
      "epoch: 3 sentence: 890/1558 loss: 7.431675374340246\n",
      "epoch: 3 sentence: 900/1558 loss: 7.613850570055921\n",
      "epoch: 3 sentence: 910/1558 loss: 7.020815009665089\n",
      "epoch: 3 sentence: 920/1558 loss: 6.075590506936504\n",
      "epoch: 3 sentence: 930/1558 loss: 4.194967977673517\n",
      "epoch: 3 sentence: 940/1558 loss: 7.7160587839470125\n",
      "epoch: 3 sentence: 950/1558 loss: 7.182574431390228\n",
      "epoch: 3 sentence: 960/1558 loss: 7.45214830310716\n",
      "epoch: 3 sentence: 970/1558 loss: 7.129837249440194\n",
      "epoch: 3 sentence: 980/1558 loss: 7.119208637982795\n",
      "epoch: 3 sentence: 990/1558 loss: 7.4745336935855455\n",
      "epoch: 3 sentence: 1000/1558 loss: 7.608964727921628\n",
      "epoch: 3 sentence: 1010/1558 loss: 7.473759202193931\n",
      "epoch: 3 sentence: 1020/1558 loss: 7.404535435423711\n",
      "epoch: 3 sentence: 1030/1558 loss: 7.69876697434056\n",
      "epoch: 3 sentence: 1040/1558 loss: 7.646559817746877\n",
      "epoch: 3 sentence: 1050/1558 loss: 7.419834481255244\n",
      "epoch: 3 sentence: 1060/1558 loss: 7.749477465029706\n",
      "epoch: 3 sentence: 1070/1558 loss: 6.6452574303766125\n",
      "epoch: 3 sentence: 1080/1558 loss: 7.474763010107271\n",
      "epoch: 3 sentence: 1090/1558 loss: 7.7181044670912335\n",
      "epoch: 3 sentence: 1100/1558 loss: 7.3888957884357875\n",
      "epoch: 3 sentence: 1110/1558 loss: 7.570839190435402\n",
      "epoch: 3 sentence: 1120/1558 loss: 7.472354268961656\n",
      "epoch: 3 sentence: 1130/1558 loss: 7.572625711606879\n",
      "epoch: 3 sentence: 1140/1558 loss: 7.277939377204793\n",
      "epoch: 3 sentence: 1150/1558 loss: 7.555196982886943\n",
      "epoch: 3 sentence: 1160/1558 loss: 7.878823468599356\n",
      "epoch: 3 sentence: 1170/1558 loss: 7.808276224312673\n",
      "epoch: 3 sentence: 1180/1558 loss: 7.505110993716034\n",
      "epoch: 3 sentence: 1190/1558 loss: 7.410647882309576\n",
      "epoch: 3 sentence: 1200/1558 loss: 6.429182082506305\n",
      "epoch: 3 sentence: 1210/1558 loss: 7.621734207832516\n",
      "epoch: 3 sentence: 1220/1558 loss: 7.914291930237342\n",
      "epoch: 3 sentence: 1230/1558 loss: 7.447034006963156\n",
      "epoch: 3 sentence: 1240/1558 loss: 7.416089448647527\n",
      "epoch: 3 sentence: 1250/1558 loss: 7.19831618704622\n",
      "epoch: 3 sentence: 1260/1558 loss: 7.287287683794826\n",
      "epoch: 3 sentence: 1270/1558 loss: 7.606543856142617\n",
      "epoch: 3 sentence: 1280/1558 loss: 6.563924130084975\n",
      "epoch: 3 sentence: 1290/1558 loss: 7.763059209072089\n",
      "epoch: 3 sentence: 1300/1558 loss: 7.115973692309077\n",
      "epoch: 3 sentence: 1310/1558 loss: 7.442451957691941\n",
      "epoch: 3 sentence: 1320/1558 loss: 7.744916341424748\n",
      "epoch: 3 sentence: 1330/1558 loss: 7.590915084474175\n",
      "epoch: 3 sentence: 1340/1558 loss: 7.626405037230427\n",
      "epoch: 3 sentence: 1350/1558 loss: 7.208864411126664\n",
      "epoch: 3 sentence: 1360/1558 loss: 7.834012475333287\n",
      "epoch: 3 sentence: 1370/1558 loss: 7.563274136390883\n",
      "epoch: 3 sentence: 1380/1558 loss: 7.63331349836235\n",
      "epoch: 3 sentence: 1390/1558 loss: 7.372300235911017\n",
      "epoch: 3 sentence: 1400/1558 loss: 7.575172201750443\n",
      "epoch: 3 sentence: 1410/1558 loss: 7.675494256671132\n",
      "epoch: 3 sentence: 1420/1558 loss: 7.5307669537569275\n",
      "epoch: 3 sentence: 1430/1558 loss: 7.237674360418434\n",
      "epoch: 3 sentence: 1440/1558 loss: 7.667746751838624\n",
      "epoch: 3 sentence: 1450/1558 loss: 6.820215294674477\n",
      "epoch: 3 sentence: 1460/1558 loss: 7.492711618905332\n",
      "epoch: 3 sentence: 1470/1558 loss: 7.476566855471048\n",
      "epoch: 3 sentence: 1480/1558 loss: 7.235344705188256\n",
      "epoch: 3 sentence: 1490/1558 loss: 7.416970968937533\n",
      "epoch: 3 sentence: 1500/1558 loss: 7.247939246327558\n",
      "epoch: 3 sentence: 1510/1558 loss: 7.254997806560123\n",
      "epoch: 3 sentence: 1520/1558 loss: 7.471698508206401\n",
      "epoch: 3 sentence: 1530/1558 loss: 7.754939837811456\n",
      "epoch: 3 sentence: 1540/1558 loss: 7.520002280830586\n",
      "epoch: 3 sentence: 1550/1558 loss: 7.327277235193501\n",
      "epoch: 4 sentence: 0/1558 loss: 6.665001240503762\n",
      "epoch: 4 sentence: 10/1558 loss: 7.049808355919865\n",
      "epoch: 4 sentence: 20/1558 loss: 7.574029251160326\n",
      "epoch: 4 sentence: 30/1558 loss: 7.582482513281073\n",
      "epoch: 4 sentence: 40/1558 loss: 7.370947603495465\n",
      "epoch: 4 sentence: 50/1558 loss: 7.07526235941623\n",
      "epoch: 4 sentence: 60/1558 loss: 7.7229804119425145\n",
      "epoch: 4 sentence: 70/1558 loss: 7.0764277246132865\n",
      "epoch: 4 sentence: 80/1558 loss: 7.737327048351523\n",
      "epoch: 4 sentence: 90/1558 loss: 6.679024850807673\n",
      "epoch: 4 sentence: 100/1558 loss: 7.120165591137479\n",
      "epoch: 4 sentence: 110/1558 loss: 7.3112681786562925\n",
      "epoch: 4 sentence: 120/1558 loss: 7.630847610146862\n",
      "epoch: 4 sentence: 130/1558 loss: 7.453394047292453\n",
      "epoch: 4 sentence: 140/1558 loss: 7.856678526107334\n",
      "epoch: 4 sentence: 150/1558 loss: 7.420157244092992\n",
      "epoch: 4 sentence: 160/1558 loss: 5.673748862167216\n",
      "epoch: 4 sentence: 170/1558 loss: 7.524208223057508\n",
      "epoch: 4 sentence: 180/1558 loss: 7.397601125247112\n",
      "epoch: 4 sentence: 190/1558 loss: 7.6722453024803405\n",
      "epoch: 4 sentence: 200/1558 loss: 7.050637074737735\n",
      "epoch: 4 sentence: 210/1558 loss: 7.437452276250306\n",
      "epoch: 4 sentence: 220/1558 loss: 7.010866270607559\n",
      "epoch: 4 sentence: 230/1558 loss: 7.616287278496969\n",
      "epoch: 4 sentence: 240/1558 loss: 7.704986025785641\n",
      "epoch: 4 sentence: 250/1558 loss: 7.49164840122091\n",
      "epoch: 4 sentence: 260/1558 loss: 7.103051512238718\n",
      "epoch: 4 sentence: 270/1558 loss: 6.438451855894303\n",
      "epoch: 4 sentence: 280/1558 loss: 7.408926170417421\n",
      "epoch: 4 sentence: 290/1558 loss: 7.287303916844226\n",
      "epoch: 4 sentence: 300/1558 loss: 7.647209876201065\n",
      "epoch: 4 sentence: 310/1558 loss: 7.6638275995657\n",
      "epoch: 4 sentence: 320/1558 loss: 7.321923323221615\n",
      "epoch: 4 sentence: 330/1558 loss: 7.116191809588217\n",
      "epoch: 4 sentence: 340/1558 loss: 7.570046068336352\n",
      "epoch: 4 sentence: 350/1558 loss: 7.100984973263072\n",
      "epoch: 4 sentence: 360/1558 loss: 6.3165189073125205\n",
      "epoch: 4 sentence: 370/1558 loss: 3.8296367901930473\n",
      "epoch: 4 sentence: 380/1558 loss: 7.436346392780131\n",
      "epoch: 4 sentence: 390/1558 loss: 6.801106238898344\n",
      "epoch: 4 sentence: 400/1558 loss: 6.170971618949695\n",
      "epoch: 4 sentence: 410/1558 loss: 7.361884778707919\n",
      "epoch: 4 sentence: 420/1558 loss: 7.203211936880095\n",
      "epoch: 4 sentence: 430/1558 loss: 7.2412345428274705\n",
      "epoch: 4 sentence: 440/1558 loss: 7.224933981938865\n",
      "epoch: 4 sentence: 450/1558 loss: 6.753446005704668\n",
      "epoch: 4 sentence: 460/1558 loss: 7.576454938543325\n",
      "epoch: 4 sentence: 470/1558 loss: 7.435592838383037\n",
      "epoch: 4 sentence: 480/1558 loss: 3.7819683409182696\n",
      "epoch: 4 sentence: 490/1558 loss: 3.764129311861934\n",
      "epoch: 4 sentence: 500/1558 loss: 7.696053990316532\n",
      "epoch: 4 sentence: 510/1558 loss: 7.403636262333621\n",
      "epoch: 4 sentence: 520/1558 loss: 7.155941173744628\n",
      "epoch: 4 sentence: 530/1558 loss: 7.335172491689821\n",
      "epoch: 4 sentence: 540/1558 loss: 6.978890421343566\n",
      "epoch: 4 sentence: 550/1558 loss: 7.806707065967014\n",
      "epoch: 4 sentence: 560/1558 loss: 7.621793459983707\n",
      "epoch: 4 sentence: 570/1558 loss: 7.259723972574312\n",
      "epoch: 4 sentence: 580/1558 loss: 6.986336844234925\n",
      "epoch: 4 sentence: 590/1558 loss: 7.445720812049717\n",
      "epoch: 4 sentence: 600/1558 loss: 7.6750880643877775\n",
      "epoch: 4 sentence: 610/1558 loss: 7.728787538199699\n",
      "epoch: 4 sentence: 620/1558 loss: 7.34619210697074\n",
      "epoch: 4 sentence: 630/1558 loss: 7.53563246814125\n",
      "epoch: 4 sentence: 640/1558 loss: 6.948289579487099\n",
      "epoch: 4 sentence: 650/1558 loss: 7.277202783049066\n",
      "epoch: 4 sentence: 660/1558 loss: 7.826775209557946\n",
      "epoch: 4 sentence: 670/1558 loss: 6.956984384844214\n",
      "epoch: 4 sentence: 680/1558 loss: 7.297919149590542\n",
      "epoch: 4 sentence: 690/1558 loss: 7.270121937880776\n",
      "epoch: 4 sentence: 700/1558 loss: 7.1326562766669195\n",
      "epoch: 4 sentence: 710/1558 loss: 7.464783622247232\n",
      "epoch: 4 sentence: 720/1558 loss: 7.010807826962245\n",
      "epoch: 4 sentence: 730/1558 loss: 6.9483627914815225\n",
      "epoch: 4 sentence: 740/1558 loss: 7.409295520963374\n",
      "epoch: 4 sentence: 750/1558 loss: 7.221058211550681\n",
      "epoch: 4 sentence: 760/1558 loss: 7.436880203236826\n",
      "epoch: 4 sentence: 770/1558 loss: 7.418936064747627\n",
      "epoch: 4 sentence: 780/1558 loss: 7.279367859085243\n",
      "epoch: 4 sentence: 790/1558 loss: 7.260311305946506\n",
      "epoch: 4 sentence: 800/1558 loss: 7.630710268980075\n",
      "epoch: 4 sentence: 810/1558 loss: 7.41431455767727\n",
      "epoch: 4 sentence: 820/1558 loss: 7.068964961627664\n",
      "epoch: 4 sentence: 830/1558 loss: 7.135124958156537\n",
      "epoch: 4 sentence: 840/1558 loss: 7.168554822775628\n",
      "epoch: 4 sentence: 850/1558 loss: 7.502120267443775\n",
      "epoch: 4 sentence: 860/1558 loss: 7.551221506896886\n",
      "epoch: 4 sentence: 870/1558 loss: 7.940696242708487\n",
      "epoch: 4 sentence: 880/1558 loss: 7.54973623544835\n",
      "epoch: 4 sentence: 890/1558 loss: 3.624916509917864\n",
      "epoch: 4 sentence: 900/1558 loss: 7.287916758318957\n",
      "epoch: 4 sentence: 910/1558 loss: 6.639850867865163\n",
      "epoch: 4 sentence: 920/1558 loss: 7.3715016237977995\n",
      "epoch: 4 sentence: 930/1558 loss: 7.701085609285349\n",
      "epoch: 4 sentence: 940/1558 loss: 7.136481253913954\n",
      "epoch: 4 sentence: 950/1558 loss: 6.3817702000134435\n",
      "epoch: 4 sentence: 960/1558 loss: 7.274280396122187\n",
      "epoch: 4 sentence: 970/1558 loss: 7.310563979328973\n",
      "epoch: 4 sentence: 980/1558 loss: 7.128223269220403\n",
      "epoch: 4 sentence: 990/1558 loss: 6.657631540118061\n",
      "epoch: 4 sentence: 1000/1558 loss: 6.9175915747684895\n",
      "epoch: 4 sentence: 1010/1558 loss: 7.2560057888753935\n",
      "epoch: 4 sentence: 1020/1558 loss: 7.750633522749969\n",
      "epoch: 4 sentence: 1030/1558 loss: 7.807577200765141\n",
      "epoch: 4 sentence: 1040/1558 loss: 7.5015787829491964\n",
      "epoch: 4 sentence: 1050/1558 loss: 6.956360709099192\n",
      "epoch: 4 sentence: 1060/1558 loss: 6.961469279513071\n",
      "epoch: 4 sentence: 1070/1558 loss: 7.040102136457861\n",
      "epoch: 4 sentence: 1080/1558 loss: 6.727075579312244\n",
      "epoch: 4 sentence: 1090/1558 loss: 7.853620192923775\n",
      "epoch: 4 sentence: 1100/1558 loss: 7.673127275514325\n",
      "epoch: 4 sentence: 1110/1558 loss: 7.35818066189982\n",
      "epoch: 4 sentence: 1120/1558 loss: 7.367188769108522\n",
      "epoch: 4 sentence: 1130/1558 loss: 7.656626832818634\n",
      "epoch: 4 sentence: 1140/1558 loss: 7.693282941501491\n",
      "epoch: 4 sentence: 1150/1558 loss: 6.9707786574595225\n",
      "epoch: 4 sentence: 1160/1558 loss: 7.108940804848335\n",
      "epoch: 4 sentence: 1170/1558 loss: 6.761732341152222\n",
      "epoch: 4 sentence: 1180/1558 loss: 7.430990205278877\n",
      "epoch: 4 sentence: 1190/1558 loss: 7.590455928953166\n",
      "epoch: 4 sentence: 1200/1558 loss: 6.635945653261327\n",
      "epoch: 4 sentence: 1210/1558 loss: 7.545680517084618\n",
      "epoch: 4 sentence: 1220/1558 loss: 7.469880738778969\n",
      "epoch: 4 sentence: 1230/1558 loss: 7.5852722113638285\n",
      "epoch: 4 sentence: 1240/1558 loss: 7.508744333087402\n",
      "epoch: 4 sentence: 1250/1558 loss: 7.2016672111471705\n",
      "epoch: 4 sentence: 1260/1558 loss: 7.3001180326721\n",
      "epoch: 4 sentence: 1270/1558 loss: 7.480964879018413\n",
      "epoch: 4 sentence: 1280/1558 loss: 7.593461570131215\n",
      "epoch: 4 sentence: 1290/1558 loss: 3.4665658225572944\n",
      "epoch: 4 sentence: 1300/1558 loss: 3.4607650862532027\n",
      "epoch: 4 sentence: 1310/1558 loss: 7.918700120207044\n",
      "epoch: 4 sentence: 1320/1558 loss: 5.0147710376045955\n",
      "epoch: 4 sentence: 1330/1558 loss: 7.457739230598187\n",
      "epoch: 4 sentence: 1340/1558 loss: 7.201975971888423\n",
      "epoch: 4 sentence: 1350/1558 loss: 7.326380629338484\n",
      "epoch: 4 sentence: 1360/1558 loss: 7.479200040370419\n",
      "epoch: 4 sentence: 1370/1558 loss: 7.241493289937238\n",
      "epoch: 4 sentence: 1380/1558 loss: 7.417129519568544\n",
      "epoch: 4 sentence: 1390/1558 loss: 6.7249005958219765\n",
      "epoch: 4 sentence: 1400/1558 loss: 6.528586198233855\n",
      "epoch: 4 sentence: 1410/1558 loss: 3.4108115078313683\n",
      "epoch: 4 sentence: 1420/1558 loss: 7.449126675310376\n",
      "epoch: 4 sentence: 1430/1558 loss: 7.7796713320819455\n",
      "epoch: 4 sentence: 1440/1558 loss: 6.6578197427276855\n",
      "epoch: 4 sentence: 1450/1558 loss: 7.518762447103046\n",
      "epoch: 4 sentence: 1460/1558 loss: 3.380750645344591\n",
      "epoch: 4 sentence: 1470/1558 loss: 7.416145051175663\n",
      "epoch: 4 sentence: 1480/1558 loss: 7.709895856016394\n",
      "epoch: 4 sentence: 1490/1558 loss: 7.311534676152544\n",
      "epoch: 4 sentence: 1500/1558 loss: 3.345088998594651\n",
      "epoch: 4 sentence: 1510/1558 loss: 7.5793485890589105\n",
      "epoch: 4 sentence: 1520/1558 loss: 7.400615166593814\n",
      "epoch: 4 sentence: 1530/1558 loss: 7.376614552341889\n",
      "epoch: 4 sentence: 1540/1558 loss: 7.545651368298482\n",
      "epoch: 4 sentence: 1550/1558 loss: 7.047334911257377\n",
      "epoch: 5 sentence: 0/1558 loss: 7.1997965934144315\n",
      "epoch: 5 sentence: 10/1558 loss: 7.855681574296973\n",
      "epoch: 5 sentence: 20/1558 loss: 6.919596666984251\n",
      "epoch: 5 sentence: 30/1558 loss: 7.389119202943925\n",
      "epoch: 5 sentence: 40/1558 loss: 7.533862555638524\n",
      "epoch: 5 sentence: 50/1558 loss: 7.155707885179825\n",
      "epoch: 5 sentence: 60/1558 loss: 7.475391835313444\n",
      "epoch: 5 sentence: 70/1558 loss: 7.159618047905438\n",
      "epoch: 5 sentence: 80/1558 loss: 7.356388711693989\n",
      "epoch: 5 sentence: 90/1558 loss: 7.329666421165428\n",
      "epoch: 5 sentence: 100/1558 loss: 7.1519835984829205\n",
      "epoch: 5 sentence: 110/1558 loss: 3.3315607991411467\n",
      "epoch: 5 sentence: 120/1558 loss: 7.233365135276118\n",
      "epoch: 5 sentence: 130/1558 loss: 6.417258746112601\n",
      "epoch: 5 sentence: 140/1558 loss: 7.358867053537444\n",
      "epoch: 5 sentence: 150/1558 loss: 7.496569270965038\n",
      "epoch: 5 sentence: 160/1558 loss: 6.4696807281339\n",
      "epoch: 5 sentence: 170/1558 loss: 7.375373230426263\n",
      "epoch: 5 sentence: 180/1558 loss: 7.596145919677182\n",
      "epoch: 5 sentence: 190/1558 loss: 7.071149842612496\n",
      "epoch: 5 sentence: 200/1558 loss: 6.696948467002153\n",
      "epoch: 5 sentence: 210/1558 loss: 5.99308237220827\n",
      "epoch: 5 sentence: 220/1558 loss: 7.353081704589426\n",
      "epoch: 5 sentence: 230/1558 loss: 7.009697400001494\n",
      "epoch: 5 sentence: 240/1558 loss: 7.362698145656876\n",
      "epoch: 5 sentence: 250/1558 loss: 7.490451424538499\n",
      "epoch: 5 sentence: 260/1558 loss: 7.387408843278318\n",
      "epoch: 5 sentence: 270/1558 loss: 7.610668155890116\n",
      "epoch: 5 sentence: 280/1558 loss: 7.357525184919612\n",
      "epoch: 5 sentence: 290/1558 loss: 7.26355078279376\n",
      "epoch: 5 sentence: 300/1558 loss: 7.981522842226386\n",
      "epoch: 5 sentence: 310/1558 loss: 6.55452779050001\n",
      "epoch: 5 sentence: 320/1558 loss: 7.467779792746301\n",
      "epoch: 5 sentence: 330/1558 loss: 7.195927737934793\n",
      "epoch: 5 sentence: 340/1558 loss: 6.737471915616047\n",
      "epoch: 5 sentence: 350/1558 loss: 3.25800536549638\n",
      "epoch: 5 sentence: 360/1558 loss: 7.230834507352874\n",
      "epoch: 5 sentence: 370/1558 loss: 7.408418784930323\n",
      "epoch: 5 sentence: 380/1558 loss: 5.355923366237875\n",
      "epoch: 5 sentence: 390/1558 loss: 6.922475440368152\n",
      "epoch: 5 sentence: 400/1558 loss: 7.281128566741051\n",
      "epoch: 5 sentence: 410/1558 loss: 3.2523665569005953\n",
      "epoch: 5 sentence: 420/1558 loss: 7.373934383102354\n",
      "epoch: 5 sentence: 430/1558 loss: 7.20518214540221\n",
      "epoch: 5 sentence: 440/1558 loss: 7.399298575655749\n",
      "epoch: 5 sentence: 450/1558 loss: 7.664941605309727\n",
      "epoch: 5 sentence: 460/1558 loss: 7.315678420346325\n",
      "epoch: 5 sentence: 470/1558 loss: 7.608005094100155\n",
      "epoch: 5 sentence: 480/1558 loss: 7.422041943975888\n",
      "epoch: 5 sentence: 490/1558 loss: 7.095221419954351\n",
      "epoch: 5 sentence: 500/1558 loss: 7.258992154677786\n",
      "epoch: 5 sentence: 510/1558 loss: 7.405894163426184\n",
      "epoch: 5 sentence: 520/1558 loss: 7.691307522686538\n",
      "epoch: 5 sentence: 530/1558 loss: 7.822788782641967\n",
      "epoch: 5 sentence: 540/1558 loss: 7.4858922773331615\n",
      "epoch: 5 sentence: 550/1558 loss: 7.5922944225846045\n",
      "epoch: 5 sentence: 560/1558 loss: 7.478786980260129\n",
      "epoch: 5 sentence: 570/1558 loss: 7.288299927300728\n",
      "epoch: 5 sentence: 580/1558 loss: 6.547624107109981\n",
      "epoch: 5 sentence: 590/1558 loss: 3.155126725451828\n",
      "epoch: 5 sentence: 600/1558 loss: 7.083724462321694\n",
      "epoch: 5 sentence: 610/1558 loss: 7.398375217882137\n",
      "epoch: 5 sentence: 620/1558 loss: 7.745523696705346\n",
      "epoch: 5 sentence: 630/1558 loss: 6.9804304816701945\n",
      "epoch: 5 sentence: 640/1558 loss: 7.588937174058476\n",
      "epoch: 5 sentence: 650/1558 loss: 7.473121218583105\n",
      "epoch: 5 sentence: 660/1558 loss: 7.3458192536869\n",
      "epoch: 5 sentence: 670/1558 loss: 7.571935720326752\n",
      "epoch: 5 sentence: 680/1558 loss: 6.468489975308953\n",
      "epoch: 5 sentence: 690/1558 loss: 7.144144821594376\n",
      "epoch: 5 sentence: 700/1558 loss: 7.225567996333117\n",
      "epoch: 5 sentence: 710/1558 loss: 5.961162570724026\n",
      "epoch: 5 sentence: 720/1558 loss: 7.397687111899629\n",
      "epoch: 5 sentence: 730/1558 loss: 6.701538922146233\n",
      "epoch: 5 sentence: 740/1558 loss: 7.6692278648505825\n",
      "epoch: 5 sentence: 750/1558 loss: 7.467444373588337\n",
      "epoch: 5 sentence: 760/1558 loss: 7.461018276403209\n",
      "epoch: 5 sentence: 770/1558 loss: 6.695684963780406\n",
      "epoch: 5 sentence: 780/1558 loss: 7.4226929896371\n",
      "epoch: 5 sentence: 790/1558 loss: 7.583137463843269\n",
      "epoch: 5 sentence: 800/1558 loss: 7.540581242805533\n",
      "epoch: 5 sentence: 810/1558 loss: 7.4194300095479315\n",
      "epoch: 5 sentence: 820/1558 loss: 7.013088250157408\n",
      "epoch: 5 sentence: 830/1558 loss: 6.938567128624636\n",
      "epoch: 5 sentence: 840/1558 loss: 7.626842807727859\n",
      "epoch: 5 sentence: 850/1558 loss: 7.732206221738315\n",
      "epoch: 5 sentence: 860/1558 loss: 6.8379655063978335\n",
      "epoch: 5 sentence: 870/1558 loss: 7.505145526218585\n",
      "epoch: 5 sentence: 880/1558 loss: 3.171742246574851\n",
      "epoch: 5 sentence: 890/1558 loss: 6.813571562406953\n",
      "epoch: 5 sentence: 900/1558 loss: 7.239494870196423\n",
      "epoch: 5 sentence: 910/1558 loss: 7.490484796776674\n",
      "epoch: 5 sentence: 920/1558 loss: 6.80770301218593\n",
      "epoch: 5 sentence: 930/1558 loss: 3.1544951793133986\n",
      "epoch: 5 sentence: 940/1558 loss: 7.698488307295955\n",
      "epoch: 5 sentence: 950/1558 loss: 7.30939289594551\n",
      "epoch: 5 sentence: 960/1558 loss: 7.262483147029329\n",
      "epoch: 5 sentence: 970/1558 loss: 7.16594765807927\n",
      "epoch: 5 sentence: 980/1558 loss: 7.160286559730401\n",
      "epoch: 5 sentence: 990/1558 loss: 7.0603345232614565\n",
      "epoch: 5 sentence: 1000/1558 loss: 7.228901896274441\n",
      "epoch: 5 sentence: 1010/1558 loss: 6.8571923833901876\n",
      "epoch: 5 sentence: 1020/1558 loss: 7.710775217969999\n",
      "epoch: 5 sentence: 1030/1558 loss: 7.370800510913075\n",
      "epoch: 5 sentence: 1040/1558 loss: 7.140189861227501\n",
      "epoch: 5 sentence: 1050/1558 loss: 7.566383460173922\n",
      "epoch: 5 sentence: 1060/1558 loss: 6.914262707224995\n",
      "epoch: 5 sentence: 1070/1558 loss: 6.896287490690506\n",
      "epoch: 5 sentence: 1080/1558 loss: 7.042293944813161\n",
      "epoch: 5 sentence: 1090/1558 loss: 3.0710537877470614\n",
      "epoch: 5 sentence: 1100/1558 loss: 3.0667219547453377\n",
      "epoch: 5 sentence: 1110/1558 loss: 7.21018120196236\n",
      "epoch: 5 sentence: 1120/1558 loss: 3.058894653426191\n",
      "epoch: 5 sentence: 1130/1558 loss: 7.629504359905922\n",
      "epoch: 5 sentence: 1140/1558 loss: 7.665063051435311\n",
      "epoch: 5 sentence: 1150/1558 loss: 7.527466536278924\n",
      "epoch: 5 sentence: 1160/1558 loss: 7.399209136642205\n",
      "epoch: 5 sentence: 1170/1558 loss: 7.566575496107072\n",
      "epoch: 5 sentence: 1180/1558 loss: 6.697337219671624\n",
      "epoch: 5 sentence: 1190/1558 loss: 6.389503426439436\n",
      "epoch: 5 sentence: 1200/1558 loss: 7.429855012981293\n",
      "epoch: 5 sentence: 1210/1558 loss: 7.7047975379799825\n",
      "epoch: 5 sentence: 1220/1558 loss: 7.142864107157735\n",
      "epoch: 5 sentence: 1230/1558 loss: 6.896043051328216\n",
      "epoch: 5 sentence: 1240/1558 loss: 7.288943254921973\n",
      "epoch: 5 sentence: 1250/1558 loss: 7.2568961101820735\n",
      "epoch: 5 sentence: 1260/1558 loss: 3.0495662326144544\n",
      "epoch: 5 sentence: 1270/1558 loss: 7.231739341814838\n",
      "epoch: 5 sentence: 1280/1558 loss: 7.518396624449474\n",
      "epoch: 5 sentence: 1290/1558 loss: 7.153292203648108\n",
      "epoch: 5 sentence: 1300/1558 loss: 7.476113118106876\n",
      "epoch: 5 sentence: 1310/1558 loss: 3.0460249770255716\n",
      "epoch: 5 sentence: 1320/1558 loss: 3.0417053479320906\n",
      "epoch: 5 sentence: 1330/1558 loss: 3.0268025484046848\n",
      "epoch: 5 sentence: 1340/1558 loss: 7.468189076315812\n",
      "epoch: 5 sentence: 1350/1558 loss: 7.2976416374303135\n",
      "epoch: 5 sentence: 1360/1558 loss: 7.543276051202128\n",
      "epoch: 5 sentence: 1370/1558 loss: 7.234832732755377\n",
      "epoch: 5 sentence: 1380/1558 loss: 6.919808230517543\n",
      "epoch: 5 sentence: 1390/1558 loss: 7.484466743804499\n",
      "epoch: 5 sentence: 1400/1558 loss: 3.0057347006775323\n",
      "epoch: 5 sentence: 1410/1558 loss: 7.137435203393468\n",
      "epoch: 5 sentence: 1420/1558 loss: 7.458203044541221\n",
      "epoch: 5 sentence: 1430/1558 loss: 7.661865563504536\n",
      "epoch: 5 sentence: 1440/1558 loss: 5.930427569821452\n",
      "epoch: 5 sentence: 1450/1558 loss: 7.793475488936606\n",
      "epoch: 5 sentence: 1460/1558 loss: 7.21601626375378\n",
      "epoch: 5 sentence: 1470/1558 loss: 7.428612355554367\n",
      "epoch: 5 sentence: 1480/1558 loss: 6.85851791370003\n",
      "epoch: 5 sentence: 1490/1558 loss: 2.9830269129671936\n",
      "epoch: 5 sentence: 1500/1558 loss: 7.056298294292377\n",
      "epoch: 5 sentence: 1510/1558 loss: 6.072680995383493\n",
      "epoch: 5 sentence: 1520/1558 loss: 7.706164749101668\n",
      "epoch: 5 sentence: 1530/1558 loss: 7.7182597958533625\n",
      "epoch: 5 sentence: 1540/1558 loss: 7.269123542088536\n",
      "epoch: 5 sentence: 1550/1558 loss: 6.763077677549617\n",
      "epoch: 6 sentence: 0/1558 loss: 7.107959399176553\n",
      "epoch: 6 sentence: 10/1558 loss: 7.4208317598054485\n",
      "epoch: 6 sentence: 20/1558 loss: 7.355382541717196\n",
      "epoch: 6 sentence: 30/1558 loss: 7.6152171281064085\n",
      "epoch: 6 sentence: 40/1558 loss: 6.709568967570845\n",
      "epoch: 6 sentence: 50/1558 loss: 7.04604472792678\n",
      "epoch: 6 sentence: 60/1558 loss: 5.901964272298188\n",
      "epoch: 6 sentence: 70/1558 loss: 7.370951933252845\n",
      "epoch: 6 sentence: 80/1558 loss: 7.369197433048548\n",
      "epoch: 6 sentence: 90/1558 loss: 7.267657545808187\n",
      "epoch: 6 sentence: 100/1558 loss: 6.718745721792155\n",
      "epoch: 6 sentence: 110/1558 loss: 7.624674126425927\n",
      "epoch: 6 sentence: 120/1558 loss: 7.18369596886014\n",
      "epoch: 6 sentence: 130/1558 loss: 7.1115373588767286\n",
      "epoch: 6 sentence: 140/1558 loss: 7.376507045729572\n",
      "epoch: 6 sentence: 150/1558 loss: 6.973143484749628\n",
      "epoch: 6 sentence: 160/1558 loss: 7.425786403201718\n",
      "epoch: 6 sentence: 170/1558 loss: 7.517427712371052\n",
      "epoch: 6 sentence: 180/1558 loss: 6.385000908590817\n",
      "epoch: 6 sentence: 190/1558 loss: 7.131155650303489\n",
      "epoch: 6 sentence: 200/1558 loss: 7.412499521039409\n",
      "epoch: 6 sentence: 210/1558 loss: 6.681495055637075\n",
      "epoch: 6 sentence: 220/1558 loss: 7.655778091570077\n",
      "epoch: 6 sentence: 230/1558 loss: 6.960380020067616\n",
      "epoch: 6 sentence: 240/1558 loss: 2.9466116053934353\n",
      "epoch: 6 sentence: 250/1558 loss: 2.945663392290082\n",
      "epoch: 6 sentence: 260/1558 loss: 6.339003405601543\n",
      "epoch: 6 sentence: 270/1558 loss: 7.383010821844432\n",
      "epoch: 6 sentence: 280/1558 loss: 7.262977717810763\n",
      "epoch: 6 sentence: 290/1558 loss: 7.585303946819434\n",
      "epoch: 6 sentence: 300/1558 loss: 6.850904409339717\n",
      "epoch: 6 sentence: 310/1558 loss: 7.774020853291191\n",
      "epoch: 6 sentence: 320/1558 loss: 6.896339477002568\n",
      "epoch: 6 sentence: 330/1558 loss: 6.918561465023602\n",
      "epoch: 6 sentence: 340/1558 loss: 7.122199654144638\n",
      "epoch: 6 sentence: 350/1558 loss: 7.511810345375569\n",
      "epoch: 6 sentence: 360/1558 loss: 7.244140782659532\n",
      "epoch: 6 sentence: 370/1558 loss: 7.132168334920323\n",
      "epoch: 6 sentence: 380/1558 loss: 6.730369248189488\n",
      "epoch: 6 sentence: 390/1558 loss: 6.6017211101626945\n",
      "epoch: 6 sentence: 400/1558 loss: 7.6002448128412805\n",
      "epoch: 6 sentence: 410/1558 loss: 7.3283742592027545\n",
      "epoch: 6 sentence: 420/1558 loss: 2.9449859596952477\n",
      "epoch: 6 sentence: 430/1558 loss: 6.77747660931191\n",
      "epoch: 6 sentence: 440/1558 loss: 6.683689551210981\n",
      "epoch: 6 sentence: 450/1558 loss: 6.881156254640187\n",
      "epoch: 6 sentence: 460/1558 loss: 6.921394897233677\n",
      "epoch: 6 sentence: 470/1558 loss: 7.784678516528191\n",
      "epoch: 6 sentence: 480/1558 loss: 7.8619512763957475\n",
      "epoch: 6 sentence: 490/1558 loss: 7.356014421235857\n",
      "epoch: 6 sentence: 500/1558 loss: 6.42645468533847\n",
      "epoch: 6 sentence: 510/1558 loss: 6.894050442112353\n",
      "epoch: 6 sentence: 520/1558 loss: 7.683712096623935\n",
      "epoch: 6 sentence: 530/1558 loss: 6.791627356090498\n",
      "epoch: 6 sentence: 540/1558 loss: 7.1975752578492385\n",
      "epoch: 6 sentence: 550/1558 loss: 7.103440894611126\n",
      "epoch: 6 sentence: 560/1558 loss: 6.870146084778615\n",
      "epoch: 6 sentence: 570/1558 loss: 7.779896511581909\n",
      "epoch: 6 sentence: 580/1558 loss: 7.405797132514586\n",
      "epoch: 6 sentence: 590/1558 loss: 7.351551784625044\n",
      "epoch: 6 sentence: 600/1558 loss: 7.053595362245755\n",
      "epoch: 6 sentence: 610/1558 loss: 7.034067157279154\n",
      "epoch: 6 sentence: 620/1558 loss: 7.024375585101024\n",
      "epoch: 6 sentence: 630/1558 loss: 7.4337436106738535\n",
      "epoch: 6 sentence: 640/1558 loss: 6.8439688045462805\n",
      "epoch: 6 sentence: 650/1558 loss: 7.598435289166675\n",
      "epoch: 6 sentence: 660/1558 loss: 7.215883457160089\n",
      "epoch: 6 sentence: 670/1558 loss: 6.855711232952317\n",
      "epoch: 6 sentence: 680/1558 loss: 6.872595251064287\n",
      "epoch: 6 sentence: 690/1558 loss: 7.443921068267285\n",
      "epoch: 6 sentence: 700/1558 loss: 6.695073654709944\n",
      "epoch: 6 sentence: 710/1558 loss: 7.715542388944209\n",
      "epoch: 6 sentence: 720/1558 loss: 7.789316347437678\n",
      "epoch: 6 sentence: 730/1558 loss: 6.766793537682496\n",
      "epoch: 6 sentence: 740/1558 loss: 7.084950554314393\n",
      "epoch: 6 sentence: 750/1558 loss: 7.352557416309808\n",
      "epoch: 6 sentence: 760/1558 loss: 7.333929610402873\n",
      "epoch: 6 sentence: 770/1558 loss: 7.27292498826654\n",
      "epoch: 6 sentence: 780/1558 loss: 7.66202968367111\n",
      "epoch: 6 sentence: 790/1558 loss: 7.239737023711452\n",
      "epoch: 6 sentence: 800/1558 loss: 7.068571976649609\n",
      "epoch: 6 sentence: 810/1558 loss: 7.4741161503350195\n",
      "epoch: 6 sentence: 820/1558 loss: 7.865267365797021\n",
      "epoch: 6 sentence: 830/1558 loss: 6.998084245532123\n",
      "epoch: 6 sentence: 840/1558 loss: 7.053436938434672\n",
      "epoch: 6 sentence: 850/1558 loss: 6.981372114168872\n",
      "epoch: 6 sentence: 860/1558 loss: 7.007110714656724\n",
      "epoch: 6 sentence: 870/1558 loss: 6.715163397183062\n",
      "epoch: 6 sentence: 880/1558 loss: 6.828213642612778\n",
      "epoch: 6 sentence: 890/1558 loss: 7.076951735031675\n",
      "epoch: 6 sentence: 900/1558 loss: 7.177678887010937\n",
      "epoch: 6 sentence: 910/1558 loss: 7.333712486212243\n",
      "epoch: 6 sentence: 920/1558 loss: 7.374065989197868\n",
      "epoch: 6 sentence: 930/1558 loss: 7.226654623891867\n",
      "epoch: 6 sentence: 940/1558 loss: 6.948052349297054\n",
      "epoch: 6 sentence: 950/1558 loss: 7.355957102313611\n",
      "epoch: 6 sentence: 960/1558 loss: 7.367986276682619\n",
      "epoch: 6 sentence: 970/1558 loss: 6.836297479013527\n",
      "epoch: 6 sentence: 980/1558 loss: 7.289906234830717\n",
      "epoch: 6 sentence: 990/1558 loss: 6.964723284637451\n",
      "epoch: 6 sentence: 1000/1558 loss: 7.257058809983498\n",
      "epoch: 6 sentence: 1010/1558 loss: 7.221190717271596\n",
      "epoch: 6 sentence: 1020/1558 loss: 7.290439506324563\n",
      "epoch: 6 sentence: 1030/1558 loss: 7.961869261319431\n",
      "epoch: 6 sentence: 1040/1558 loss: 7.176875640343616\n",
      "epoch: 6 sentence: 1050/1558 loss: 7.181141673806399\n",
      "epoch: 6 sentence: 1060/1558 loss: 7.360615825360673\n",
      "epoch: 6 sentence: 1070/1558 loss: 6.497947421235387\n",
      "epoch: 6 sentence: 1080/1558 loss: 7.730479426146382\n",
      "epoch: 6 sentence: 1090/1558 loss: 7.40319051794041\n",
      "epoch: 6 sentence: 1100/1558 loss: 7.38327821350443\n",
      "epoch: 6 sentence: 1110/1558 loss: 6.609388612684244\n",
      "epoch: 6 sentence: 1120/1558 loss: 6.916914210501788\n",
      "epoch: 6 sentence: 1130/1558 loss: 7.485976941000498\n",
      "epoch: 6 sentence: 1140/1558 loss: 7.768069122508477\n",
      "epoch: 6 sentence: 1150/1558 loss: 7.60744057281305\n",
      "epoch: 6 sentence: 1160/1558 loss: 7.204755087706984\n",
      "epoch: 6 sentence: 1170/1558 loss: 7.182555198587681\n",
      "epoch: 6 sentence: 1180/1558 loss: 2.8260531678128546\n",
      "epoch: 6 sentence: 1190/1558 loss: 7.126335761102772\n",
      "epoch: 6 sentence: 1200/1558 loss: 6.369024439944253\n",
      "epoch: 6 sentence: 1210/1558 loss: 7.265087639492829\n",
      "epoch: 6 sentence: 1220/1558 loss: 6.83786407890913\n",
      "epoch: 6 sentence: 1230/1558 loss: 6.877311659179839\n",
      "epoch: 6 sentence: 1240/1558 loss: 6.932851251528837\n",
      "epoch: 6 sentence: 1250/1558 loss: 2.8458218602208483\n",
      "epoch: 6 sentence: 1260/1558 loss: 6.104241747938524\n",
      "epoch: 6 sentence: 1270/1558 loss: 7.457002673240888\n",
      "epoch: 6 sentence: 1280/1558 loss: 7.258928433409001\n",
      "epoch: 6 sentence: 1290/1558 loss: 6.670938113935336\n",
      "epoch: 6 sentence: 1300/1558 loss: 7.269910458482345\n",
      "epoch: 6 sentence: 1310/1558 loss: 6.683589247061011\n",
      "epoch: 6 sentence: 1320/1558 loss: 6.900806646248218\n",
      "epoch: 6 sentence: 1330/1558 loss: 2.830245225243932\n",
      "epoch: 6 sentence: 1340/1558 loss: 7.346315415097459\n",
      "epoch: 6 sentence: 1350/1558 loss: 7.128323582450455\n",
      "epoch: 6 sentence: 1360/1558 loss: 7.0428071404541805\n",
      "epoch: 6 sentence: 1370/1558 loss: 7.329530644925727\n",
      "epoch: 6 sentence: 1380/1558 loss: 6.222473597678207\n",
      "epoch: 6 sentence: 1390/1558 loss: 7.80834294658032\n",
      "epoch: 6 sentence: 1400/1558 loss: 7.410998226466433\n",
      "epoch: 6 sentence: 1410/1558 loss: 6.911273462320691\n",
      "epoch: 6 sentence: 1420/1558 loss: 7.480502637002687\n",
      "epoch: 6 sentence: 1430/1558 loss: 7.425665349711056\n",
      "epoch: 6 sentence: 1440/1558 loss: 7.122595205811471\n",
      "epoch: 6 sentence: 1450/1558 loss: 6.786531149215417\n",
      "epoch: 6 sentence: 1460/1558 loss: 7.453066662834453\n",
      "epoch: 6 sentence: 1470/1558 loss: 7.1388361463339765\n",
      "epoch: 6 sentence: 1480/1558 loss: 7.200922481759007\n",
      "epoch: 6 sentence: 1490/1558 loss: 6.2855718615159795\n",
      "epoch: 6 sentence: 1500/1558 loss: 7.064787320823762\n",
      "epoch: 6 sentence: 1510/1558 loss: 2.8337842067751877\n",
      "epoch: 6 sentence: 1520/1558 loss: 5.4616786828644495\n",
      "epoch: 6 sentence: 1530/1558 loss: 2.8358453723450885\n",
      "epoch: 6 sentence: 1540/1558 loss: 6.880801212570177\n",
      "epoch: 6 sentence: 1550/1558 loss: 7.1402462883471784\n",
      "epoch: 7 sentence: 0/1558 loss: 6.129895241372639\n",
      "epoch: 7 sentence: 10/1558 loss: 7.552614331278927\n",
      "epoch: 7 sentence: 20/1558 loss: 7.391809553976955\n",
      "epoch: 7 sentence: 30/1558 loss: 7.714018993848245\n",
      "epoch: 7 sentence: 40/1558 loss: 6.971417625413596\n",
      "epoch: 7 sentence: 50/1558 loss: 6.7683635835466305\n",
      "epoch: 7 sentence: 60/1558 loss: 6.851667770947525\n",
      "epoch: 7 sentence: 70/1558 loss: 6.746053420264023\n",
      "epoch: 7 sentence: 80/1558 loss: 7.119605599301844\n",
      "epoch: 7 sentence: 90/1558 loss: 7.142615240196232\n",
      "epoch: 7 sentence: 100/1558 loss: 7.228543313707031\n",
      "epoch: 7 sentence: 110/1558 loss: 7.5147565456111325\n",
      "epoch: 7 sentence: 120/1558 loss: 7.388770299521345\n",
      "epoch: 7 sentence: 130/1558 loss: 6.8421350427647605\n",
      "epoch: 7 sentence: 140/1558 loss: 6.7434388216773336\n",
      "epoch: 7 sentence: 150/1558 loss: 2.7713120516549385\n",
      "epoch: 7 sentence: 160/1558 loss: 7.163754081113903\n",
      "epoch: 7 sentence: 170/1558 loss: 7.894777300493826\n",
      "epoch: 7 sentence: 180/1558 loss: 7.798755905013671\n",
      "epoch: 7 sentence: 190/1558 loss: 7.063674154255501\n",
      "epoch: 7 sentence: 200/1558 loss: 7.415864346679425\n",
      "epoch: 7 sentence: 210/1558 loss: 7.119512211643342\n",
      "epoch: 7 sentence: 220/1558 loss: 5.774708371040606\n",
      "epoch: 7 sentence: 230/1558 loss: 6.754708851436092\n",
      "epoch: 7 sentence: 240/1558 loss: 6.754688072238425\n",
      "epoch: 7 sentence: 250/1558 loss: 2.761455466265443\n",
      "epoch: 7 sentence: 260/1558 loss: 6.606964647616796\n",
      "epoch: 7 sentence: 270/1558 loss: 7.361431554191845\n",
      "epoch: 7 sentence: 280/1558 loss: 7.177881652718408\n",
      "epoch: 7 sentence: 290/1558 loss: 6.861309320259437\n",
      "epoch: 7 sentence: 300/1558 loss: 7.2636663446028535\n",
      "epoch: 7 sentence: 310/1558 loss: 7.570266295583302\n",
      "epoch: 7 sentence: 320/1558 loss: 6.947081925811773\n",
      "epoch: 7 sentence: 330/1558 loss: 6.818262733234696\n",
      "epoch: 7 sentence: 340/1558 loss: 7.648466767359406\n",
      "epoch: 7 sentence: 350/1558 loss: 7.0944325810486335\n",
      "epoch: 7 sentence: 360/1558 loss: 7.353225669294706\n",
      "epoch: 7 sentence: 370/1558 loss: 6.107208569807946\n",
      "epoch: 7 sentence: 380/1558 loss: 7.340681152212245\n",
      "epoch: 7 sentence: 390/1558 loss: 6.163006490188928\n",
      "epoch: 7 sentence: 400/1558 loss: 2.7497809123242\n",
      "epoch: 7 sentence: 410/1558 loss: 6.925851941323611\n",
      "epoch: 7 sentence: 420/1558 loss: 7.218634950352579\n",
      "epoch: 7 sentence: 430/1558 loss: 7.072101652085628\n",
      "epoch: 7 sentence: 440/1558 loss: 7.329472558482311\n",
      "epoch: 7 sentence: 450/1558 loss: 6.988019904055258\n",
      "epoch: 7 sentence: 460/1558 loss: 7.656365629687855\n",
      "epoch: 7 sentence: 470/1558 loss: 6.060967756367902\n",
      "epoch: 7 sentence: 480/1558 loss: 7.222162032732817\n",
      "epoch: 7 sentence: 490/1558 loss: 2.743000203595593\n",
      "epoch: 7 sentence: 500/1558 loss: 6.72385910979834\n",
      "epoch: 7 sentence: 510/1558 loss: 6.766230706355976\n",
      "epoch: 7 sentence: 520/1558 loss: 7.442186661816095\n",
      "epoch: 7 sentence: 530/1558 loss: 7.507189878877418\n",
      "epoch: 7 sentence: 540/1558 loss: 6.161911115632947\n",
      "epoch: 7 sentence: 550/1558 loss: 7.130045360467737\n",
      "epoch: 7 sentence: 560/1558 loss: 5.6182585618815555\n",
      "epoch: 7 sentence: 570/1558 loss: 6.974656441298748\n",
      "epoch: 7 sentence: 580/1558 loss: 6.468362873545925\n",
      "epoch: 7 sentence: 590/1558 loss: 7.068115197154467\n",
      "epoch: 7 sentence: 600/1558 loss: 7.501251894418279\n",
      "epoch: 7 sentence: 610/1558 loss: 7.698876381876551\n",
      "epoch: 7 sentence: 620/1558 loss: 6.963246615667301\n",
      "epoch: 7 sentence: 630/1558 loss: 7.130978245183983\n",
      "epoch: 7 sentence: 640/1558 loss: 7.338447919557595\n",
      "epoch: 7 sentence: 650/1558 loss: 2.7591200896820456\n",
      "epoch: 7 sentence: 660/1558 loss: 7.263534039305067\n",
      "epoch: 7 sentence: 670/1558 loss: 7.459233137851129\n",
      "epoch: 7 sentence: 680/1558 loss: 7.3911430394857245\n",
      "epoch: 7 sentence: 690/1558 loss: 7.180207976662965\n",
      "epoch: 7 sentence: 700/1558 loss: 2.7393799843104607\n",
      "epoch: 7 sentence: 710/1558 loss: 7.477796457121236\n",
      "epoch: 7 sentence: 720/1558 loss: 6.962054411157426\n",
      "epoch: 7 sentence: 730/1558 loss: 6.392329309333142\n",
      "epoch: 7 sentence: 740/1558 loss: 7.367389613008321\n",
      "epoch: 7 sentence: 750/1558 loss: 5.115142456348279\n",
      "epoch: 7 sentence: 760/1558 loss: 7.3583879438471635\n",
      "epoch: 7 sentence: 770/1558 loss: 6.115657262990622\n",
      "epoch: 7 sentence: 780/1558 loss: 7.164443402297341\n",
      "epoch: 7 sentence: 790/1558 loss: 6.535244790033679\n",
      "epoch: 7 sentence: 800/1558 loss: 7.618576497002819\n",
      "epoch: 7 sentence: 810/1558 loss: 6.564929738116853\n",
      "epoch: 7 sentence: 820/1558 loss: 7.776886212997589\n",
      "epoch: 7 sentence: 830/1558 loss: 7.621607729014001\n",
      "epoch: 7 sentence: 840/1558 loss: 6.841670952261652\n",
      "epoch: 7 sentence: 850/1558 loss: 6.660082264468915\n",
      "epoch: 7 sentence: 860/1558 loss: 2.7000187188155627\n",
      "epoch: 7 sentence: 870/1558 loss: 7.164117368148726\n",
      "epoch: 7 sentence: 880/1558 loss: 6.991824886232549\n",
      "epoch: 7 sentence: 890/1558 loss: 2.6992645886811846\n",
      "epoch: 7 sentence: 900/1558 loss: 2.6765876252100917\n",
      "epoch: 7 sentence: 910/1558 loss: 6.146964768067378\n",
      "epoch: 7 sentence: 920/1558 loss: 7.199011316505364\n",
      "epoch: 7 sentence: 930/1558 loss: 6.951165962960561\n",
      "epoch: 7 sentence: 940/1558 loss: 6.521193856544198\n",
      "epoch: 7 sentence: 950/1558 loss: 7.426399214172302\n",
      "epoch: 7 sentence: 960/1558 loss: 6.956742916240273\n",
      "epoch: 7 sentence: 970/1558 loss: 7.368858001501228\n",
      "epoch: 7 sentence: 980/1558 loss: 7.126694739996805\n",
      "epoch: 7 sentence: 990/1558 loss: 7.046339029591107\n",
      "epoch: 7 sentence: 1000/1558 loss: 7.0928237975013175\n",
      "epoch: 7 sentence: 1010/1558 loss: 7.0520855875160615\n",
      "epoch: 7 sentence: 1020/1558 loss: 7.383910716342817\n",
      "epoch: 7 sentence: 1030/1558 loss: 6.883257063918252\n",
      "epoch: 7 sentence: 1040/1558 loss: 7.065407044086189\n",
      "epoch: 7 sentence: 1050/1558 loss: 7.581843003554248\n",
      "epoch: 7 sentence: 1060/1558 loss: 6.5708742937918005\n",
      "epoch: 7 sentence: 1070/1558 loss: 6.012802329766778\n",
      "epoch: 7 sentence: 1080/1558 loss: 6.821547553597033\n",
      "epoch: 7 sentence: 1090/1558 loss: 7.583226025545798\n",
      "epoch: 7 sentence: 1100/1558 loss: 7.279053269655068\n",
      "epoch: 7 sentence: 1110/1558 loss: 7.834624633527579\n",
      "epoch: 7 sentence: 1120/1558 loss: 7.433353460717889\n",
      "epoch: 7 sentence: 1130/1558 loss: 7.428619543595659\n",
      "epoch: 7 sentence: 1140/1558 loss: 7.106616187283654\n",
      "epoch: 7 sentence: 1150/1558 loss: 5.946008508320602\n",
      "epoch: 7 sentence: 1160/1558 loss: 2.685932263067344\n",
      "epoch: 7 sentence: 1170/1558 loss: 7.123616281868221\n",
      "epoch: 7 sentence: 1180/1558 loss: 7.481577259114985\n",
      "epoch: 7 sentence: 1190/1558 loss: 6.9964843293026275\n",
      "epoch: 7 sentence: 1200/1558 loss: 6.791548504908103\n",
      "epoch: 7 sentence: 1210/1558 loss: 6.406281560148481\n",
      "epoch: 7 sentence: 1220/1558 loss: 7.709132038641966\n",
      "epoch: 7 sentence: 1230/1558 loss: 5.622191937244477\n",
      "epoch: 7 sentence: 1240/1558 loss: 7.300039528499898\n",
      "epoch: 7 sentence: 1250/1558 loss: 6.341382550181784\n",
      "epoch: 7 sentence: 1260/1558 loss: 6.907583197097632\n",
      "epoch: 7 sentence: 1270/1558 loss: 6.700981197110683\n",
      "epoch: 7 sentence: 1280/1558 loss: 5.98182357460674\n",
      "epoch: 7 sentence: 1290/1558 loss: 7.548734771993826\n",
      "epoch: 7 sentence: 1300/1558 loss: 6.644231467434622\n",
      "epoch: 7 sentence: 1310/1558 loss: 6.413532712383098\n",
      "epoch: 7 sentence: 1320/1558 loss: 7.166346601734715\n",
      "epoch: 7 sentence: 1330/1558 loss: 6.695661743043291\n",
      "epoch: 7 sentence: 1340/1558 loss: 6.669544048936579\n",
      "epoch: 7 sentence: 1350/1558 loss: 6.841716828967601\n",
      "epoch: 7 sentence: 1360/1558 loss: 7.184343977763476\n",
      "epoch: 7 sentence: 1370/1558 loss: 2.748015185694538\n",
      "epoch: 7 sentence: 1380/1558 loss: 7.0794497398020475\n",
      "epoch: 7 sentence: 1390/1558 loss: 7.202638440315308\n",
      "epoch: 7 sentence: 1400/1558 loss: 6.773037080166865\n",
      "epoch: 7 sentence: 1410/1558 loss: 7.19838598161369\n",
      "epoch: 7 sentence: 1420/1558 loss: 7.3120632882020615\n",
      "epoch: 7 sentence: 1430/1558 loss: 6.867622708477512\n",
      "epoch: 7 sentence: 1440/1558 loss: 5.252576055372888\n",
      "epoch: 7 sentence: 1450/1558 loss: 7.059524976061201\n",
      "epoch: 7 sentence: 1460/1558 loss: 7.33886928186088\n",
      "epoch: 7 sentence: 1470/1558 loss: 7.248952509022826\n",
      "epoch: 7 sentence: 1480/1558 loss: 6.084254567843058\n",
      "epoch: 7 sentence: 1490/1558 loss: 7.113745347063122\n",
      "epoch: 7 sentence: 1500/1558 loss: 6.187782720246002\n",
      "epoch: 7 sentence: 1510/1558 loss: 7.469912471702157\n",
      "epoch: 7 sentence: 1520/1558 loss: 7.217469116818881\n",
      "epoch: 7 sentence: 1530/1558 loss: 6.844990984043919\n",
      "epoch: 7 sentence: 1540/1558 loss: 6.883775795792792\n",
      "epoch: 7 sentence: 1550/1558 loss: 7.808845400973994\n",
      "epoch: 8 sentence: 0/1558 loss: 7.7628563626841816\n",
      "epoch: 8 sentence: 10/1558 loss: 6.657678931732486\n",
      "epoch: 8 sentence: 20/1558 loss: 6.95399840944786\n",
      "epoch: 8 sentence: 30/1558 loss: 7.605445010311655\n",
      "epoch: 8 sentence: 40/1558 loss: 7.118165710191002\n",
      "epoch: 8 sentence: 50/1558 loss: 2.7924694650047766\n",
      "epoch: 8 sentence: 60/1558 loss: 7.491558442103084\n",
      "epoch: 8 sentence: 70/1558 loss: 7.412830298767109\n",
      "epoch: 8 sentence: 80/1558 loss: 6.112887893615377\n",
      "epoch: 8 sentence: 90/1558 loss: 7.227369973475351\n",
      "epoch: 8 sentence: 100/1558 loss: 7.242999887792834\n",
      "epoch: 8 sentence: 110/1558 loss: 6.973833336925874\n",
      "epoch: 8 sentence: 120/1558 loss: 5.972372044440706\n",
      "epoch: 8 sentence: 130/1558 loss: 6.1407612767155975\n",
      "epoch: 8 sentence: 140/1558 loss: 6.24997957945399\n",
      "epoch: 8 sentence: 150/1558 loss: 6.466807488978919\n",
      "epoch: 8 sentence: 160/1558 loss: 6.97802296586087\n",
      "epoch: 8 sentence: 170/1558 loss: 7.830813538052166\n",
      "epoch: 8 sentence: 180/1558 loss: 7.41149730274278\n",
      "epoch: 8 sentence: 190/1558 loss: 6.820452715956237\n",
      "epoch: 8 sentence: 200/1558 loss: 6.588208022860573\n",
      "epoch: 8 sentence: 210/1558 loss: 6.880301215061789\n",
      "epoch: 8 sentence: 220/1558 loss: 6.6558505846010165\n",
      "epoch: 8 sentence: 230/1558 loss: 7.639328847839239\n",
      "epoch: 8 sentence: 240/1558 loss: 7.622356335669013\n",
      "epoch: 8 sentence: 250/1558 loss: 7.127863241272924\n",
      "epoch: 8 sentence: 260/1558 loss: 7.201029208283025\n",
      "epoch: 8 sentence: 270/1558 loss: 6.773805517456754\n",
      "epoch: 8 sentence: 280/1558 loss: 7.1352051466557835\n",
      "epoch: 8 sentence: 290/1558 loss: 7.205730943470069\n",
      "epoch: 8 sentence: 300/1558 loss: 6.961668116174374\n",
      "epoch: 8 sentence: 310/1558 loss: 6.871716344398485\n",
      "epoch: 8 sentence: 320/1558 loss: 7.15324323122658\n",
      "epoch: 8 sentence: 330/1558 loss: 6.160513296750117\n",
      "epoch: 8 sentence: 340/1558 loss: 6.588177193821153\n",
      "epoch: 8 sentence: 350/1558 loss: 7.293093937948415\n",
      "epoch: 8 sentence: 360/1558 loss: 7.125141638454526\n",
      "epoch: 8 sentence: 370/1558 loss: 7.359616377111076\n",
      "epoch: 8 sentence: 380/1558 loss: 5.667121442657319\n",
      "epoch: 8 sentence: 390/1558 loss: 6.447305006262389\n",
      "epoch: 8 sentence: 400/1558 loss: 7.272015161948647\n",
      "epoch: 8 sentence: 410/1558 loss: 7.5817708976218565\n",
      "epoch: 8 sentence: 420/1558 loss: 6.9764812163992405\n",
      "epoch: 8 sentence: 430/1558 loss: 7.165675093522208\n",
      "epoch: 8 sentence: 440/1558 loss: 7.285223061437275\n",
      "epoch: 8 sentence: 450/1558 loss: 7.569357928215832\n",
      "epoch: 8 sentence: 460/1558 loss: 7.092327985466645\n",
      "epoch: 8 sentence: 470/1558 loss: 6.750116840532513\n",
      "epoch: 8 sentence: 480/1558 loss: 6.668340341728685\n",
      "epoch: 8 sentence: 490/1558 loss: 6.593011129518675\n",
      "epoch: 8 sentence: 500/1558 loss: 6.549920140833209\n",
      "epoch: 8 sentence: 510/1558 loss: 7.3511522712938\n",
      "epoch: 8 sentence: 520/1558 loss: 7.157444338188874\n",
      "epoch: 8 sentence: 530/1558 loss: 6.490980660141853\n",
      "epoch: 8 sentence: 540/1558 loss: 7.207125509663794\n",
      "epoch: 8 sentence: 550/1558 loss: 5.5241346022079245\n",
      "epoch: 8 sentence: 560/1558 loss: 7.198517635284101\n",
      "epoch: 8 sentence: 570/1558 loss: 7.75701690284128\n",
      "epoch: 8 sentence: 580/1558 loss: 6.1811930334484595\n",
      "epoch: 8 sentence: 590/1558 loss: 6.8793834961885745\n",
      "epoch: 8 sentence: 600/1558 loss: 7.150347949422724\n",
      "epoch: 8 sentence: 610/1558 loss: 7.310512180018593\n",
      "epoch: 8 sentence: 620/1558 loss: 7.323679650831305\n",
      "epoch: 8 sentence: 630/1558 loss: 7.148035136937407\n",
      "epoch: 8 sentence: 640/1558 loss: 2.787312261770042\n",
      "epoch: 8 sentence: 650/1558 loss: 7.371092406620625\n",
      "epoch: 8 sentence: 660/1558 loss: 7.047879141693643\n",
      "epoch: 8 sentence: 670/1558 loss: 6.236659741980292\n",
      "epoch: 8 sentence: 680/1558 loss: 6.796579509528778\n",
      "epoch: 8 sentence: 690/1558 loss: 7.168877170542709\n",
      "epoch: 8 sentence: 700/1558 loss: 6.940063304216683\n",
      "epoch: 8 sentence: 710/1558 loss: 7.012865430674102\n",
      "epoch: 8 sentence: 720/1558 loss: 6.775201073418042\n",
      "epoch: 8 sentence: 730/1558 loss: 7.352315155309196\n",
      "epoch: 8 sentence: 740/1558 loss: 5.715340359201689\n",
      "epoch: 8 sentence: 750/1558 loss: 7.262425483354756\n",
      "epoch: 8 sentence: 760/1558 loss: 7.037598086265014\n",
      "epoch: 8 sentence: 770/1558 loss: 6.843792986223314\n",
      "epoch: 8 sentence: 780/1558 loss: 6.816409418169682\n",
      "epoch: 8 sentence: 790/1558 loss: 7.403352503122471\n",
      "epoch: 8 sentence: 800/1558 loss: 6.917838451953685\n",
      "epoch: 8 sentence: 810/1558 loss: 7.51335331330331\n",
      "epoch: 8 sentence: 820/1558 loss: 6.860439990180536\n",
      "epoch: 8 sentence: 830/1558 loss: 7.194684893858985\n",
      "epoch: 8 sentence: 840/1558 loss: 2.7504731694828353\n",
      "epoch: 8 sentence: 850/1558 loss: 7.413051818697764\n",
      "epoch: 8 sentence: 860/1558 loss: 7.548374576333383\n",
      "epoch: 8 sentence: 870/1558 loss: 7.124708001742836\n",
      "epoch: 8 sentence: 880/1558 loss: 7.438469892841506\n",
      "epoch: 8 sentence: 890/1558 loss: 6.576366714067003\n",
      "epoch: 8 sentence: 900/1558 loss: 7.164144021369564\n",
      "epoch: 8 sentence: 910/1558 loss: 7.007122274430493\n",
      "epoch: 8 sentence: 920/1558 loss: 6.997988565615242\n",
      "epoch: 8 sentence: 930/1558 loss: 7.515582434715193\n",
      "epoch: 8 sentence: 940/1558 loss: 2.755071209765115\n",
      "epoch: 8 sentence: 950/1558 loss: 6.301548076978784\n",
      "epoch: 8 sentence: 960/1558 loss: 7.22207327009008\n",
      "epoch: 8 sentence: 970/1558 loss: 7.069633273634992\n",
      "epoch: 8 sentence: 980/1558 loss: 7.121428408338231\n",
      "epoch: 8 sentence: 990/1558 loss: 7.232323035707848\n",
      "epoch: 8 sentence: 1000/1558 loss: 7.331111424162243\n",
      "epoch: 8 sentence: 1010/1558 loss: 7.714026148554799\n",
      "epoch: 8 sentence: 1020/1558 loss: 6.493732957306894\n",
      "epoch: 8 sentence: 1030/1558 loss: 7.635706435596137\n",
      "epoch: 8 sentence: 1040/1558 loss: 7.748765654681472\n",
      "epoch: 8 sentence: 1050/1558 loss: 2.75090319236015\n",
      "epoch: 8 sentence: 1060/1558 loss: 7.160155643612368\n",
      "epoch: 8 sentence: 1070/1558 loss: 6.948177462134618\n",
      "epoch: 8 sentence: 1080/1558 loss: 7.359280930609765\n",
      "epoch: 8 sentence: 1090/1558 loss: 6.6965406108743455\n",
      "epoch: 8 sentence: 1100/1558 loss: 7.148605095552907\n",
      "epoch: 8 sentence: 1110/1558 loss: 7.192018161496601\n",
      "epoch: 8 sentence: 1120/1558 loss: 7.449194400303582\n",
      "epoch: 8 sentence: 1130/1558 loss: 6.845625669036274\n",
      "epoch: 8 sentence: 1140/1558 loss: 6.494339494044523\n",
      "epoch: 8 sentence: 1150/1558 loss: 7.089599645358604\n",
      "epoch: 8 sentence: 1160/1558 loss: 7.788547507138733\n",
      "epoch: 8 sentence: 1170/1558 loss: 2.7525020795261064\n",
      "epoch: 8 sentence: 1180/1558 loss: 2.74947633394837\n",
      "epoch: 8 sentence: 1190/1558 loss: 6.915862546248819\n",
      "epoch: 8 sentence: 1200/1558 loss: 2.733576647126852\n",
      "epoch: 8 sentence: 1210/1558 loss: 6.368060796263715\n",
      "epoch: 8 sentence: 1220/1558 loss: 6.822146333328964\n",
      "epoch: 8 sentence: 1230/1558 loss: 6.9103360365029225\n",
      "epoch: 8 sentence: 1240/1558 loss: 7.323015792896477\n",
      "epoch: 8 sentence: 1250/1558 loss: 6.965499987500265\n",
      "epoch: 8 sentence: 1260/1558 loss: 2.724216264350557\n",
      "epoch: 8 sentence: 1270/1558 loss: 7.109463490239646\n",
      "epoch: 8 sentence: 1280/1558 loss: 7.366859955945129\n",
      "epoch: 8 sentence: 1290/1558 loss: 5.963930527580463\n",
      "epoch: 8 sentence: 1300/1558 loss: 7.183128224992806\n",
      "epoch: 8 sentence: 1310/1558 loss: 7.449872406067556\n",
      "epoch: 8 sentence: 1320/1558 loss: 7.3685508944421185\n",
      "epoch: 8 sentence: 1330/1558 loss: 6.788136871170431\n",
      "epoch: 8 sentence: 1340/1558 loss: 5.876203931917117\n",
      "epoch: 8 sentence: 1350/1558 loss: 6.2808251303081875\n",
      "epoch: 8 sentence: 1360/1558 loss: 5.091530532295758\n",
      "epoch: 8 sentence: 1370/1558 loss: 6.507523316178309\n",
      "epoch: 8 sentence: 1380/1558 loss: 5.711001731800997\n",
      "epoch: 8 sentence: 1390/1558 loss: 7.488525157672425\n",
      "epoch: 8 sentence: 1400/1558 loss: 6.203103159435759\n",
      "epoch: 8 sentence: 1410/1558 loss: 7.3762519509771565\n",
      "epoch: 8 sentence: 1420/1558 loss: 2.7023531453210987\n",
      "epoch: 8 sentence: 1430/1558 loss: 7.232552466751688\n",
      "epoch: 8 sentence: 1440/1558 loss: 7.280986444887652\n",
      "epoch: 8 sentence: 1450/1558 loss: 6.9008109396105555\n",
      "epoch: 8 sentence: 1460/1558 loss: 7.435003230510693\n",
      "epoch: 8 sentence: 1470/1558 loss: 7.594338203053283\n",
      "epoch: 8 sentence: 1480/1558 loss: 7.172868517552682\n",
      "epoch: 8 sentence: 1490/1558 loss: 6.991099623773258\n",
      "epoch: 8 sentence: 1500/1558 loss: 7.156037906552216\n",
      "epoch: 8 sentence: 1510/1558 loss: 2.701705368384293\n",
      "epoch: 8 sentence: 1520/1558 loss: 7.676183841248283\n",
      "epoch: 8 sentence: 1530/1558 loss: 6.536657323586517\n",
      "epoch: 8 sentence: 1540/1558 loss: 7.0526296147403835\n",
      "epoch: 8 sentence: 1550/1558 loss: 6.851951565505689\n",
      "epoch: 9 sentence: 0/1558 loss: 7.378881442615877\n",
      "epoch: 9 sentence: 10/1558 loss: 6.82993551727341\n",
      "epoch: 9 sentence: 20/1558 loss: 7.189962282756131\n",
      "epoch: 9 sentence: 30/1558 loss: 6.4889004282177565\n",
      "epoch: 9 sentence: 40/1558 loss: 6.7411292060802435\n",
      "epoch: 9 sentence: 50/1558 loss: 7.313568301931236\n",
      "epoch: 9 sentence: 60/1558 loss: 7.425569726704336\n",
      "epoch: 9 sentence: 70/1558 loss: 6.873698429299703\n",
      "epoch: 9 sentence: 80/1558 loss: 7.635354451233848\n",
      "epoch: 9 sentence: 90/1558 loss: 2.676464271224695\n",
      "epoch: 9 sentence: 100/1558 loss: 2.6648751706793643\n",
      "epoch: 9 sentence: 110/1558 loss: 7.217094437588964\n",
      "epoch: 9 sentence: 120/1558 loss: 6.749427665621698\n",
      "epoch: 9 sentence: 130/1558 loss: 6.00257512646067\n",
      "epoch: 9 sentence: 140/1558 loss: 7.200089730685294\n",
      "epoch: 9 sentence: 150/1558 loss: 7.144280636448413\n",
      "epoch: 9 sentence: 160/1558 loss: 7.6524536956355345\n",
      "epoch: 9 sentence: 170/1558 loss: 7.046179545088089\n",
      "epoch: 9 sentence: 180/1558 loss: 7.309001037932963\n",
      "epoch: 9 sentence: 190/1558 loss: 7.014139174621305\n",
      "epoch: 9 sentence: 200/1558 loss: 7.224109044636678\n",
      "epoch: 9 sentence: 210/1558 loss: 7.275327424354221\n",
      "epoch: 9 sentence: 220/1558 loss: 7.000346826531579\n",
      "epoch: 9 sentence: 230/1558 loss: 6.902402781480354\n",
      "epoch: 9 sentence: 240/1558 loss: 2.6384933723208723\n",
      "epoch: 9 sentence: 250/1558 loss: 7.899043933970078\n",
      "epoch: 9 sentence: 260/1558 loss: 2.621883207171263\n",
      "epoch: 9 sentence: 270/1558 loss: 6.4738108879356\n",
      "epoch: 9 sentence: 280/1558 loss: 6.796037416042107\n",
      "epoch: 9 sentence: 290/1558 loss: 7.007013082915817\n",
      "epoch: 9 sentence: 300/1558 loss: 6.97681005448014\n",
      "epoch: 9 sentence: 310/1558 loss: 6.877054142537233\n",
      "epoch: 9 sentence: 320/1558 loss: 6.594419669595673\n",
      "epoch: 9 sentence: 330/1558 loss: 2.5977454627114005\n",
      "epoch: 9 sentence: 340/1558 loss: 6.824782059533501\n",
      "epoch: 9 sentence: 350/1558 loss: 6.843229026591424\n",
      "epoch: 9 sentence: 360/1558 loss: 4.236400944543171\n",
      "epoch: 9 sentence: 370/1558 loss: 7.200083827279302\n",
      "epoch: 9 sentence: 380/1558 loss: 7.003577704276421\n",
      "epoch: 9 sentence: 390/1558 loss: 7.0545824900050995\n",
      "epoch: 9 sentence: 400/1558 loss: 7.293850804134018\n",
      "epoch: 9 sentence: 410/1558 loss: 7.0412745510683425\n",
      "epoch: 9 sentence: 420/1558 loss: 7.197627962860452\n",
      "epoch: 9 sentence: 430/1558 loss: 6.630740076837718\n",
      "epoch: 9 sentence: 440/1558 loss: 6.523233128445057\n",
      "epoch: 9 sentence: 450/1558 loss: 6.837156311368053\n",
      "epoch: 9 sentence: 460/1558 loss: 7.901568148352787\n",
      "epoch: 9 sentence: 470/1558 loss: 6.538559866645827\n",
      "epoch: 9 sentence: 480/1558 loss: 6.823838449406221\n",
      "epoch: 9 sentence: 490/1558 loss: 7.250726922033339\n",
      "epoch: 9 sentence: 500/1558 loss: 7.745986485713026\n",
      "epoch: 9 sentence: 510/1558 loss: 7.052926629912454\n",
      "epoch: 9 sentence: 520/1558 loss: 7.5835051108942295\n",
      "epoch: 9 sentence: 530/1558 loss: 7.2724355991474\n",
      "epoch: 9 sentence: 540/1558 loss: 7.068994653830124\n",
      "epoch: 9 sentence: 550/1558 loss: 7.578213001946738\n",
      "epoch: 9 sentence: 560/1558 loss: 7.306461930610984\n",
      "epoch: 9 sentence: 570/1558 loss: 7.274870422506144\n",
      "epoch: 9 sentence: 580/1558 loss: 6.280806536935813\n",
      "epoch: 9 sentence: 590/1558 loss: 6.424440417507954\n",
      "epoch: 9 sentence: 600/1558 loss: 2.636850353728198\n",
      "epoch: 9 sentence: 610/1558 loss: 6.893759667328843\n",
      "epoch: 9 sentence: 620/1558 loss: 7.132176000431959\n",
      "epoch: 9 sentence: 630/1558 loss: 5.604597729158537\n",
      "epoch: 9 sentence: 640/1558 loss: 6.360294068056632\n",
      "epoch: 9 sentence: 650/1558 loss: 5.239282803017646\n",
      "epoch: 9 sentence: 660/1558 loss: 7.219574163484705\n",
      "epoch: 9 sentence: 670/1558 loss: 6.990021225113079\n",
      "epoch: 9 sentence: 680/1558 loss: 6.868529525668649\n",
      "epoch: 9 sentence: 690/1558 loss: 7.5520598869000555\n",
      "epoch: 9 sentence: 700/1558 loss: 7.292032825692723\n",
      "epoch: 9 sentence: 710/1558 loss: 7.253310712360151\n",
      "epoch: 9 sentence: 720/1558 loss: 7.032277035123647\n",
      "epoch: 9 sentence: 730/1558 loss: 5.073081549581517\n",
      "epoch: 9 sentence: 740/1558 loss: 8.000244422585961\n",
      "epoch: 9 sentence: 750/1558 loss: 7.284629821595161\n",
      "epoch: 9 sentence: 760/1558 loss: 7.292443656782785\n",
      "epoch: 9 sentence: 770/1558 loss: 7.511023490357944\n",
      "epoch: 9 sentence: 780/1558 loss: 7.055495945379675\n",
      "epoch: 9 sentence: 790/1558 loss: 7.460847743948497\n",
      "epoch: 9 sentence: 800/1558 loss: 7.399834697802201\n",
      "epoch: 9 sentence: 810/1558 loss: 5.190825719100944\n",
      "epoch: 9 sentence: 820/1558 loss: 6.875819244840529\n",
      "epoch: 9 sentence: 830/1558 loss: 7.125799831291198\n",
      "epoch: 9 sentence: 840/1558 loss: 6.536854321290792\n",
      "epoch: 9 sentence: 850/1558 loss: 7.313409552708024\n",
      "epoch: 9 sentence: 860/1558 loss: 7.069287098087224\n",
      "epoch: 9 sentence: 870/1558 loss: 6.966401303730118\n",
      "epoch: 9 sentence: 880/1558 loss: 5.906091456473838\n",
      "epoch: 9 sentence: 890/1558 loss: 2.660359086564215\n",
      "epoch: 9 sentence: 900/1558 loss: 7.24456185985875\n",
      "epoch: 9 sentence: 910/1558 loss: 6.941425031395201\n",
      "epoch: 9 sentence: 920/1558 loss: 6.766822190663978\n",
      "epoch: 9 sentence: 930/1558 loss: 7.494016606599732\n",
      "epoch: 9 sentence: 940/1558 loss: 6.989915344178757\n",
      "epoch: 9 sentence: 950/1558 loss: 6.726094061890393\n",
      "epoch: 9 sentence: 960/1558 loss: 4.80317184118071\n",
      "epoch: 9 sentence: 970/1558 loss: 7.5722393313322245\n",
      "epoch: 9 sentence: 980/1558 loss: 7.033624186922708\n",
      "epoch: 9 sentence: 990/1558 loss: 2.6798108765747126\n",
      "epoch: 9 sentence: 1000/1558 loss: 6.9400973642766655\n",
      "epoch: 9 sentence: 1010/1558 loss: 7.6284016813812\n",
      "epoch: 9 sentence: 1020/1558 loss: 6.59650009281458\n",
      "epoch: 9 sentence: 1030/1558 loss: 6.836088706340725\n",
      "epoch: 9 sentence: 1040/1558 loss: 5.409013831860072\n",
      "epoch: 9 sentence: 1050/1558 loss: 6.308149245948838\n",
      "epoch: 9 sentence: 1060/1558 loss: 7.5258506657631585\n",
      "epoch: 9 sentence: 1070/1558 loss: 6.004717394452744\n",
      "epoch: 9 sentence: 1080/1558 loss: 7.701739057654348\n",
      "epoch: 9 sentence: 1090/1558 loss: 6.9514110077095586\n",
      "epoch: 9 sentence: 1100/1558 loss: 6.372881760539752\n",
      "epoch: 9 sentence: 1110/1558 loss: 5.7844658376188685\n",
      "epoch: 9 sentence: 1120/1558 loss: 6.708079883215202\n",
      "epoch: 9 sentence: 1130/1558 loss: 7.21457581643368\n",
      "epoch: 9 sentence: 1140/1558 loss: 7.131310007805567\n",
      "epoch: 9 sentence: 1150/1558 loss: 7.31038627311412\n",
      "epoch: 9 sentence: 1160/1558 loss: 5.964075626133799\n",
      "epoch: 9 sentence: 1170/1558 loss: 7.4329854058310145\n",
      "epoch: 9 sentence: 1180/1558 loss: 7.084396558276275\n",
      "epoch: 9 sentence: 1190/1558 loss: 7.5500310639043615\n",
      "epoch: 9 sentence: 1200/1558 loss: 6.912154572893192\n",
      "epoch: 9 sentence: 1210/1558 loss: 6.815071750956651\n",
      "epoch: 9 sentence: 1220/1558 loss: 6.867339151887048\n",
      "epoch: 9 sentence: 1230/1558 loss: 6.269140865526566\n",
      "epoch: 9 sentence: 1240/1558 loss: 6.903747813150831\n",
      "epoch: 9 sentence: 1250/1558 loss: 2.7235335000115555\n",
      "epoch: 9 sentence: 1260/1558 loss: 7.17381979797142\n",
      "epoch: 9 sentence: 1270/1558 loss: 6.020639284265059\n",
      "epoch: 9 sentence: 1280/1558 loss: 6.321560704026069\n",
      "epoch: 9 sentence: 1290/1558 loss: 7.469491391315659\n",
      "epoch: 9 sentence: 1300/1558 loss: 7.240695441861389\n",
      "epoch: 9 sentence: 1310/1558 loss: 7.11393053002858\n",
      "epoch: 9 sentence: 1320/1558 loss: 6.627876590021496\n",
      "epoch: 9 sentence: 1330/1558 loss: 7.381513687355927\n",
      "epoch: 9 sentence: 1340/1558 loss: 7.536414430364773\n",
      "epoch: 9 sentence: 1350/1558 loss: 7.046703074061925\n",
      "epoch: 9 sentence: 1360/1558 loss: 7.142893899445625\n",
      "epoch: 9 sentence: 1370/1558 loss: 6.3929512567642455\n",
      "epoch: 9 sentence: 1380/1558 loss: 7.7725993437515335\n",
      "epoch: 9 sentence: 1390/1558 loss: 7.2135753774261975\n",
      "epoch: 9 sentence: 1400/1558 loss: 6.911668562180034\n",
      "epoch: 9 sentence: 1410/1558 loss: 6.33077487200383\n",
      "epoch: 9 sentence: 1420/1558 loss: 6.94565839637768\n",
      "epoch: 9 sentence: 1430/1558 loss: 2.7445535485176666\n",
      "epoch: 9 sentence: 1440/1558 loss: 6.525143304085336\n",
      "epoch: 9 sentence: 1450/1558 loss: 7.65106760747605\n",
      "epoch: 9 sentence: 1460/1558 loss: 6.709765510703667\n",
      "epoch: 9 sentence: 1470/1558 loss: 6.404172525084816\n",
      "epoch: 9 sentence: 1480/1558 loss: 6.758012013353823\n",
      "epoch: 9 sentence: 1490/1558 loss: 2.717728787245152\n",
      "epoch: 9 sentence: 1500/1558 loss: 7.007333901003741\n",
      "epoch: 9 sentence: 1510/1558 loss: 7.253596623158866\n",
      "epoch: 9 sentence: 1520/1558 loss: 5.91517457344686\n",
      "epoch: 9 sentence: 1530/1558 loss: 2.7221281371112465\n",
      "epoch: 9 sentence: 1540/1558 loss: 7.000003586505353\n",
      "epoch: 9 sentence: 1550/1558 loss: 6.93465821110189\n",
      "epoch: 10 sentence: 0/1558 loss: 5.566005421273724\n",
      "epoch: 10 sentence: 10/1558 loss: 5.754641902696183\n",
      "epoch: 10 sentence: 20/1558 loss: 6.934134771623473\n",
      "epoch: 10 sentence: 30/1558 loss: 6.931415494043944\n",
      "epoch: 10 sentence: 40/1558 loss: 6.984283259165727\n",
      "epoch: 10 sentence: 50/1558 loss: 6.942974212718564\n",
      "epoch: 10 sentence: 60/1558 loss: 7.237843225562998\n",
      "epoch: 10 sentence: 70/1558 loss: 6.509027791761082\n",
      "epoch: 10 sentence: 80/1558 loss: 6.325469381272955\n",
      "epoch: 10 sentence: 90/1558 loss: 7.22898611961052\n",
      "epoch: 10 sentence: 100/1558 loss: 2.7397382632637597\n",
      "epoch: 10 sentence: 110/1558 loss: 6.772711118584727\n",
      "epoch: 10 sentence: 120/1558 loss: 6.518294838656416\n",
      "epoch: 10 sentence: 130/1558 loss: 7.180902584829823\n",
      "epoch: 10 sentence: 140/1558 loss: 6.7556676848032255\n",
      "epoch: 10 sentence: 150/1558 loss: 6.656419913950033\n",
      "epoch: 10 sentence: 160/1558 loss: 6.560804214116737\n",
      "epoch: 10 sentence: 170/1558 loss: 7.012138017811719\n",
      "epoch: 10 sentence: 180/1558 loss: 6.819929406217138\n",
      "epoch: 10 sentence: 190/1558 loss: 7.2638246561517095\n",
      "epoch: 10 sentence: 200/1558 loss: 6.891711074102402\n",
      "epoch: 10 sentence: 210/1558 loss: 7.718580069050152\n",
      "epoch: 10 sentence: 220/1558 loss: 7.464727780825683\n",
      "epoch: 10 sentence: 230/1558 loss: 6.854175484098441\n",
      "epoch: 10 sentence: 240/1558 loss: 2.7944280861468793\n",
      "epoch: 10 sentence: 250/1558 loss: 7.040867851072221\n",
      "epoch: 10 sentence: 260/1558 loss: 6.746322002795051\n",
      "epoch: 10 sentence: 270/1558 loss: 7.218207175535366\n",
      "epoch: 10 sentence: 280/1558 loss: 6.663736533979427\n",
      "epoch: 10 sentence: 290/1558 loss: 6.211372949713567\n",
      "epoch: 10 sentence: 300/1558 loss: 7.400607414354957\n",
      "epoch: 10 sentence: 310/1558 loss: 6.035535526218318\n",
      "epoch: 10 sentence: 320/1558 loss: 6.952251417962025\n",
      "epoch: 10 sentence: 330/1558 loss: 7.712318068224815\n",
      "epoch: 10 sentence: 340/1558 loss: 7.513674570393984\n",
      "epoch: 10 sentence: 350/1558 loss: 6.809238904150636\n",
      "epoch: 10 sentence: 360/1558 loss: 6.794605417443444\n",
      "epoch: 10 sentence: 370/1558 loss: 6.865471981902577\n",
      "epoch: 10 sentence: 380/1558 loss: 6.830057583552712\n",
      "epoch: 10 sentence: 390/1558 loss: 6.746463179046287\n",
      "epoch: 10 sentence: 400/1558 loss: 6.794464799462678\n",
      "epoch: 10 sentence: 410/1558 loss: 6.830864683115178\n",
      "epoch: 10 sentence: 420/1558 loss: 2.774789406540995\n",
      "epoch: 10 sentence: 430/1558 loss: 6.8584433110489895\n",
      "epoch: 10 sentence: 440/1558 loss: 6.980944121030084\n",
      "epoch: 10 sentence: 450/1558 loss: 5.957296589813364\n",
      "epoch: 10 sentence: 460/1558 loss: 7.097707123794803\n",
      "epoch: 10 sentence: 470/1558 loss: 5.929273085030216\n",
      "epoch: 10 sentence: 480/1558 loss: 6.996099437401292\n",
      "epoch: 10 sentence: 490/1558 loss: 6.693401094666205\n",
      "epoch: 10 sentence: 500/1558 loss: 7.094001618753933\n",
      "epoch: 10 sentence: 510/1558 loss: 6.976203006002173\n",
      "epoch: 10 sentence: 520/1558 loss: 2.7607782092564483\n",
      "epoch: 10 sentence: 530/1558 loss: 7.091811950121273\n",
      "epoch: 10 sentence: 540/1558 loss: 7.269463791657839\n",
      "epoch: 10 sentence: 550/1558 loss: 2.749805992615886\n",
      "epoch: 10 sentence: 560/1558 loss: 6.9841947736865695\n",
      "epoch: 10 sentence: 570/1558 loss: 5.339232557219508\n",
      "epoch: 10 sentence: 580/1558 loss: 6.432929339255687\n",
      "epoch: 10 sentence: 590/1558 loss: 7.061533558431696\n",
      "epoch: 10 sentence: 600/1558 loss: 6.912896007137367\n",
      "epoch: 10 sentence: 610/1558 loss: 6.926979478831101\n",
      "epoch: 10 sentence: 620/1558 loss: 2.753217159452931\n",
      "epoch: 10 sentence: 630/1558 loss: 7.254068667141707\n",
      "epoch: 10 sentence: 640/1558 loss: 7.085392991139418\n",
      "epoch: 10 sentence: 650/1558 loss: 6.412281485656192\n",
      "epoch: 10 sentence: 660/1558 loss: 6.367429220004276\n",
      "epoch: 10 sentence: 670/1558 loss: 6.968765512212786\n",
      "epoch: 10 sentence: 680/1558 loss: 2.7321885857226813\n",
      "epoch: 10 sentence: 690/1558 loss: 7.550256592677779\n",
      "epoch: 10 sentence: 700/1558 loss: 7.4903769713953325\n",
      "epoch: 10 sentence: 710/1558 loss: 6.934013765003075\n",
      "epoch: 10 sentence: 720/1558 loss: 7.0261641851798196\n",
      "epoch: 10 sentence: 730/1558 loss: 7.180046897650974\n",
      "epoch: 10 sentence: 740/1558 loss: 2.720855724522977\n",
      "epoch: 10 sentence: 750/1558 loss: 6.799237412839662\n",
      "epoch: 10 sentence: 760/1558 loss: 7.054767402344259\n",
      "epoch: 10 sentence: 770/1558 loss: 6.29136589644244\n",
      "epoch: 10 sentence: 780/1558 loss: 6.31154984833348\n",
      "epoch: 10 sentence: 790/1558 loss: 7.803292692780483\n",
      "epoch: 10 sentence: 800/1558 loss: 2.711940542059211\n",
      "epoch: 10 sentence: 810/1558 loss: 6.508092620048707\n",
      "epoch: 10 sentence: 820/1558 loss: 7.833362776539516\n",
      "epoch: 10 sentence: 830/1558 loss: 7.291922705338736\n",
      "epoch: 10 sentence: 840/1558 loss: 6.6625089230163566\n",
      "epoch: 10 sentence: 850/1558 loss: 6.523294763700508\n",
      "epoch: 10 sentence: 860/1558 loss: 7.068141554638882\n",
      "epoch: 10 sentence: 870/1558 loss: 6.977788961048449\n",
      "epoch: 10 sentence: 880/1558 loss: 6.0440544081605285\n",
      "epoch: 10 sentence: 890/1558 loss: 6.8986235732478685\n",
      "epoch: 10 sentence: 900/1558 loss: 6.913301654235887\n",
      "epoch: 10 sentence: 910/1558 loss: 7.013010661559584\n",
      "epoch: 10 sentence: 920/1558 loss: 5.3244559288234905\n",
      "epoch: 10 sentence: 930/1558 loss: 6.409601040237772\n",
      "epoch: 10 sentence: 940/1558 loss: 7.06572185622853\n",
      "epoch: 10 sentence: 950/1558 loss: 7.8963336946904645\n",
      "epoch: 10 sentence: 960/1558 loss: 7.218151120056668\n",
      "epoch: 10 sentence: 970/1558 loss: 7.639904078138477\n",
      "epoch: 10 sentence: 980/1558 loss: 7.211608421583922\n",
      "epoch: 10 sentence: 990/1558 loss: 2.699346236444882\n",
      "epoch: 10 sentence: 1000/1558 loss: 6.597507719877835\n",
      "epoch: 10 sentence: 1010/1558 loss: 6.645496148279052\n",
      "epoch: 10 sentence: 1020/1558 loss: 7.847978898081639\n",
      "epoch: 10 sentence: 1030/1558 loss: 7.079105614927551\n",
      "epoch: 10 sentence: 1040/1558 loss: 6.10240053270394\n",
      "epoch: 10 sentence: 1050/1558 loss: 7.813971189426245\n",
      "epoch: 10 sentence: 1060/1558 loss: 7.177558809260979\n",
      "epoch: 10 sentence: 1070/1558 loss: 6.423900199106472\n",
      "epoch: 10 sentence: 1080/1558 loss: 6.296675950913175\n",
      "epoch: 10 sentence: 1090/1558 loss: 7.050089159147298\n",
      "epoch: 10 sentence: 1100/1558 loss: 6.945574809144598\n",
      "epoch: 10 sentence: 1110/1558 loss: 7.329117972972881\n",
      "epoch: 10 sentence: 1120/1558 loss: 6.79443907264306\n",
      "epoch: 10 sentence: 1130/1558 loss: 6.285677201236064\n",
      "epoch: 10 sentence: 1140/1558 loss: 6.776676240761748\n",
      "epoch: 10 sentence: 1150/1558 loss: 6.834479530418947\n",
      "epoch: 10 sentence: 1160/1558 loss: 6.898061658801166\n",
      "epoch: 10 sentence: 1170/1558 loss: 7.088727279602766\n",
      "epoch: 10 sentence: 1180/1558 loss: 6.53335496338298\n",
      "epoch: 10 sentence: 1190/1558 loss: 6.596053713223734\n",
      "epoch: 10 sentence: 1200/1558 loss: 6.7672565933585656\n",
      "epoch: 10 sentence: 1210/1558 loss: 2.771154883397648\n",
      "epoch: 10 sentence: 1220/1558 loss: 6.730021180006486\n",
      "epoch: 10 sentence: 1230/1558 loss: 6.245320236296803\n",
      "epoch: 10 sentence: 1240/1558 loss: 6.817066642945871\n",
      "epoch: 10 sentence: 1250/1558 loss: 6.460661821390716\n",
      "epoch: 10 sentence: 1260/1558 loss: 6.292120157997047\n",
      "epoch: 10 sentence: 1270/1558 loss: 7.466319393294757\n",
      "epoch: 10 sentence: 1280/1558 loss: 7.046110259465438\n",
      "epoch: 10 sentence: 1290/1558 loss: 6.900917386089095\n",
      "epoch: 10 sentence: 1300/1558 loss: 7.119386863630773\n",
      "epoch: 10 sentence: 1310/1558 loss: 6.8427057554555795\n",
      "epoch: 10 sentence: 1320/1558 loss: 7.180163655845575\n",
      "epoch: 10 sentence: 1330/1558 loss: 5.9929325745396484\n",
      "epoch: 10 sentence: 1340/1558 loss: 7.028173883410654\n",
      "epoch: 10 sentence: 1350/1558 loss: 6.915488189986279\n",
      "epoch: 10 sentence: 1360/1558 loss: 6.623718044022899\n",
      "epoch: 10 sentence: 1370/1558 loss: 6.44345847057228\n",
      "epoch: 10 sentence: 1380/1558 loss: 7.28059701702478\n",
      "epoch: 10 sentence: 1390/1558 loss: 6.853823101794212\n",
      "epoch: 10 sentence: 1400/1558 loss: 6.4251481559175305\n",
      "epoch: 10 sentence: 1410/1558 loss: 6.569420320407049\n",
      "epoch: 10 sentence: 1420/1558 loss: 6.445209284543609\n",
      "epoch: 10 sentence: 1430/1558 loss: 6.873569480900709\n",
      "epoch: 10 sentence: 1440/1558 loss: 6.523031615388782\n",
      "epoch: 10 sentence: 1450/1558 loss: 2.709617924089163\n",
      "epoch: 10 sentence: 1460/1558 loss: 7.1352196194050475\n",
      "epoch: 10 sentence: 1470/1558 loss: 6.3456590371269606\n",
      "epoch: 10 sentence: 1480/1558 loss: 6.910258541153781\n",
      "epoch: 10 sentence: 1490/1558 loss: 7.096896563347569\n",
      "epoch: 10 sentence: 1500/1558 loss: 7.309085537154696\n",
      "epoch: 10 sentence: 1510/1558 loss: 7.3187170162619335\n",
      "epoch: 10 sentence: 1520/1558 loss: 7.152812198841004\n",
      "epoch: 10 sentence: 1530/1558 loss: 6.817883614357634\n",
      "epoch: 10 sentence: 1540/1558 loss: 5.971775003622359\n",
      "epoch: 10 sentence: 1550/1558 loss: 7.715329930436421\n",
      "epoch: 11 sentence: 0/1558 loss: 5.27449558130345\n",
      "epoch: 11 sentence: 10/1558 loss: 6.408679104422222\n",
      "epoch: 11 sentence: 20/1558 loss: 6.435466616381978\n",
      "epoch: 11 sentence: 30/1558 loss: 2.67500555460984\n",
      "epoch: 11 sentence: 40/1558 loss: 7.034996061482685\n",
      "epoch: 11 sentence: 50/1558 loss: 2.666758739677671\n",
      "epoch: 11 sentence: 60/1558 loss: 6.8271691507159415\n",
      "epoch: 11 sentence: 70/1558 loss: 7.350430340380075\n",
      "epoch: 11 sentence: 80/1558 loss: 7.260942128092909\n",
      "epoch: 11 sentence: 90/1558 loss: 6.919652327891665\n",
      "epoch: 11 sentence: 100/1558 loss: 7.252049307716157\n",
      "epoch: 11 sentence: 110/1558 loss: 7.49819389047871\n",
      "epoch: 11 sentence: 120/1558 loss: 7.048260606798128\n",
      "epoch: 11 sentence: 130/1558 loss: 7.167806790132531\n",
      "epoch: 11 sentence: 140/1558 loss: 7.1654821933059845\n",
      "epoch: 11 sentence: 150/1558 loss: 7.183213656119593\n",
      "epoch: 11 sentence: 160/1558 loss: 6.816884369223433\n",
      "epoch: 11 sentence: 170/1558 loss: 6.5305947952089305\n",
      "epoch: 11 sentence: 180/1558 loss: 2.671862769308982\n",
      "epoch: 11 sentence: 190/1558 loss: 6.480030438109985\n",
      "epoch: 11 sentence: 200/1558 loss: 2.6658157233532846\n",
      "epoch: 11 sentence: 210/1558 loss: 7.538370704601659\n",
      "epoch: 11 sentence: 220/1558 loss: 6.719627978086536\n",
      "epoch: 11 sentence: 230/1558 loss: 7.248651624805458\n",
      "epoch: 11 sentence: 240/1558 loss: 6.967503104030882\n",
      "epoch: 11 sentence: 250/1558 loss: 7.402608882762711\n",
      "epoch: 11 sentence: 260/1558 loss: 7.975342769677816\n",
      "epoch: 11 sentence: 270/1558 loss: 7.11339795110644\n",
      "epoch: 11 sentence: 280/1558 loss: 7.4073888077535255\n",
      "epoch: 11 sentence: 290/1558 loss: 6.54810625257994\n",
      "epoch: 11 sentence: 300/1558 loss: 6.352150707786917\n",
      "epoch: 11 sentence: 310/1558 loss: 6.798132687618479\n",
      "epoch: 11 sentence: 320/1558 loss: 5.756348097531183\n",
      "epoch: 11 sentence: 330/1558 loss: 6.991746153201609\n",
      "epoch: 11 sentence: 340/1558 loss: 6.709328540899351\n",
      "epoch: 11 sentence: 350/1558 loss: 6.771329207938027\n",
      "epoch: 11 sentence: 360/1558 loss: 6.268713425417869\n",
      "epoch: 11 sentence: 370/1558 loss: 7.687814946341168\n",
      "epoch: 11 sentence: 380/1558 loss: 7.256360033256054\n",
      "epoch: 11 sentence: 390/1558 loss: 7.5343220258518775\n",
      "epoch: 11 sentence: 400/1558 loss: 6.399896602946755\n",
      "epoch: 11 sentence: 410/1558 loss: 2.6845332880487356\n",
      "epoch: 11 sentence: 420/1558 loss: 6.361650529168885\n",
      "epoch: 11 sentence: 430/1558 loss: 7.312535059295861\n",
      "epoch: 11 sentence: 440/1558 loss: 6.962713728722627\n",
      "epoch: 11 sentence: 450/1558 loss: 7.51932212896486\n",
      "epoch: 11 sentence: 460/1558 loss: 5.70402681972409\n",
      "epoch: 11 sentence: 470/1558 loss: 4.4459404651998025\n",
      "epoch: 11 sentence: 480/1558 loss: 2.6684162129641518\n",
      "epoch: 11 sentence: 490/1558 loss: 7.728228303385082\n",
      "epoch: 11 sentence: 500/1558 loss: 2.6553247628597063\n",
      "epoch: 11 sentence: 510/1558 loss: 6.936896822287253\n",
      "epoch: 11 sentence: 520/1558 loss: 6.408122562635013\n",
      "epoch: 11 sentence: 530/1558 loss: 2.6576760119851626\n",
      "epoch: 11 sentence: 540/1558 loss: 6.9415361860810645\n",
      "epoch: 11 sentence: 550/1558 loss: 6.480460026050852\n",
      "epoch: 11 sentence: 560/1558 loss: 7.50770603061669\n",
      "epoch: 11 sentence: 570/1558 loss: 6.8427572591120285\n",
      "epoch: 11 sentence: 580/1558 loss: 7.263335991190647\n",
      "epoch: 11 sentence: 590/1558 loss: 6.607045453768646\n",
      "epoch: 11 sentence: 600/1558 loss: 6.239242952452797\n",
      "epoch: 11 sentence: 610/1558 loss: 7.4533498557819895\n",
      "epoch: 11 sentence: 620/1558 loss: 6.760571803966847\n",
      "epoch: 11 sentence: 630/1558 loss: 2.636871352352757\n",
      "epoch: 11 sentence: 640/1558 loss: 7.303071861783556\n",
      "epoch: 11 sentence: 650/1558 loss: 7.131956272154792\n",
      "epoch: 11 sentence: 660/1558 loss: 7.391601013581047\n",
      "epoch: 11 sentence: 670/1558 loss: 7.0554872629693515\n",
      "epoch: 11 sentence: 680/1558 loss: 7.695924414473472\n",
      "epoch: 11 sentence: 690/1558 loss: 6.987698211714511\n",
      "epoch: 11 sentence: 700/1558 loss: 2.6504330120888326\n",
      "epoch: 11 sentence: 710/1558 loss: 7.4561687678249475\n",
      "epoch: 11 sentence: 720/1558 loss: 6.745174174939523\n",
      "epoch: 11 sentence: 730/1558 loss: 2.6534308300219585\n",
      "epoch: 11 sentence: 740/1558 loss: 6.900071344782698\n",
      "epoch: 11 sentence: 750/1558 loss: 8.094014558115148\n",
      "epoch: 11 sentence: 760/1558 loss: 7.201148199047936\n",
      "epoch: 11 sentence: 770/1558 loss: 6.826692855891759\n",
      "epoch: 11 sentence: 780/1558 loss: 6.419391450067659\n",
      "epoch: 11 sentence: 790/1558 loss: 6.741798410189365\n",
      "epoch: 11 sentence: 800/1558 loss: 6.905921040959131\n",
      "epoch: 11 sentence: 810/1558 loss: 7.51060388355022\n",
      "epoch: 11 sentence: 820/1558 loss: 7.388369979724206\n",
      "epoch: 11 sentence: 830/1558 loss: 6.836231610764752\n",
      "epoch: 11 sentence: 840/1558 loss: 7.172087973710028\n",
      "epoch: 11 sentence: 850/1558 loss: 2.632292573724806\n",
      "epoch: 11 sentence: 860/1558 loss: 7.295705844885765\n",
      "epoch: 11 sentence: 870/1558 loss: 6.061670993362513\n",
      "epoch: 11 sentence: 880/1558 loss: 5.5557837370208265\n",
      "epoch: 11 sentence: 890/1558 loss: 6.340702056580858\n",
      "epoch: 11 sentence: 900/1558 loss: 6.09937598050762\n",
      "epoch: 11 sentence: 910/1558 loss: 7.120466854667699\n",
      "epoch: 11 sentence: 920/1558 loss: 6.964455788270364\n",
      "epoch: 11 sentence: 930/1558 loss: 6.772066407309249\n",
      "epoch: 11 sentence: 940/1558 loss: 6.609309721106464\n",
      "epoch: 11 sentence: 950/1558 loss: 6.314353830673404\n",
      "epoch: 11 sentence: 960/1558 loss: 6.469012334459329\n",
      "epoch: 11 sentence: 970/1558 loss: 6.5947280829939965\n",
      "epoch: 11 sentence: 980/1558 loss: 6.658985622999236\n",
      "epoch: 11 sentence: 990/1558 loss: 7.522482006662246\n",
      "epoch: 11 sentence: 1000/1558 loss: 6.94115083449409\n",
      "epoch: 11 sentence: 1010/1558 loss: 7.339439986881307\n",
      "epoch: 11 sentence: 1020/1558 loss: 7.760658786936193\n",
      "epoch: 11 sentence: 1030/1558 loss: 6.567708486225278\n",
      "epoch: 11 sentence: 1040/1558 loss: 6.627010209398951\n",
      "epoch: 11 sentence: 1050/1558 loss: 6.1894121531638095\n",
      "epoch: 11 sentence: 1060/1558 loss: 6.865882250006656\n",
      "epoch: 11 sentence: 1070/1558 loss: 7.2313147442201355\n",
      "epoch: 11 sentence: 1080/1558 loss: 6.561695044969709\n",
      "epoch: 11 sentence: 1090/1558 loss: 6.68192972539133\n",
      "epoch: 11 sentence: 1100/1558 loss: 7.079202173064157\n",
      "epoch: 11 sentence: 1110/1558 loss: 2.6869265577810664\n",
      "epoch: 11 sentence: 1120/1558 loss: 7.140533674783223\n",
      "epoch: 11 sentence: 1130/1558 loss: 6.053893245065341\n",
      "epoch: 11 sentence: 1140/1558 loss: 6.771220919688237\n",
      "epoch: 11 sentence: 1150/1558 loss: 2.6865806044446785\n",
      "epoch: 11 sentence: 1160/1558 loss: 6.492751949069132\n",
      "epoch: 11 sentence: 1170/1558 loss: 6.166948631153133\n",
      "epoch: 11 sentence: 1180/1558 loss: 5.299907821258777\n",
      "epoch: 11 sentence: 1190/1558 loss: 7.101925838735893\n",
      "epoch: 11 sentence: 1200/1558 loss: 6.866581293466199\n",
      "epoch: 11 sentence: 1210/1558 loss: 5.979162760849499\n",
      "epoch: 11 sentence: 1220/1558 loss: 6.3099407358582305\n",
      "epoch: 11 sentence: 1230/1558 loss: 5.933251279192432\n",
      "epoch: 11 sentence: 1240/1558 loss: 5.928834568925986\n",
      "epoch: 11 sentence: 1250/1558 loss: 6.732667893271053\n",
      "epoch: 11 sentence: 1260/1558 loss: 6.182240636722759\n",
      "epoch: 11 sentence: 1270/1558 loss: 6.671569092224592\n",
      "epoch: 11 sentence: 1280/1558 loss: 6.6657442739787935\n",
      "epoch: 11 sentence: 1290/1558 loss: 7.64088114003882\n",
      "epoch: 11 sentence: 1300/1558 loss: 2.6777595481319447\n",
      "epoch: 11 sentence: 1310/1558 loss: 7.199675552984749\n",
      "epoch: 11 sentence: 1320/1558 loss: 6.063419577226458\n",
      "epoch: 11 sentence: 1330/1558 loss: 5.900223683413062\n",
      "epoch: 11 sentence: 1340/1558 loss: 7.040307023466861\n",
      "epoch: 11 sentence: 1350/1558 loss: 6.267297580728995\n",
      "epoch: 11 sentence: 1360/1558 loss: 5.567818763761348\n",
      "epoch: 11 sentence: 1370/1558 loss: 6.985761897086826\n",
      "epoch: 11 sentence: 1380/1558 loss: 7.0804527976662905\n",
      "epoch: 11 sentence: 1390/1558 loss: 4.329918843444227\n",
      "epoch: 11 sentence: 1400/1558 loss: 7.048734762519473\n",
      "epoch: 11 sentence: 1410/1558 loss: 6.997398755845126\n",
      "epoch: 11 sentence: 1420/1558 loss: 7.055006111835942\n",
      "epoch: 11 sentence: 1430/1558 loss: 6.94915822970413\n",
      "epoch: 11 sentence: 1440/1558 loss: 7.242707975510847\n",
      "epoch: 11 sentence: 1450/1558 loss: 7.080571838361245\n",
      "epoch: 11 sentence: 1460/1558 loss: 6.990966882510078\n",
      "epoch: 11 sentence: 1470/1558 loss: 7.820063945047172\n",
      "epoch: 11 sentence: 1480/1558 loss: 6.948427536509877\n",
      "epoch: 11 sentence: 1490/1558 loss: 6.882333374198568\n",
      "epoch: 11 sentence: 1500/1558 loss: 7.118414040908055\n",
      "epoch: 11 sentence: 1510/1558 loss: 6.862466067816517\n",
      "epoch: 11 sentence: 1520/1558 loss: 6.168007706123593\n",
      "epoch: 11 sentence: 1530/1558 loss: 5.648694963058963\n",
      "epoch: 11 sentence: 1540/1558 loss: 6.987375535215542\n",
      "epoch: 11 sentence: 1550/1558 loss: 6.08684578513508\n",
      "epoch: 12 sentence: 0/1558 loss: 2.6887713656344903\n",
      "epoch: 12 sentence: 10/1558 loss: 6.480298634697461\n",
      "epoch: 12 sentence: 20/1558 loss: 6.769585278244256\n",
      "epoch: 12 sentence: 30/1558 loss: 6.325144361069113\n",
      "epoch: 12 sentence: 40/1558 loss: 6.905513162215064\n",
      "epoch: 12 sentence: 50/1558 loss: 7.168935402941091\n",
      "epoch: 12 sentence: 60/1558 loss: 6.1028464962165465\n",
      "epoch: 12 sentence: 70/1558 loss: 6.410663869570857\n",
      "epoch: 12 sentence: 80/1558 loss: 6.807069591711448\n",
      "epoch: 12 sentence: 90/1558 loss: 7.556554504542139\n",
      "epoch: 12 sentence: 100/1558 loss: 6.2251124790466426\n",
      "epoch: 12 sentence: 110/1558 loss: 7.195083225211417\n",
      "epoch: 12 sentence: 120/1558 loss: 7.546839631028142\n",
      "epoch: 12 sentence: 130/1558 loss: 7.180196001379665\n",
      "epoch: 12 sentence: 140/1558 loss: 5.857465041634737\n",
      "epoch: 12 sentence: 150/1558 loss: 6.442077035437154\n",
      "epoch: 12 sentence: 160/1558 loss: 6.603103530100134\n",
      "epoch: 12 sentence: 170/1558 loss: 6.896167071437193\n",
      "epoch: 12 sentence: 180/1558 loss: 6.855946192441844\n",
      "epoch: 12 sentence: 190/1558 loss: 7.261642014517855\n",
      "epoch: 12 sentence: 200/1558 loss: 6.764218985284536\n",
      "epoch: 12 sentence: 210/1558 loss: 6.7219888296109325\n",
      "epoch: 12 sentence: 220/1558 loss: 6.650613124959051\n",
      "epoch: 12 sentence: 230/1558 loss: 6.807401956553519\n",
      "epoch: 12 sentence: 240/1558 loss: 6.648182852560345\n",
      "epoch: 12 sentence: 250/1558 loss: 6.0558449097302\n",
      "epoch: 12 sentence: 260/1558 loss: 7.551883974456178\n",
      "epoch: 12 sentence: 270/1558 loss: 6.874599378454012\n",
      "epoch: 12 sentence: 280/1558 loss: 6.375610121561343\n",
      "epoch: 12 sentence: 290/1558 loss: 7.308256908104165\n",
      "epoch: 12 sentence: 300/1558 loss: 6.422099134842457\n",
      "epoch: 12 sentence: 310/1558 loss: 7.672364679410304\n",
      "epoch: 12 sentence: 320/1558 loss: 6.948984322548046\n",
      "epoch: 12 sentence: 330/1558 loss: 6.975672430544811\n",
      "epoch: 12 sentence: 340/1558 loss: 6.193803805947062\n",
      "epoch: 12 sentence: 350/1558 loss: 5.122899405933558\n",
      "epoch: 12 sentence: 360/1558 loss: 6.730822116146383\n",
      "epoch: 12 sentence: 370/1558 loss: 6.465750940272753\n",
      "epoch: 12 sentence: 380/1558 loss: 6.509484043691346\n",
      "epoch: 12 sentence: 390/1558 loss: 6.786952337235682\n",
      "epoch: 12 sentence: 400/1558 loss: 5.887772836343091\n",
      "epoch: 12 sentence: 410/1558 loss: 6.634657012426021\n",
      "epoch: 12 sentence: 420/1558 loss: 7.118108382774579\n",
      "epoch: 12 sentence: 430/1558 loss: 6.052360181967868\n",
      "epoch: 12 sentence: 440/1558 loss: 7.810374418818978\n",
      "epoch: 12 sentence: 450/1558 loss: 5.845236189089294\n",
      "epoch: 12 sentence: 460/1558 loss: 5.988649931624616\n",
      "epoch: 12 sentence: 470/1558 loss: 5.8881793875184645\n",
      "epoch: 12 sentence: 480/1558 loss: 6.865461225287845\n",
      "epoch: 12 sentence: 490/1558 loss: 7.129807873659718\n",
      "epoch: 12 sentence: 500/1558 loss: 6.857605083961094\n",
      "epoch: 12 sentence: 510/1558 loss: 6.612981611751035\n",
      "epoch: 12 sentence: 520/1558 loss: 7.286275730262122\n",
      "epoch: 12 sentence: 530/1558 loss: 6.503675257363571\n",
      "epoch: 12 sentence: 540/1558 loss: 6.463524851473631\n",
      "epoch: 12 sentence: 550/1558 loss: 7.136548957026014\n",
      "epoch: 12 sentence: 560/1558 loss: 6.512909424328951\n",
      "epoch: 12 sentence: 570/1558 loss: 7.211479099739172\n",
      "epoch: 12 sentence: 580/1558 loss: 5.494565685430617\n",
      "epoch: 12 sentence: 590/1558 loss: 7.780057731046058\n",
      "epoch: 12 sentence: 600/1558 loss: 6.766393399880752\n",
      "epoch: 12 sentence: 610/1558 loss: 7.160184101685084\n",
      "epoch: 12 sentence: 620/1558 loss: 6.575867283577066\n",
      "epoch: 12 sentence: 630/1558 loss: 6.220149595993519\n",
      "epoch: 12 sentence: 640/1558 loss: 6.982311709409096\n",
      "epoch: 12 sentence: 650/1558 loss: 6.745908532863149\n",
      "epoch: 12 sentence: 660/1558 loss: 2.7564212788445506\n",
      "epoch: 12 sentence: 670/1558 loss: 7.284494900245717\n",
      "epoch: 12 sentence: 680/1558 loss: 7.2884318010311935\n",
      "epoch: 12 sentence: 690/1558 loss: 5.62338848273219\n",
      "epoch: 12 sentence: 700/1558 loss: 5.451849670094474\n",
      "epoch: 12 sentence: 710/1558 loss: 2.780799657259139\n",
      "epoch: 12 sentence: 720/1558 loss: 7.729314218399725\n",
      "epoch: 12 sentence: 730/1558 loss: 4.933465792029246\n",
      "epoch: 12 sentence: 740/1558 loss: 6.721675315231715\n",
      "epoch: 12 sentence: 750/1558 loss: 7.992322183878014\n",
      "epoch: 12 sentence: 760/1558 loss: 7.368161065616099\n",
      "epoch: 12 sentence: 770/1558 loss: 2.755232656841769\n",
      "epoch: 12 sentence: 780/1558 loss: 5.4122729101207865\n",
      "epoch: 12 sentence: 790/1558 loss: 6.826682042066643\n",
      "epoch: 12 sentence: 800/1558 loss: 5.748668196113166\n",
      "epoch: 12 sentence: 810/1558 loss: 6.951042118856363\n",
      "epoch: 12 sentence: 820/1558 loss: 2.7466561596480075\n",
      "epoch: 12 sentence: 830/1558 loss: 6.450371754606746\n",
      "epoch: 12 sentence: 840/1558 loss: 5.411376121982696\n",
      "epoch: 12 sentence: 850/1558 loss: 6.149108770331348\n",
      "epoch: 12 sentence: 860/1558 loss: 2.684244652347346\n",
      "epoch: 12 sentence: 870/1558 loss: 6.990066626095353\n",
      "epoch: 12 sentence: 880/1558 loss: 6.9215728925639635\n",
      "epoch: 12 sentence: 890/1558 loss: 7.27358131259474\n",
      "epoch: 12 sentence: 900/1558 loss: 6.895727667296149\n",
      "epoch: 12 sentence: 910/1558 loss: 7.149013789517625\n",
      "epoch: 12 sentence: 920/1558 loss: 6.9641330363459995\n",
      "epoch: 12 sentence: 930/1558 loss: 6.10583257939353\n",
      "epoch: 12 sentence: 940/1558 loss: 7.105734697131723\n",
      "epoch: 12 sentence: 950/1558 loss: 6.530215769431776\n",
      "epoch: 12 sentence: 960/1558 loss: 7.309767599453522\n",
      "epoch: 12 sentence: 970/1558 loss: 6.804196315026853\n",
      "epoch: 12 sentence: 980/1558 loss: 6.55212123535502\n",
      "epoch: 12 sentence: 990/1558 loss: 7.0214010859220535\n",
      "epoch: 12 sentence: 1000/1558 loss: 7.093715303736551\n",
      "epoch: 12 sentence: 1010/1558 loss: 7.163132919355476\n",
      "epoch: 12 sentence: 1020/1558 loss: 6.896466675727127\n",
      "epoch: 12 sentence: 1030/1558 loss: 6.83775985806294\n",
      "epoch: 12 sentence: 1040/1558 loss: 6.73366114626252\n",
      "epoch: 12 sentence: 1050/1558 loss: 7.510544975891294\n",
      "epoch: 12 sentence: 1060/1558 loss: 6.424867015727222\n",
      "epoch: 12 sentence: 1070/1558 loss: 6.167700076527024\n",
      "epoch: 12 sentence: 1080/1558 loss: 7.3794721957148655\n",
      "epoch: 12 sentence: 1090/1558 loss: 5.871779587140211\n",
      "epoch: 12 sentence: 1100/1558 loss: 6.805448093996391\n",
      "epoch: 12 sentence: 1110/1558 loss: 7.026872679635908\n",
      "epoch: 12 sentence: 1120/1558 loss: 5.46533637762827\n",
      "epoch: 12 sentence: 1130/1558 loss: 6.500184447149896\n",
      "epoch: 12 sentence: 1140/1558 loss: 7.062565110207969\n",
      "epoch: 12 sentence: 1150/1558 loss: 6.855848872001583\n",
      "epoch: 12 sentence: 1160/1558 loss: 6.911779119129618\n",
      "epoch: 12 sentence: 1170/1558 loss: 7.289013187907819\n",
      "epoch: 12 sentence: 1180/1558 loss: 7.967771744264434\n",
      "epoch: 12 sentence: 1190/1558 loss: 7.027119145030168\n",
      "epoch: 12 sentence: 1200/1558 loss: 6.711545050328108\n",
      "epoch: 12 sentence: 1210/1558 loss: 6.79548563476593\n",
      "epoch: 12 sentence: 1220/1558 loss: 6.50901862382028\n",
      "epoch: 12 sentence: 1230/1558 loss: 5.42996769765517\n",
      "epoch: 12 sentence: 1240/1558 loss: 2.6914808198425844\n",
      "epoch: 12 sentence: 1250/1558 loss: 7.099067506305309\n",
      "epoch: 12 sentence: 1260/1558 loss: 6.104591221085659\n",
      "epoch: 12 sentence: 1270/1558 loss: 7.027185167814255\n",
      "epoch: 12 sentence: 1280/1558 loss: 5.157820096735671\n",
      "epoch: 12 sentence: 1290/1558 loss: 2.7005006519282957\n",
      "epoch: 12 sentence: 1300/1558 loss: 7.355781866820422\n",
      "epoch: 12 sentence: 1310/1558 loss: 2.7034054863879455\n",
      "epoch: 12 sentence: 1320/1558 loss: 7.330045057077442\n",
      "epoch: 12 sentence: 1330/1558 loss: 6.916377421514844\n",
      "epoch: 12 sentence: 1340/1558 loss: 4.089092286130416\n",
      "epoch: 12 sentence: 1350/1558 loss: 6.75018196973258\n",
      "epoch: 12 sentence: 1360/1558 loss: 6.672129057200073\n",
      "epoch: 12 sentence: 1370/1558 loss: 2.7066723973805757\n",
      "epoch: 12 sentence: 1380/1558 loss: 5.880535590669374\n",
      "epoch: 12 sentence: 1390/1558 loss: 7.139412698422086\n",
      "epoch: 12 sentence: 1400/1558 loss: 7.166173482112876\n",
      "epoch: 12 sentence: 1410/1558 loss: 7.216207475324221\n",
      "epoch: 12 sentence: 1420/1558 loss: 6.6355441989656105\n",
      "epoch: 12 sentence: 1430/1558 loss: 5.38982392112899\n",
      "epoch: 12 sentence: 1440/1558 loss: 6.811327193421708\n",
      "epoch: 12 sentence: 1450/1558 loss: 6.6229012369595655\n",
      "epoch: 12 sentence: 1460/1558 loss: 6.686050102935702\n",
      "epoch: 12 sentence: 1470/1558 loss: 6.855336983334415\n",
      "epoch: 12 sentence: 1480/1558 loss: 7.061029269389724\n",
      "epoch: 12 sentence: 1490/1558 loss: 7.26795068253324\n",
      "epoch: 12 sentence: 1500/1558 loss: 6.752820178470184\n",
      "epoch: 12 sentence: 1510/1558 loss: 6.2026223947922485\n",
      "epoch: 12 sentence: 1520/1558 loss: 7.76975667200591\n",
      "epoch: 12 sentence: 1530/1558 loss: 6.809599441410655\n",
      "epoch: 12 sentence: 1540/1558 loss: 6.818865860071582\n",
      "epoch: 12 sentence: 1550/1558 loss: 7.352315996278859\n",
      "epoch: 13 sentence: 0/1558 loss: 2.6701694005056282\n",
      "epoch: 13 sentence: 10/1558 loss: 6.406199114603816\n",
      "epoch: 13 sentence: 20/1558 loss: 6.860934407645428\n",
      "epoch: 13 sentence: 30/1558 loss: 7.818535485783485\n",
      "epoch: 13 sentence: 40/1558 loss: 6.515400105001774\n",
      "epoch: 13 sentence: 50/1558 loss: 2.6387811761437545\n",
      "epoch: 13 sentence: 60/1558 loss: 6.096302484009303\n",
      "epoch: 13 sentence: 70/1558 loss: 2.633725924463445\n",
      "epoch: 13 sentence: 80/1558 loss: 7.110068876539361\n",
      "epoch: 13 sentence: 90/1558 loss: 6.646052963539248\n",
      "epoch: 13 sentence: 100/1558 loss: 2.6159952464906415\n",
      "epoch: 13 sentence: 110/1558 loss: 7.099353316089689\n",
      "epoch: 13 sentence: 120/1558 loss: 6.715112582415725\n",
      "epoch: 13 sentence: 130/1558 loss: 6.514787494514428\n",
      "epoch: 13 sentence: 140/1558 loss: 7.038738401904917\n",
      "epoch: 13 sentence: 150/1558 loss: 6.80597787721328\n",
      "epoch: 13 sentence: 160/1558 loss: 7.3300330673211045\n",
      "epoch: 13 sentence: 170/1558 loss: 6.288633437763956\n",
      "epoch: 13 sentence: 180/1558 loss: 6.4649271191888475\n",
      "epoch: 13 sentence: 190/1558 loss: 6.797457316795275\n",
      "epoch: 13 sentence: 200/1558 loss: 6.357214126671667\n",
      "epoch: 13 sentence: 210/1558 loss: 6.743955086341405\n",
      "epoch: 13 sentence: 220/1558 loss: 6.717832344644199\n",
      "epoch: 13 sentence: 230/1558 loss: 6.863534332020108\n",
      "epoch: 13 sentence: 240/1558 loss: 2.6016735907056683\n",
      "epoch: 13 sentence: 250/1558 loss: 6.869862654604142\n",
      "epoch: 13 sentence: 260/1558 loss: 6.3292304753175825\n",
      "epoch: 13 sentence: 270/1558 loss: 7.1999885910153205\n",
      "epoch: 13 sentence: 280/1558 loss: 6.267745797072001\n",
      "epoch: 13 sentence: 290/1558 loss: 6.183399062249019\n",
      "epoch: 13 sentence: 300/1558 loss: 6.211461619267865\n",
      "epoch: 13 sentence: 310/1558 loss: 6.746881336298608\n",
      "epoch: 13 sentence: 320/1558 loss: 6.738223288357691\n",
      "epoch: 13 sentence: 330/1558 loss: 5.263293250041821\n",
      "epoch: 13 sentence: 340/1558 loss: 6.583847164084672\n",
      "epoch: 13 sentence: 350/1558 loss: 6.694017599887793\n",
      "epoch: 13 sentence: 360/1558 loss: 2.6516603444863214\n",
      "epoch: 13 sentence: 370/1558 loss: 7.738620503236585\n",
      "epoch: 13 sentence: 380/1558 loss: 6.944813383150495\n",
      "epoch: 13 sentence: 390/1558 loss: 6.729342224268134\n",
      "epoch: 13 sentence: 400/1558 loss: 5.738789172428827\n",
      "epoch: 13 sentence: 410/1558 loss: 7.1374566804634245\n",
      "epoch: 13 sentence: 420/1558 loss: 6.439843042570421\n",
      "epoch: 13 sentence: 430/1558 loss: 6.978881983664796\n",
      "epoch: 13 sentence: 440/1558 loss: 7.699317023742907\n",
      "epoch: 13 sentence: 450/1558 loss: 7.409996500516183\n",
      "epoch: 13 sentence: 460/1558 loss: 7.263943456034863\n",
      "epoch: 13 sentence: 470/1558 loss: 6.474429129082877\n",
      "epoch: 13 sentence: 480/1558 loss: 7.066174124512214\n",
      "epoch: 13 sentence: 490/1558 loss: 6.424838828428857\n",
      "epoch: 13 sentence: 500/1558 loss: 6.121071957819047\n",
      "epoch: 13 sentence: 510/1558 loss: 6.7311666907470755\n",
      "epoch: 13 sentence: 520/1558 loss: 6.958162920847548\n",
      "epoch: 13 sentence: 530/1558 loss: 7.106522312554422\n",
      "epoch: 13 sentence: 540/1558 loss: 4.985050496444892\n",
      "epoch: 13 sentence: 550/1558 loss: 5.569001257215404\n",
      "epoch: 13 sentence: 560/1558 loss: 7.221800335551194\n",
      "epoch: 13 sentence: 570/1558 loss: 6.641691972277554\n",
      "epoch: 13 sentence: 580/1558 loss: 7.763833022783914\n",
      "epoch: 13 sentence: 590/1558 loss: 6.6566525815704525\n",
      "epoch: 13 sentence: 600/1558 loss: 2.6271353276040212\n",
      "epoch: 13 sentence: 610/1558 loss: 7.286631640279402\n",
      "epoch: 13 sentence: 620/1558 loss: 7.311560637867278\n",
      "epoch: 13 sentence: 630/1558 loss: 4.302731042115513\n",
      "epoch: 13 sentence: 640/1558 loss: 6.055180288959642\n",
      "epoch: 13 sentence: 650/1558 loss: 6.383402202399507\n",
      "epoch: 13 sentence: 660/1558 loss: 6.732033624263477\n",
      "epoch: 13 sentence: 670/1558 loss: 5.609762766482865\n",
      "epoch: 13 sentence: 680/1558 loss: 6.995949320470805\n",
      "epoch: 13 sentence: 690/1558 loss: 6.796627268496908\n",
      "epoch: 13 sentence: 700/1558 loss: 7.657884719885086\n",
      "epoch: 13 sentence: 710/1558 loss: 4.49316079317189\n",
      "epoch: 13 sentence: 720/1558 loss: 7.652370308425276\n",
      "epoch: 13 sentence: 730/1558 loss: 7.0290640335358745\n",
      "epoch: 13 sentence: 740/1558 loss: 6.9552957991241735\n",
      "epoch: 13 sentence: 750/1558 loss: 7.009643375112567\n",
      "epoch: 13 sentence: 760/1558 loss: 5.337763916820425\n",
      "epoch: 13 sentence: 770/1558 loss: 5.8776453013502215\n",
      "epoch: 13 sentence: 780/1558 loss: 7.501119582034267\n",
      "epoch: 13 sentence: 790/1558 loss: 6.82728908917525\n",
      "epoch: 13 sentence: 800/1558 loss: 6.388951622999823\n",
      "epoch: 13 sentence: 810/1558 loss: 2.6080444646037693\n",
      "epoch: 13 sentence: 820/1558 loss: 6.814692138598914\n",
      "epoch: 13 sentence: 830/1558 loss: 7.435896525179081\n",
      "epoch: 13 sentence: 840/1558 loss: 4.213953706347787\n",
      "epoch: 13 sentence: 850/1558 loss: 6.964147713951941\n",
      "epoch: 13 sentence: 860/1558 loss: 6.605355476023735\n",
      "epoch: 13 sentence: 870/1558 loss: 7.006012776034812\n",
      "epoch: 13 sentence: 880/1558 loss: 6.470254982132165\n",
      "epoch: 13 sentence: 890/1558 loss: 6.506487644068964\n",
      "epoch: 13 sentence: 900/1558 loss: 7.081114844009429\n",
      "epoch: 13 sentence: 910/1558 loss: 6.877273183637053\n",
      "epoch: 13 sentence: 920/1558 loss: 7.766263213939914\n",
      "epoch: 13 sentence: 930/1558 loss: 6.7112585081911345\n",
      "epoch: 13 sentence: 940/1558 loss: 6.622071111332377\n",
      "epoch: 13 sentence: 950/1558 loss: 6.759216316696629\n",
      "epoch: 13 sentence: 960/1558 loss: 6.066605660658867\n",
      "epoch: 13 sentence: 970/1558 loss: 2.6604585393058304\n",
      "epoch: 13 sentence: 980/1558 loss: 7.169988659856089\n",
      "epoch: 13 sentence: 990/1558 loss: 6.834546518760677\n",
      "epoch: 13 sentence: 1000/1558 loss: 6.397357681070289\n",
      "epoch: 13 sentence: 1010/1558 loss: 6.8569043884275\n",
      "epoch: 13 sentence: 1020/1558 loss: 7.127140065633901\n",
      "epoch: 13 sentence: 1030/1558 loss: 7.082721947165352\n",
      "epoch: 13 sentence: 1040/1558 loss: 7.062569917463971\n",
      "epoch: 13 sentence: 1050/1558 loss: 7.825314281481953\n",
      "epoch: 13 sentence: 1060/1558 loss: 6.754693632377346\n",
      "epoch: 13 sentence: 1070/1558 loss: 6.954186045978815\n",
      "epoch: 13 sentence: 1080/1558 loss: 6.4603507310736275\n",
      "epoch: 13 sentence: 1090/1558 loss: 6.670406506144501\n",
      "epoch: 13 sentence: 1100/1558 loss: 6.7207539732127035\n",
      "epoch: 13 sentence: 1110/1558 loss: 6.775147826898284\n",
      "epoch: 13 sentence: 1120/1558 loss: 7.59391685855724\n",
      "epoch: 13 sentence: 1130/1558 loss: 6.214179925329488\n",
      "epoch: 13 sentence: 1140/1558 loss: 6.5865285724159435\n",
      "epoch: 13 sentence: 1150/1558 loss: 7.0614489355295635\n",
      "epoch: 13 sentence: 1160/1558 loss: 6.836201110161876\n",
      "epoch: 13 sentence: 1170/1558 loss: 2.673805427965026\n",
      "epoch: 13 sentence: 1180/1558 loss: 5.36948984131572\n",
      "epoch: 13 sentence: 1190/1558 loss: 6.639456893747893\n",
      "epoch: 13 sentence: 1200/1558 loss: 6.634091197274735\n",
      "epoch: 13 sentence: 1210/1558 loss: 2.6835688452500963\n",
      "epoch: 13 sentence: 1220/1558 loss: 6.75198884794153\n",
      "epoch: 13 sentence: 1230/1558 loss: 5.76916562876148\n",
      "epoch: 13 sentence: 1240/1558 loss: 7.3155802008172985\n",
      "epoch: 13 sentence: 1250/1558 loss: 6.496566074036196\n",
      "epoch: 13 sentence: 1260/1558 loss: 6.239137110635805\n",
      "epoch: 13 sentence: 1270/1558 loss: 6.5618355209443315\n",
      "epoch: 13 sentence: 1280/1558 loss: 7.650337682541765\n",
      "epoch: 13 sentence: 1290/1558 loss: 6.303489188712913\n",
      "epoch: 13 sentence: 1300/1558 loss: 6.799299213342795\n",
      "epoch: 13 sentence: 1310/1558 loss: 7.626468617429205\n",
      "epoch: 13 sentence: 1320/1558 loss: 6.361757689701433\n",
      "epoch: 13 sentence: 1330/1558 loss: 6.643924796412855\n",
      "epoch: 13 sentence: 1340/1558 loss: 7.398476018096991\n",
      "epoch: 13 sentence: 1350/1558 loss: 6.293210294635416\n",
      "epoch: 13 sentence: 1360/1558 loss: 6.908518235781144\n",
      "epoch: 13 sentence: 1370/1558 loss: 6.717501327467412\n",
      "epoch: 13 sentence: 1380/1558 loss: 6.940227628704927\n",
      "epoch: 13 sentence: 1390/1558 loss: 6.861349289479851\n",
      "epoch: 13 sentence: 1400/1558 loss: 6.814006628180166\n",
      "epoch: 13 sentence: 1410/1558 loss: 6.731121401594633\n",
      "epoch: 13 sentence: 1420/1558 loss: 5.517319925278253\n",
      "epoch: 13 sentence: 1430/1558 loss: 6.312991836177526\n",
      "epoch: 13 sentence: 1440/1558 loss: 2.688025204808994\n",
      "epoch: 13 sentence: 1450/1558 loss: 6.978396588399082\n",
      "epoch: 13 sentence: 1460/1558 loss: 7.239115684029577\n",
      "epoch: 13 sentence: 1470/1558 loss: 7.012129302678047\n",
      "epoch: 13 sentence: 1480/1558 loss: 5.982295755548247\n",
      "epoch: 13 sentence: 1490/1558 loss: 6.2613878544442425\n",
      "epoch: 13 sentence: 1500/1558 loss: 6.058510310190848\n",
      "epoch: 13 sentence: 1510/1558 loss: 2.6967559788588567\n",
      "epoch: 13 sentence: 1520/1558 loss: 7.228934616400611\n",
      "epoch: 13 sentence: 1530/1558 loss: 5.445950035775054\n",
      "epoch: 13 sentence: 1540/1558 loss: 6.95174450515678\n",
      "epoch: 13 sentence: 1550/1558 loss: 7.0753870734454924\n",
      "epoch: 14 sentence: 0/1558 loss: 7.823085701084684\n",
      "epoch: 14 sentence: 10/1558 loss: 6.981683113119886\n",
      "epoch: 14 sentence: 20/1558 loss: 6.800931036839407\n",
      "epoch: 14 sentence: 30/1558 loss: 6.852070555305786\n",
      "epoch: 14 sentence: 40/1558 loss: 6.794288422969584\n",
      "epoch: 14 sentence: 50/1558 loss: 6.688733859332586\n",
      "epoch: 14 sentence: 60/1558 loss: 6.409834809524033\n",
      "epoch: 14 sentence: 70/1558 loss: 6.8538295125536415\n",
      "epoch: 14 sentence: 80/1558 loss: 7.253860126258575\n",
      "epoch: 14 sentence: 90/1558 loss: 6.141184748253774\n",
      "epoch: 14 sentence: 100/1558 loss: 6.585835608866491\n",
      "epoch: 14 sentence: 110/1558 loss: 6.217388037011421\n",
      "epoch: 14 sentence: 120/1558 loss: 7.861671175725449\n",
      "epoch: 14 sentence: 130/1558 loss: 7.113348762874599\n",
      "epoch: 14 sentence: 140/1558 loss: 7.291576459351516\n",
      "epoch: 14 sentence: 150/1558 loss: 7.462902251966547\n",
      "epoch: 14 sentence: 160/1558 loss: 7.056250007438371\n",
      "epoch: 14 sentence: 170/1558 loss: 7.174146365332399\n",
      "epoch: 14 sentence: 180/1558 loss: 6.531280384181786\n",
      "epoch: 14 sentence: 190/1558 loss: 7.034039183348758\n",
      "epoch: 14 sentence: 200/1558 loss: 6.983664220734099\n",
      "epoch: 14 sentence: 210/1558 loss: 7.498324004288017\n",
      "epoch: 14 sentence: 220/1558 loss: 6.801508726077233\n",
      "epoch: 14 sentence: 230/1558 loss: 7.142831232847018\n",
      "epoch: 14 sentence: 240/1558 loss: 6.697006335052439\n",
      "epoch: 14 sentence: 250/1558 loss: 6.264372849724198\n",
      "epoch: 14 sentence: 260/1558 loss: 6.202809833816086\n",
      "epoch: 14 sentence: 270/1558 loss: 6.771754689998302\n",
      "epoch: 14 sentence: 280/1558 loss: 5.875462453338038\n",
      "epoch: 14 sentence: 290/1558 loss: 7.21834255127504\n",
      "epoch: 14 sentence: 300/1558 loss: 6.11902095863136\n",
      "epoch: 14 sentence: 310/1558 loss: 7.131315020425653\n",
      "epoch: 14 sentence: 320/1558 loss: 6.112283352191567\n",
      "epoch: 14 sentence: 330/1558 loss: 6.740515982793679\n",
      "epoch: 14 sentence: 340/1558 loss: 7.409705687410864\n",
      "epoch: 14 sentence: 350/1558 loss: 2.7443302397318714\n",
      "epoch: 14 sentence: 360/1558 loss: 6.610578942379226\n",
      "epoch: 14 sentence: 370/1558 loss: 7.057421267677351\n",
      "epoch: 14 sentence: 380/1558 loss: 6.198975313624018\n",
      "epoch: 14 sentence: 390/1558 loss: 6.4034803265386016\n",
      "epoch: 14 sentence: 400/1558 loss: 7.132810836168652\n",
      "epoch: 14 sentence: 410/1558 loss: 6.367872304756665\n",
      "epoch: 14 sentence: 420/1558 loss: 5.727929870884984\n",
      "epoch: 14 sentence: 430/1558 loss: 6.40474252208235\n",
      "epoch: 14 sentence: 440/1558 loss: 6.666108788531028\n",
      "epoch: 14 sentence: 450/1558 loss: 7.985533793189611\n",
      "epoch: 14 sentence: 460/1558 loss: 5.759901135419061\n",
      "epoch: 14 sentence: 470/1558 loss: 5.556914900649862\n",
      "epoch: 14 sentence: 480/1558 loss: 5.636041269564038\n",
      "epoch: 14 sentence: 490/1558 loss: 6.641148780508581\n",
      "epoch: 14 sentence: 500/1558 loss: 6.690078380695343\n",
      "epoch: 14 sentence: 510/1558 loss: 6.224193501159173\n",
      "epoch: 14 sentence: 520/1558 loss: 7.03894121941659\n",
      "epoch: 14 sentence: 530/1558 loss: 6.041316680679914\n",
      "epoch: 14 sentence: 540/1558 loss: 2.7725563190939733\n",
      "epoch: 14 sentence: 550/1558 loss: 6.8597338959192165\n",
      "epoch: 14 sentence: 560/1558 loss: 6.401246939945192\n",
      "epoch: 14 sentence: 570/1558 loss: 6.931154039732057\n",
      "epoch: 14 sentence: 580/1558 loss: 6.972910593105419\n",
      "epoch: 14 sentence: 590/1558 loss: 6.701026546002442\n",
      "epoch: 14 sentence: 600/1558 loss: 7.2764076723391184\n",
      "epoch: 14 sentence: 610/1558 loss: 6.474436426876513\n",
      "epoch: 14 sentence: 620/1558 loss: 6.410317728053765\n",
      "epoch: 14 sentence: 630/1558 loss: 7.153916554580488\n",
      "epoch: 14 sentence: 640/1558 loss: 6.457035798422142\n",
      "epoch: 14 sentence: 650/1558 loss: 6.0915479394523695\n",
      "epoch: 14 sentence: 660/1558 loss: 6.9037498093570004\n",
      "epoch: 14 sentence: 670/1558 loss: 6.621335234584518\n",
      "epoch: 14 sentence: 680/1558 loss: 6.81994431586018\n",
      "epoch: 14 sentence: 690/1558 loss: 6.818844202665229\n",
      "epoch: 14 sentence: 700/1558 loss: 6.652950507026319\n",
      "epoch: 14 sentence: 710/1558 loss: 5.9611425672265606\n",
      "epoch: 14 sentence: 720/1558 loss: 6.947115204592865\n",
      "epoch: 14 sentence: 730/1558 loss: 6.90513290112765\n",
      "epoch: 14 sentence: 740/1558 loss: 6.020529691586056\n",
      "epoch: 14 sentence: 750/1558 loss: 6.453233541070673\n",
      "epoch: 14 sentence: 760/1558 loss: 7.234087484436796\n",
      "epoch: 14 sentence: 770/1558 loss: 6.372949530951112\n",
      "epoch: 14 sentence: 780/1558 loss: 7.701387268698039\n",
      "epoch: 14 sentence: 790/1558 loss: 7.688292618891643\n",
      "epoch: 14 sentence: 800/1558 loss: 6.370502008610089\n",
      "epoch: 14 sentence: 810/1558 loss: 2.7892264522361097\n",
      "epoch: 14 sentence: 820/1558 loss: 6.9986853682096415\n",
      "epoch: 14 sentence: 830/1558 loss: 2.7717972093864396\n",
      "epoch: 14 sentence: 840/1558 loss: 6.435982213804019\n",
      "epoch: 14 sentence: 850/1558 loss: 6.611908816505552\n",
      "epoch: 14 sentence: 860/1558 loss: 7.416306964679538\n",
      "epoch: 14 sentence: 870/1558 loss: 6.071271888948383\n",
      "epoch: 14 sentence: 880/1558 loss: 7.262745661928244\n",
      "epoch: 14 sentence: 890/1558 loss: 6.812480188244993\n",
      "epoch: 14 sentence: 900/1558 loss: 6.452610963945796\n",
      "epoch: 14 sentence: 910/1558 loss: 7.628541727533761\n",
      "epoch: 14 sentence: 920/1558 loss: 7.502684434770391\n",
      "epoch: 14 sentence: 930/1558 loss: 7.645022609603953\n",
      "epoch: 14 sentence: 940/1558 loss: 5.992082724701244\n",
      "epoch: 14 sentence: 950/1558 loss: 6.9896207965539725\n",
      "epoch: 14 sentence: 960/1558 loss: 6.907292449058922\n",
      "epoch: 14 sentence: 970/1558 loss: 6.785050523280022\n",
      "epoch: 14 sentence: 980/1558 loss: 2.722113693541923\n",
      "epoch: 14 sentence: 990/1558 loss: 6.703578792611305\n",
      "epoch: 14 sentence: 1000/1558 loss: 6.70022681783157\n",
      "epoch: 14 sentence: 1010/1558 loss: 6.0597930546211485\n",
      "epoch: 14 sentence: 1020/1558 loss: 5.274323514412778\n",
      "epoch: 14 sentence: 1030/1558 loss: 6.676888416099859\n",
      "epoch: 14 sentence: 1040/1558 loss: 6.87021901077145\n",
      "epoch: 14 sentence: 1050/1558 loss: 2.694852127556207\n",
      "epoch: 14 sentence: 1060/1558 loss: 2.680690970805176\n",
      "epoch: 14 sentence: 1070/1558 loss: 2.671136813303288\n",
      "epoch: 14 sentence: 1080/1558 loss: 6.3762947074401115\n",
      "epoch: 14 sentence: 1090/1558 loss: 6.457056950527885\n",
      "epoch: 14 sentence: 1100/1558 loss: 7.313533276479135\n",
      "epoch: 14 sentence: 1110/1558 loss: 7.083974703356212\n",
      "epoch: 14 sentence: 1120/1558 loss: 5.855838855937685\n",
      "epoch: 14 sentence: 1130/1558 loss: 5.560047068375758\n",
      "epoch: 14 sentence: 1140/1558 loss: 6.809734337285681\n",
      "epoch: 14 sentence: 1150/1558 loss: 3.292298625251263\n",
      "epoch: 14 sentence: 1160/1558 loss: 6.639745917531921\n",
      "epoch: 14 sentence: 1170/1558 loss: 6.18937576678717\n",
      "epoch: 14 sentence: 1180/1558 loss: 7.6130277290684365\n",
      "epoch: 14 sentence: 1190/1558 loss: 6.363980465372507\n",
      "epoch: 14 sentence: 1200/1558 loss: 6.8348541846368045\n",
      "epoch: 14 sentence: 1210/1558 loss: 7.334916681740587\n",
      "epoch: 14 sentence: 1220/1558 loss: 6.693528021620478\n",
      "epoch: 14 sentence: 1230/1558 loss: 6.486312997526191\n",
      "epoch: 14 sentence: 1240/1558 loss: 7.424618496091291\n",
      "epoch: 14 sentence: 1250/1558 loss: 6.840262021451283\n",
      "epoch: 14 sentence: 1260/1558 loss: 6.113436459667323\n",
      "epoch: 14 sentence: 1270/1558 loss: 2.699180649423979\n",
      "epoch: 14 sentence: 1280/1558 loss: 5.9120799521048895\n",
      "epoch: 14 sentence: 1290/1558 loss: 5.077294664639169\n",
      "epoch: 14 sentence: 1300/1558 loss: 7.342360303046073\n",
      "epoch: 14 sentence: 1310/1558 loss: 6.421921812092282\n",
      "epoch: 14 sentence: 1320/1558 loss: 5.120987792372281\n",
      "epoch: 14 sentence: 1330/1558 loss: 6.816267882873563\n",
      "epoch: 14 sentence: 1340/1558 loss: 6.720510822365834\n",
      "epoch: 14 sentence: 1350/1558 loss: 6.983519031358624\n",
      "epoch: 14 sentence: 1360/1558 loss: 5.457542982058537\n",
      "epoch: 14 sentence: 1370/1558 loss: 6.388651101917304\n",
      "epoch: 14 sentence: 1380/1558 loss: 5.843530962865576\n",
      "epoch: 14 sentence: 1390/1558 loss: 7.13678669768021\n",
      "epoch: 14 sentence: 1400/1558 loss: 6.666212223598686\n",
      "epoch: 14 sentence: 1410/1558 loss: 6.149094698657478\n",
      "epoch: 14 sentence: 1420/1558 loss: 6.582219807179072\n",
      "epoch: 14 sentence: 1430/1558 loss: 2.695289707706601\n",
      "epoch: 14 sentence: 1440/1558 loss: 6.762031215537733\n",
      "epoch: 14 sentence: 1450/1558 loss: 6.503108204907614\n",
      "epoch: 14 sentence: 1460/1558 loss: 7.267851741431715\n",
      "epoch: 14 sentence: 1470/1558 loss: 6.629012941687213\n",
      "epoch: 14 sentence: 1480/1558 loss: 6.557934876144958\n",
      "epoch: 14 sentence: 1490/1558 loss: 5.856403780341691\n",
      "epoch: 14 sentence: 1500/1558 loss: 6.565647491884891\n",
      "epoch: 14 sentence: 1510/1558 loss: 6.100109569204224\n",
      "epoch: 14 sentence: 1520/1558 loss: 6.3519629368375075\n",
      "epoch: 14 sentence: 1530/1558 loss: 7.322667046645364\n",
      "epoch: 14 sentence: 1540/1558 loss: 6.608396938581098\n",
      "epoch: 14 sentence: 1550/1558 loss: 2.6621571480377826\n",
      "epoch: 15 sentence: 0/1558 loss: 7.404193859006027\n",
      "epoch: 15 sentence: 10/1558 loss: 7.321820985835572\n",
      "epoch: 15 sentence: 20/1558 loss: 6.828874675515256\n",
      "epoch: 15 sentence: 30/1558 loss: 8.019497616337105\n",
      "epoch: 15 sentence: 40/1558 loss: 6.527116515354737\n",
      "epoch: 15 sentence: 50/1558 loss: 6.1656978178567545\n",
      "epoch: 15 sentence: 60/1558 loss: 7.023588477838656\n",
      "epoch: 15 sentence: 70/1558 loss: 5.501700485397145\n",
      "epoch: 15 sentence: 80/1558 loss: 6.400082621432341\n",
      "epoch: 15 sentence: 90/1558 loss: 6.766488502197039\n",
      "epoch: 15 sentence: 100/1558 loss: 6.30343260273112\n",
      "epoch: 15 sentence: 110/1558 loss: 6.9889828326827\n",
      "epoch: 15 sentence: 120/1558 loss: 6.956187083713361\n",
      "epoch: 15 sentence: 130/1558 loss: 6.457623010477958\n",
      "epoch: 15 sentence: 140/1558 loss: 4.822141223800252\n",
      "epoch: 15 sentence: 150/1558 loss: 7.1671354084556675\n",
      "epoch: 15 sentence: 160/1558 loss: 2.645157944762938\n",
      "epoch: 15 sentence: 170/1558 loss: 7.051875046860715\n",
      "epoch: 15 sentence: 180/1558 loss: 6.521866772069896\n",
      "epoch: 15 sentence: 190/1558 loss: 6.770558262743972\n",
      "epoch: 15 sentence: 200/1558 loss: 5.383106791171342\n",
      "epoch: 15 sentence: 210/1558 loss: 6.062147633932136\n",
      "epoch: 15 sentence: 220/1558 loss: 8.026448934969247\n",
      "epoch: 15 sentence: 230/1558 loss: 5.864428292333729\n",
      "epoch: 15 sentence: 240/1558 loss: 8.007613241377992\n",
      "epoch: 15 sentence: 250/1558 loss: 6.659637266045515\n",
      "epoch: 15 sentence: 260/1558 loss: 6.7107568305659955\n",
      "epoch: 15 sentence: 270/1558 loss: 5.369172758992282\n",
      "epoch: 15 sentence: 280/1558 loss: 6.034927800304053\n",
      "epoch: 15 sentence: 290/1558 loss: 6.309789496279711\n",
      "epoch: 15 sentence: 300/1558 loss: 5.976267087939888\n",
      "epoch: 15 sentence: 310/1558 loss: 7.322827438926149\n",
      "epoch: 15 sentence: 320/1558 loss: 6.78244593502796\n",
      "epoch: 15 sentence: 330/1558 loss: 6.6337017112528285\n",
      "epoch: 15 sentence: 340/1558 loss: 6.867966144212457\n",
      "epoch: 15 sentence: 350/1558 loss: 6.865375930582264\n",
      "epoch: 15 sentence: 360/1558 loss: 7.155539132549454\n",
      "epoch: 15 sentence: 370/1558 loss: 6.820903844115616\n",
      "epoch: 15 sentence: 380/1558 loss: 6.937974327357435\n",
      "epoch: 15 sentence: 390/1558 loss: 6.550764512186222\n",
      "epoch: 15 sentence: 400/1558 loss: 7.338021942717342\n",
      "epoch: 15 sentence: 410/1558 loss: 6.446451447850278\n",
      "epoch: 15 sentence: 420/1558 loss: 7.3652268350880234\n",
      "epoch: 15 sentence: 430/1558 loss: 6.701523181065926\n",
      "epoch: 15 sentence: 440/1558 loss: 6.61973817838827\n",
      "epoch: 15 sentence: 450/1558 loss: 6.910333994472019\n",
      "epoch: 15 sentence: 460/1558 loss: 6.799256659984201\n",
      "epoch: 15 sentence: 470/1558 loss: 6.038902370886031\n",
      "epoch: 15 sentence: 480/1558 loss: 6.947170319311889\n",
      "epoch: 15 sentence: 490/1558 loss: 2.6892626018740753\n",
      "epoch: 15 sentence: 500/1558 loss: 6.9568231898115425\n",
      "epoch: 15 sentence: 510/1558 loss: 6.8303790796767325\n",
      "epoch: 15 sentence: 520/1558 loss: 4.96991957926211\n",
      "epoch: 15 sentence: 530/1558 loss: 6.859247682364393\n",
      "epoch: 15 sentence: 540/1558 loss: 5.458349559026346\n",
      "epoch: 15 sentence: 550/1558 loss: 6.507282250625539\n",
      "epoch: 15 sentence: 560/1558 loss: 6.0597167201353965\n",
      "epoch: 15 sentence: 570/1558 loss: 5.157820162312785\n",
      "epoch: 15 sentence: 580/1558 loss: 7.03503696580143\n",
      "epoch: 15 sentence: 590/1558 loss: 4.896965266386382\n",
      "epoch: 15 sentence: 600/1558 loss: 2.6972655104045193\n",
      "epoch: 15 sentence: 610/1558 loss: 6.259483088404653\n",
      "epoch: 15 sentence: 620/1558 loss: 6.602528990488213\n",
      "epoch: 15 sentence: 630/1558 loss: 6.867959006790425\n",
      "epoch: 15 sentence: 640/1558 loss: 6.602106842097355\n",
      "epoch: 15 sentence: 650/1558 loss: 6.826834597641506\n",
      "epoch: 15 sentence: 660/1558 loss: 5.364203375359042\n",
      "epoch: 15 sentence: 670/1558 loss: 6.181657441075148\n",
      "epoch: 15 sentence: 680/1558 loss: 6.630360688439231\n",
      "epoch: 15 sentence: 690/1558 loss: 6.066950775432425\n",
      "epoch: 15 sentence: 700/1558 loss: 6.747212227193447\n",
      "epoch: 15 sentence: 710/1558 loss: 7.0052788598044655\n",
      "epoch: 15 sentence: 720/1558 loss: 7.02443003958468\n",
      "epoch: 15 sentence: 730/1558 loss: 5.916636474092218\n",
      "epoch: 15 sentence: 740/1558 loss: 7.670837094342248\n",
      "epoch: 15 sentence: 750/1558 loss: 7.596981150728043\n",
      "epoch: 15 sentence: 760/1558 loss: 6.230093013424332\n",
      "epoch: 15 sentence: 770/1558 loss: 6.605237113274607\n",
      "epoch: 15 sentence: 780/1558 loss: 2.659434288858258\n",
      "epoch: 15 sentence: 790/1558 loss: 6.479731057128407\n",
      "epoch: 15 sentence: 800/1558 loss: 6.8845559592106484\n",
      "epoch: 15 sentence: 810/1558 loss: 6.978098357852914\n",
      "epoch: 15 sentence: 820/1558 loss: 7.644062658580815\n",
      "epoch: 15 sentence: 830/1558 loss: 5.499724305392803\n",
      "epoch: 15 sentence: 840/1558 loss: 6.334231084238012\n",
      "epoch: 15 sentence: 850/1558 loss: 6.737904910230876\n",
      "epoch: 15 sentence: 860/1558 loss: 6.327294786297368\n",
      "epoch: 15 sentence: 870/1558 loss: 7.247822485309537\n",
      "epoch: 15 sentence: 880/1558 loss: 6.819299234247074\n",
      "epoch: 15 sentence: 890/1558 loss: 6.52054388583117\n",
      "epoch: 15 sentence: 900/1558 loss: 6.92541167041957\n",
      "epoch: 15 sentence: 910/1558 loss: 6.885373356338001\n",
      "epoch: 15 sentence: 920/1558 loss: 6.931302457888479\n",
      "epoch: 15 sentence: 930/1558 loss: 6.8080620184729534\n",
      "epoch: 15 sentence: 940/1558 loss: 6.6782830773808435\n",
      "epoch: 15 sentence: 950/1558 loss: 6.248070448085565\n",
      "epoch: 15 sentence: 960/1558 loss: 6.8862449126627645\n",
      "epoch: 15 sentence: 970/1558 loss: 2.684115245337843\n",
      "epoch: 15 sentence: 980/1558 loss: 5.267655277769741\n",
      "epoch: 15 sentence: 990/1558 loss: 6.551982316668602\n",
      "epoch: 15 sentence: 1000/1558 loss: 6.320808936124598\n",
      "epoch: 15 sentence: 1010/1558 loss: 6.674053847333151\n",
      "epoch: 15 sentence: 1020/1558 loss: 7.736084720373736\n",
      "epoch: 15 sentence: 1030/1558 loss: 6.634131473537532\n",
      "epoch: 15 sentence: 1040/1558 loss: 6.501568289029521\n",
      "epoch: 15 sentence: 1050/1558 loss: 7.160911048419408\n",
      "epoch: 15 sentence: 1060/1558 loss: 7.0719637431166635\n",
      "epoch: 15 sentence: 1070/1558 loss: 6.442785258201198\n",
      "epoch: 15 sentence: 1080/1558 loss: 2.704976077069541\n",
      "epoch: 15 sentence: 1090/1558 loss: 7.730437469445135\n",
      "epoch: 15 sentence: 1100/1558 loss: 7.062617790742262\n",
      "epoch: 15 sentence: 1110/1558 loss: 6.102242272497418\n",
      "epoch: 15 sentence: 1120/1558 loss: 2.695005149224622\n",
      "epoch: 15 sentence: 1130/1558 loss: 6.721236215617493\n",
      "epoch: 15 sentence: 1140/1558 loss: 7.226614681049715\n",
      "epoch: 15 sentence: 1150/1558 loss: 5.605011174874571\n",
      "epoch: 15 sentence: 1160/1558 loss: 6.5852392849028325\n",
      "epoch: 15 sentence: 1170/1558 loss: 6.894273421696143\n",
      "epoch: 15 sentence: 1180/1558 loss: 7.855369145226361\n",
      "epoch: 15 sentence: 1190/1558 loss: 6.593106449759493\n",
      "epoch: 15 sentence: 1200/1558 loss: 6.35306753265961\n",
      "epoch: 15 sentence: 1210/1558 loss: 7.058577641810611\n",
      "epoch: 15 sentence: 1220/1558 loss: 6.475579899285406\n",
      "epoch: 15 sentence: 1230/1558 loss: 6.492041086673334\n",
      "epoch: 15 sentence: 1240/1558 loss: 5.1774085666232175\n",
      "epoch: 15 sentence: 1250/1558 loss: 6.620513576375851\n",
      "epoch: 15 sentence: 1260/1558 loss: 6.838367813768011\n",
      "epoch: 15 sentence: 1270/1558 loss: 6.649693380063991\n",
      "epoch: 15 sentence: 1280/1558 loss: 2.6957261357018765\n",
      "epoch: 15 sentence: 1290/1558 loss: 6.824885833744464\n",
      "epoch: 15 sentence: 1300/1558 loss: 2.69000127133714\n",
      "epoch: 15 sentence: 1310/1558 loss: 7.969696469731273\n",
      "epoch: 15 sentence: 1320/1558 loss: 2.683500305279643\n",
      "epoch: 15 sentence: 1330/1558 loss: 6.147775174123644\n",
      "epoch: 15 sentence: 1340/1558 loss: 6.698140738369918\n",
      "epoch: 15 sentence: 1350/1558 loss: 5.288873337992169\n",
      "epoch: 15 sentence: 1360/1558 loss: 5.524935227776694\n",
      "epoch: 15 sentence: 1370/1558 loss: 7.3088787814127505\n",
      "epoch: 15 sentence: 1380/1558 loss: 4.460684158483862\n",
      "epoch: 15 sentence: 1390/1558 loss: 7.29952486343654\n",
      "epoch: 15 sentence: 1400/1558 loss: 6.114863494178477\n",
      "epoch: 15 sentence: 1410/1558 loss: 5.29265784279698\n",
      "epoch: 15 sentence: 1420/1558 loss: 7.419738475271519\n",
      "epoch: 15 sentence: 1430/1558 loss: 6.758764762514053\n",
      "epoch: 15 sentence: 1440/1558 loss: 7.284269237107028\n",
      "epoch: 15 sentence: 1450/1558 loss: 6.436474154055956\n",
      "epoch: 15 sentence: 1460/1558 loss: 6.234292405980396\n",
      "epoch: 15 sentence: 1470/1558 loss: 2.6468108121467604\n",
      "epoch: 15 sentence: 1480/1558 loss: 6.6277285856711075\n",
      "epoch: 15 sentence: 1490/1558 loss: 2.6418953608853273\n",
      "epoch: 15 sentence: 1500/1558 loss: 7.714421062411448\n",
      "epoch: 15 sentence: 1510/1558 loss: 6.937702695690202\n",
      "epoch: 15 sentence: 1520/1558 loss: 7.301553422997462\n",
      "epoch: 15 sentence: 1530/1558 loss: 5.56317814832157\n",
      "epoch: 15 sentence: 1540/1558 loss: 7.6682599637044335\n",
      "epoch: 15 sentence: 1550/1558 loss: 6.817645012186245\n",
      "epoch: 16 sentence: 0/1558 loss: 6.7685101959360265\n",
      "epoch: 16 sentence: 10/1558 loss: 6.662625877366117\n",
      "epoch: 16 sentence: 20/1558 loss: 6.760549929217621\n",
      "epoch: 16 sentence: 30/1558 loss: 7.007863131356495\n",
      "epoch: 16 sentence: 40/1558 loss: 6.599100115849482\n",
      "epoch: 16 sentence: 50/1558 loss: 6.832498445990201\n",
      "epoch: 16 sentence: 60/1558 loss: 6.810467769447391\n",
      "epoch: 16 sentence: 70/1558 loss: 6.337122029127691\n",
      "epoch: 16 sentence: 80/1558 loss: 6.820972885145457\n",
      "epoch: 16 sentence: 90/1558 loss: 6.745604060566807\n",
      "epoch: 16 sentence: 100/1558 loss: 6.804490780334589\n",
      "epoch: 16 sentence: 110/1558 loss: 7.035839961442944\n",
      "epoch: 16 sentence: 120/1558 loss: 6.664388904855478\n",
      "epoch: 16 sentence: 130/1558 loss: 7.735686781087194\n",
      "epoch: 16 sentence: 140/1558 loss: 6.472174361131484\n",
      "epoch: 16 sentence: 150/1558 loss: 7.061884514042684\n",
      "epoch: 16 sentence: 160/1558 loss: 2.6962612609752203\n",
      "epoch: 16 sentence: 170/1558 loss: 6.194277219989612\n",
      "epoch: 16 sentence: 180/1558 loss: 6.564931076683959\n",
      "epoch: 16 sentence: 190/1558 loss: 2.705320060980607\n",
      "epoch: 16 sentence: 200/1558 loss: 7.224840401240641\n",
      "epoch: 16 sentence: 210/1558 loss: 6.470117997268602\n",
      "epoch: 16 sentence: 220/1558 loss: 7.187024710795224\n",
      "epoch: 16 sentence: 230/1558 loss: 4.815938077774752\n",
      "epoch: 16 sentence: 240/1558 loss: 6.994682441032086\n",
      "epoch: 16 sentence: 250/1558 loss: 2.704385504593376\n",
      "epoch: 16 sentence: 260/1558 loss: 7.516407622035376\n",
      "epoch: 16 sentence: 270/1558 loss: 5.762723349620681\n",
      "epoch: 16 sentence: 280/1558 loss: 6.608811093260019\n",
      "epoch: 16 sentence: 290/1558 loss: 6.6876973353278535\n",
      "epoch: 16 sentence: 300/1558 loss: 6.635958506806691\n",
      "epoch: 16 sentence: 310/1558 loss: 7.01000077409235\n",
      "epoch: 16 sentence: 320/1558 loss: 6.306575526030762\n",
      "epoch: 16 sentence: 330/1558 loss: 5.715237802548728\n",
      "epoch: 16 sentence: 340/1558 loss: 6.564321535801866\n",
      "epoch: 16 sentence: 350/1558 loss: 4.454439105515479\n",
      "epoch: 16 sentence: 360/1558 loss: 7.126010667828231\n",
      "epoch: 16 sentence: 370/1558 loss: 7.619751598930665\n",
      "epoch: 16 sentence: 380/1558 loss: 7.970987976401102\n",
      "epoch: 16 sentence: 390/1558 loss: 2.6775732934378302\n",
      "epoch: 16 sentence: 400/1558 loss: 6.804355995583678\n",
      "epoch: 16 sentence: 410/1558 loss: 7.100308315613772\n",
      "epoch: 16 sentence: 420/1558 loss: 7.697229496275445\n",
      "epoch: 16 sentence: 430/1558 loss: 6.753685247604753\n",
      "epoch: 16 sentence: 440/1558 loss: 3.6499913159015787\n",
      "epoch: 16 sentence: 450/1558 loss: 6.939140590602795\n",
      "epoch: 16 sentence: 460/1558 loss: 6.46590138973496\n",
      "epoch: 16 sentence: 470/1558 loss: 6.177560568482751\n",
      "epoch: 16 sentence: 480/1558 loss: 6.468343433671743\n",
      "epoch: 16 sentence: 490/1558 loss: 6.06659492375561\n",
      "epoch: 16 sentence: 500/1558 loss: 6.581437093670628\n",
      "epoch: 16 sentence: 510/1558 loss: 6.483230047948414\n",
      "epoch: 16 sentence: 520/1558 loss: 5.54504971847235\n",
      "epoch: 16 sentence: 530/1558 loss: 6.919687057978608\n",
      "epoch: 16 sentence: 540/1558 loss: 5.0592935367351135\n",
      "epoch: 16 sentence: 550/1558 loss: 6.128658087926432\n",
      "epoch: 16 sentence: 560/1558 loss: 7.857008854011909\n",
      "epoch: 16 sentence: 570/1558 loss: 6.230167449339495\n",
      "epoch: 16 sentence: 580/1558 loss: 5.168525164487469\n",
      "epoch: 16 sentence: 590/1558 loss: 7.4605678786596785\n",
      "epoch: 16 sentence: 600/1558 loss: 7.8563626376778535\n",
      "epoch: 16 sentence: 610/1558 loss: 7.432759085695205\n",
      "epoch: 16 sentence: 620/1558 loss: 6.749943585811478\n",
      "epoch: 16 sentence: 630/1558 loss: 6.260938327032236\n",
      "epoch: 16 sentence: 640/1558 loss: 6.428131637531212\n",
      "epoch: 16 sentence: 650/1558 loss: 2.667456469065704\n",
      "epoch: 16 sentence: 660/1558 loss: 6.564325836373987\n",
      "epoch: 16 sentence: 670/1558 loss: 6.90606923995065\n",
      "epoch: 16 sentence: 680/1558 loss: 6.668239055324172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_logistic(full_sentences_idx, \u001b[38;5;28mlen\u001b[39m(word2idx))\n",
      "Cell \u001b[0;32mIn[19], line 48\u001b[0m, in \u001b[0;36mtrain_logistic\u001b[0;34m(sentences, vocab_size, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m W \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(predictions \u001b[38;5;241m-\u001b[39m targets)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the loss at each iteration (we don't use it here, but you may want to plot it later)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(targets \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(predictions)) \u001b[38;5;241m/\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)     \n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_logistic(full_sentences_idx, len(word2idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
