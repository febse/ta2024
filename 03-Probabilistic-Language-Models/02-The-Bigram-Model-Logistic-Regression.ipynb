{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "In the previous notebook we counted the number of times each bi-gram appeared in the text. We can also formulate this problem as a classification problem and use a logistic regression model to predict the second word of a bi-gram given the first word.\n",
    "\n",
    "Let's start by looking at a simple sentence (the same one we used in the previous notebook) and see how we can formulate the classification problem.\n",
    "\n",
    "```\n",
    "Be brave as brave can be.\n",
    "```\n",
    "\n",
    "We convert this sentence into lower case and remove the punctuation to get the following bi-grams:\n",
    "\n",
    "- BEGINNING be\n",
    "- be brave\n",
    "- brave as\n",
    "- as brave\n",
    "- brave can\n",
    "- can be\n",
    "- be END\n",
    "\n",
    "In the counting approach we did not need a vector representation of the words but for the logistic regression model the inputs and the outputs must be numeric. We can use a one-hot encoding to represent the words. In this encoding each word is represented by a vector of zeros with a one at the index of the word in the vocabulary. In the example we have the following 6 tokens in the vocabulary:\n",
    "\n",
    "```\n",
    "vocabulary = ['BEGINNING', 'END', 'be', 'brave', 'as', 'can']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The logistic regression model should predict the probability of the second word in the bi-gram given the first word. To make the example clear, consider a vocabulary of only 3 words: [\"be\", \"brave\", \"as\"] and the bi-gram [\"be\", \"brave\"].\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A1[1] -->|w11| B1[z1] --> SM\n",
    "    A1 -->|w12| B2[z2] --> SM\n",
    "    A1 -->|w13| B3[z3] --> SM\n",
    "    A2[0] -->|w21| B1 \n",
    "    A2 -->|w22| B2\n",
    "    A2 -->|w23| B3\n",
    "    A3[0] -->|w31| B1\n",
    "    A3 -->|w32| B2\n",
    "    A3 -->|w33| B3\n",
    "    SM[Softmax]  --> P1[P1] --> Y1[0]\n",
    "    SM --> P2[P2] --> Y2[1]\n",
    "    SM --> P3[P3] --> Y3[0]\n",
    "```\n",
    "\n",
    "The one-hot encoding of the first word \"be\" is `[1, 0, 0]` and the one-hot encoding of the second word \"brave\" is `[0, 1, 0]`. Given the input vector `x = [1, 0, 0]` the model should output a vector of length 3 with the probabilities of each word in the vocabulary. The output vector `y = [P1, P2, P3]` is the result of the softmax function applied to the dot product of the input vector `x` and the weight matrix `W`:\n",
    "\n",
    "## The Model for a Single Bi-gram\n",
    "\n",
    "Let $x$ be the one-hot-encoded vector of the first word in the bi-gram and $y$ be the one-hot-encoded vector of the second word in the bi-gram. The model for a single bi-gram is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z & = W^T x \\in \\mathbb{R}^3 \\\\\n",
    "\\hat{y} & = \\text{softmax}(z) \\in \\mathbb{R}^3\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $W$ the weight matrix of size $3 \\times 3$.\n",
    "\n",
    "## Backward Pass (Gradient Descent)\n",
    "\n",
    "Now that we have computed the loss for the batch in the forward pass, we can compute the gradients of the loss with respect to the weights. Here it is convenient to use matrix notation and vectorized operations.\n",
    "\n",
    "For a single observation $i$, the predicted probabilities are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z & = W^T x \\\\\n",
    "\\hat{y} & = \\text{softmax}(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The cross-entropy loss is is a scalar value that depends on the weights $W$. The gradient of the loss with respect to the weights is a matrix of the same shape as $W$.\n",
    "\n",
    "$$\n",
    "\\text{CE}(W) = - \\sum_{k=1}^K y_{k} \\log \\hat{y}_{k}\n",
    "$$\n",
    "\n",
    "The chain rule tells us that the gradient of the loss with respect to the weights is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial W} = \\frac{\\partial \\text{CE}(W)}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "The derivative of the cross-entropy loss with respect to $z$ is actually quite simple. As we are differentiation a mapping from $\\mathbb{R}^{K \\times 1}$ to $\\mathbb{R}$, the derivative is a matrix of the same shape as $z$. It is easy to derive this derivative with respect to a single element of $z$: $z_{k}$. The first thing to notice is that the loss depends on $z_{k}$ through $\\hat{y}_{k}$, and $\\hat{y}_{k}$ depends on $z_{1}$, $Z_{2}$, ..., $Z_{K}$ because the softmax function divides each element of the column by the _sum of all elements_ of the column. The chain rule tells us that the derivative of the loss with respect to $z_{k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial z_{k'}} = \\sum_{k=1}^K \\frac{y_{k}}{\\hat{y}_{k}}\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k}}\n",
    "$$\n",
    "\n",
    "Note that we are using $k'$ as the index of the class in the derivative to avoid confusion with the summation index $k$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case is when $k \\neq k'$. In this case, the derivative is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial z_{k'}} \\hat{y}_{k} & = \\frac{\\partial}{\\partial z_{k'}} \\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\\\\n",
    "& = \\frac{0 - e^{z_{k}}e^{z_{k'}}}{\\left(\\sum_{j=1}^K e^{z_{j}}\\right)^2} \\\\\n",
    "& = -\\frac{e^{z_{k}}}{\\sum_{j=1}^K e^{z_{j}}} \\frac{e^{z_{k'}}}{\\sum_{j=1}^K e^{z_{j}}} \\\\\n",
    "& = -\\hat{y}_{k} \\hat{y}_{k'}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both cases into a single expression using the Kronecker delta $\\delta_{kk'}$ which is 1 if $k = k'$ and 0 otherwise:\n",
    "\n",
    "$$\n",
    "\\delta_{kk'} = \\begin{cases}\n",
    "1 & \\text{if } k = k' \\\\\n",
    "0 & \\text{if } k \\neq k'\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial z_{k'}} \\hat{y}_{k} & = \\hat{y}_{k} (\\delta_{kk'} - \\hat{y}_{k'})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You can check that this expression is correct by verifying that it gives the correct results for the two cases we considered above.\n",
    "\n",
    "Now we are ready to substitute this derivative into the expression for the derivative of the loss with respect to $z_{ki}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial \\hat{y}_{k'}} & = - \\sum_{k=1}^K \\frac{y_{k}}{\\hat{y}_{k}}\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k'}} \\\\\n",
    "& = - \\sum_{k=1}^K \\frac{y_{k}}{\\cancel{\\hat{y}_{k}}} \\cancel{\\hat{y}_{k}} (\\delta_{kk'} - \\hat{y}_{k'}) \\\\\n",
    "& = - \\sum_{k = 1}^{K} y_{ki} (\\delta_{kk'} - \\hat{y}_{k'})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The inner sum simplifies beautifully because of the special structure of $\\delta_{kk'}$ and $y_{ki}$. The inner sum is \n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{k} (\\delta_{kk'} - \\hat{y}_{k'}) = \\sum_{k = 1}^{K} y_{k} \\delta_{kk'} - \\sum_{k = 1}^{K} y_{k} \\hat{y}_{k'}\n",
    "$$\n",
    "\n",
    "Now you need to consider only two things. In the first sum we are multiplying $y_{k}$ by $\\delta_{kk'}$. The Kronecker delta is 1 only when $k = k'$, so the sum is only over the terms where $k = k'$.\n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{k} \\delta_{kk'} = y_{k'}\n",
    "$$\n",
    "\n",
    "For the second sum, notice that the $\\hat{y}_{k'}$ does not depend on the summation index $k$. Therefore, we can take it out of the sum.\n",
    "\n",
    "$$\n",
    "\\sum_{k = 1}^{K} y_{ki} \\hat{y}_{k'} = \\hat{y}_{k'} \\sum_{k = 1}^{K} y_{k} = \\hat{y}_{k'}\n",
    "$$\n",
    "\n",
    "The last equality is true because the sum is over the elements of the $i$-th row of $Y$, which is a one-hot encoded vector showing the true class of the $i$-th observation. Therefore, its sum over all elements is 1.\n",
    "\n",
    "In the end, the derivative of the loss with respect to the predicted probabilities is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial z_{k'}} = (-y_{k'} + \\hat{y}_{k'}) = \\hat{y}_{k'} - y_{k'}\n",
    "$$\n",
    "\n",
    "So the $ki$-th element of the gradient of the loss with respect to $z$ is the difference between the predicted probability and the true label. We can write this as a matrix operation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "What remains now is to compute the derivative of $z = W^T x$ with respect to $W$. Here it is helpful to consider the derivative with respect to a single weight $W_{ij}$ and consider a small example.\n",
    "\n",
    "Let $W$ be a $3 \\times 4$ matrix:\n",
    "\n",
    "$$\n",
    "z = W^T x = \\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "w_{13} & w_{23} & w_{33} \\\\\n",
    "w_{14} & w_{24} & w_{34}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "w_{11}x_1 + w_{21}x_2 + w_{31}x_3 \\\\\n",
    "w_{12}x_1 + w_{22}x_2 + w_{32}x_3 \\\\\n",
    "w_{13}x_1 + w_{23}x_2 + w_{33}x_3 \\\\\n",
    "w_{14}x_1 + w_{24}x_2 + w_{34}x_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The derivative of $z_{k}$ with respect to $W_{ij}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{k}}{\\partial W_{ij}} = \\begin{cases}\n",
    "x_{j} & \\text{if } i = k \\\\\n",
    "0 & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So the derivative of the whole vector $z$ with respect to a single weight, say $W_{1j}$ is again a vector of the same shape as $z$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial W_{1j}} = \\begin{bmatrix}\n",
    "x_{j} \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial z}{\\partial W_{2j}} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "x_{j} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial z}{\\partial W_{3j}} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "x_{j} \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The last result implies that when we take the derivative of the loss with respect to the weights, we will get a matrix of the same shape as $W$\n",
    "with the $ij$-th element being the product of the $i$-th row of the derivative of the loss with respect to $z$ and the $j$-th element of the input vector $x$ which is the outer product of the prediction error and the input vector.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}(W)}{\\partial W} = (\\hat{y} - y) x^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_doc(text: str) -> list:\n",
    "    sentences = []\n",
    "    text_doc = nlp(text)\n",
    "    word2idx = {\n",
    "        \"BEGINNING\": 0,\n",
    "        \"END\": 1\n",
    "    }\n",
    "    idx2word = {\n",
    "        0: \"BEGINNING\",\n",
    "        1: \"END\"\n",
    "    }\n",
    "    \n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        tokens = [\"BEGINNING\"]\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "\n",
    "            token_normalized = token.lower_ \n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            if token_normalized not in word2idx:            \n",
    "                idx = len(word2idx)\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        tokens.append(\"END\")\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences, sample_word2idx, sample_idx2word = tokenize_doc(\"Be brave as brave can be.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BEGINNING', 'be', 'brave', 'as', 'brave', 'can', 'be', 'END']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a one-hot encoded vector with 1 at the index of idx\n",
    "def one_hot_encode_word(idx: int, vocab_size: int):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[idx] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector of \"be\"\n",
    "\n",
    "idx_be = sample_word2idx[\"be\"]\n",
    "print(idx_be)\n",
    "\n",
    "one_hot_encode_word(idx_be, len(sample_word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is convenient to have a function that processes the raw text and returns the word indices\n",
    "def text_to_indexed_sentences(sentences: list, word2idx: dict):\n",
    "    sentences_with_idx = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_with_idx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            idx = word2idx[word]    \n",
    "            sentence_with_idx.append(idx)\n",
    "            \n",
    "        sentences_with_idx.append(sentence_with_idx)\n",
    "    \n",
    "    return sentences_with_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, 3, 4, 3, 5, 2, 1]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences_indexed_list = text_to_indexed_sentences(sample_sentences, sample_word2idx)\n",
    "sample_sentences_indexed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_indexed = sample_sentences_indexed_list[0]\n",
    "\n",
    "V = len(sample_word2idx)\n",
    "sample_n = len(sample_sentence_indexed)\n",
    "\n",
    "sample_inputs = np.zeros((sample_n - 1, V))\n",
    "sample_targets = np.zeros((sample_n - 1, V))\n",
    "sample_inputs[np.arange(sample_n - 1), sample_sentence_indexed[:sample_n-1]] = 1\n",
    "sample_targets[np.arange(sample_n - 1), sample_sentence_indexed[1:]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sentence:  8\n",
      "Number of words in the vocabulary:  6\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in the sentence and in the vocabulary\n",
    "\n",
    "print(\"Number of words in the sentence: \", sample_n)\n",
    "print(\"Number of words in the vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input matrix has one row less than the sentence length (this is the number of bi-grams) and has as many columns as the vocabulary size\n",
    "# (this is the length of the one-hot encoded vectors).\n",
    "sample_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The target matrix has the same shape as the input matrix.\n",
    "sample_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the cross-entropy loss\n",
    " \n",
    "$$\n",
    "J(w) = -\\frac{1}{N}\\sum_{i = 1}^{N} \\sum_{j = 1}^{V} y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "the gradient descent update rule for the weights is\n",
    "\n",
    "$$\n",
    "W^{\\text{new}} = W^{\\text{old}} - \\eta \\nabla_{w} J(w) \\\\\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate and the gradient of the loss with respect to the weights is\n",
    "\n",
    "$$\n",
    "\\nabla J = X^T (\\hat{Y} - Y)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will define the softmax function that will take a np.array of shape (N, D) and return a np.array of shape (N, D) where each row is\n",
    "# the softmax of the corresponding row in the input array\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def train_logistic(sentences: list[list[int]], V: int, learning_rate: float = 0.01, epochs: int = 100):    \n",
    "    losses = []\n",
    "\n",
    "    # Initialize the weights\n",
    "    W = np.random.randn(V, V) / np.sqrt(V)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        j = 0 # keep track of iterations\n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            \n",
    "            # An example sentence has the form [\"BEGINNING\", \"hello\", \"my\", \"name\", \"is\", \"john\", \"END\"]\n",
    "            # Only with the word indices instead of the words\n",
    "            # It has n = 7 words and therefore n - 1 = 6 bi-grams\n",
    "            # So each row of the inputs and targets matrices will have the shape (1, vocab_size)\n",
    "            \n",
    "            n = len(sentence)\n",
    "            \n",
    "            inputs = np.zeros((n - 1, V))\n",
    "            targets = np.zeros((n - 1, V))\n",
    "            inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "            \n",
    "            # Compute the loss and update the weights\n",
    "            \n",
    "            # if j % 10 == 0:\n",
    "            #     print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "            # j += 1\n",
    "    \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sentences, full_word2idx, full_idx2word = tokenize_doc(alice)\n",
    "full_sentences_idx = text_to_indexed_sentences(full_sentences, full_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_logistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_sentences_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword2idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m, in \u001b[0;36mtrain_logistic\u001b[0;34m(sentences, vocab_size, learning_rate, epochs)\u001b[0m\n",
      "\u001b[1;32m     27\u001b[0m targets[np\u001b[38;5;241m.\u001b[39marange(n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), sentence[\u001b[38;5;241m1\u001b[39m:]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get output predictions\u001b[39;00m\n",
      "\u001b[0;32m---> 30\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m(inputs\u001b[38;5;241m.\u001b[39mdot(W))\n",
      "\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# do a gradient descent step\u001b[39;00m\n",
      "\u001b[1;32m     33\u001b[0m W \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(predictions \u001b[38;5;241m-\u001b[39m targets)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "train_logistic(full_sentences_idx, len(alice_word2idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
