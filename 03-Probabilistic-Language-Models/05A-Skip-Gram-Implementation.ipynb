{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5017f3e060c3a02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment\n",
    "\n",
    "The goal in this assignment is to implement a skip-gram model using negative sampling by following the skeleton code provided below. Please refer to 05-Neural-Language-Models for the theory behind the skip-gram model and negative sampling. In the code, look for the TODO comments to see where you need to add your own code.\n",
    "\n",
    "Email me with any questions you may have. We can also schedule an online meeting to discuss the assignment.\n",
    "\n",
    "\n",
    "There are a few things you need to do:\n",
    "- Complete the implementation of the `tokenize_doc` function to tokenize the input text and return a list of sentences where each sentence is a list of words (represented by unique integers). You also need to return two dictionaries: `word2idx` and `idx2word` that map each word to a unique integer and vice versa.\n",
    "\n",
    "- Complete the implementation of the `create_word_frequencies` function to create a numpy array of shape (vocabulary size,) containing the frequencies of each word in the vocabulary.\n",
    "- Complete the implementation of the `get_context` function to extract a list of context words for a given position in a sentence.\n",
    "- Complete the implementation of the `nsd` function to calculate the negative sampling distribution given a list of sentences (words encoded as integers) and the vocabulary size.\n",
    "- Complete the implementation of the `train` function to train the skip-gram model using negative sampling. The function should return the trained model parameters and a list of losses at each epoch.\n",
    "\n",
    "You can mail your (group) solution to [amarov@feb.uni-sofia.bg](mailto:amarov@feb.uni-sofia.bg).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbe9a87cf8504e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9e380cdfbd3830fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_doc(text: str) -> (list, dict, dict, np.array):\n",
    "    \"\"\"\n",
    "    :param text: The text input to be tokenized\n",
    "    :return: Returns a tuple of (sentences, word2idx, idx2word)\n",
    "        The sentences list contains lists of words (represented by unique integers) in each sentence. The\n",
    "        pre-processing steps are:\n",
    "            - Removal of punctuation\n",
    "            - Removal of whitespace\n",
    "            - Lowercase\n",
    "        The word2idx dictionary maps each word to a unique integer.\n",
    "        The idx2word dictionary maps each integer to a unique word.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    text_doc = nlp(text)\n",
    "\n",
    "    # TODO\n",
    "    # TODO\n",
    "\n",
    "    for sentence in text_doc.sents:\n",
    "        tokens = []\n",
    "\n",
    "        for token in sentence:\n",
    "            # TODO \n",
    "                continue\n",
    "            token_normalized = # TODO\n",
    "\n",
    "            if token_normalized not in word2idx:\n",
    "                # TODO\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "\n",
    "        sentences.append([word2idx[token] for token in tokens])\n",
    "\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca555a754afae0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_word_frequencies(sentences: list, V: int) -> np.array:\n",
    "    \"\"\"\n",
    "    :param sentences: A list of sentences where each sentence is a list of words (represented by unique integers)\n",
    "    :param V: The size of the vocabulary\n",
    "    :return: A numpy array of shape (V,) containing the frequencies of each word in the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaccec03959c03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "        sentences_num: list,\n",
    "        word_freqs: np.array,\n",
    "        window_size: int = 5,\n",
    "        learning_rate: float = 0.025,\n",
    "        num_negatives: int = 5,\n",
    "        drop_threshold: float = 1e-5,\n",
    "        epochs: int = 10,\n",
    "        D: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a skip-gram model using a list sentences where each word is encoded by a unique integer.\n",
    "    :param sentences_num: A list of sentences\n",
    "    :param word_freqs: A numpy array holding the word frequencies\n",
    "    :param window_size: The size of the window to use for context words\n",
    "    :param learning_rate: The learning rate to use for gradient descent\n",
    "    :param num_negatives: The number of negative samples to draw per input word\n",
    "    :param drop_threshold: The threshold for subsampling frequent words\n",
    "    :param epochs: The number of epochs to train for\n",
    "    :param D: The dimensionality of the word vectors (word embeddings)\n",
    "    :return:\n",
    "        A tuple of trained model parameters and a list of losses at each epoch: W1, W2, losses\n",
    "    \"\"\"\n",
    "    # Determine the vocabulary size\n",
    "    V = ???\n",
    "\n",
    "    # The vocabulary size (number of unique words)\n",
    "\n",
    "    neg_sampling_dist = ???\n",
    "\n",
    "    # Print basic information about the training data\n",
    "    print(f\"Training with vocabulary size {V:5}\")\n",
    "\n",
    "    # Print the parameters\n",
    "    print(\n",
    "        f\"\"\"\n",
    "            window_size: {window_size},\n",
    "            embedding size D={D},\n",
    "            number of negative samples: {num_negatives},\n",
    "            epochs: {epochs}, \n",
    "            learning rate: {learning_rate}\"\"\"\n",
    "    )\n",
    "\n",
    "    # First we initialize the model parameters\n",
    "    # which are the two weight matrices\n",
    "\n",
    "    W1 = # TODO: initialize the input to hidden weights matrix\n",
    "    W2 = # TODO: initialize the hidden to output weights matrix\n",
    "\n",
    "    # The probability distribution for drawing negative samples\n",
    "    prob_negative = ???\n",
    "\n",
    "    # We will store the costs in a list, so we can eventually plot them later\n",
    "    losses = []\n",
    "\n",
    "    # for subsampling each sentence\n",
    "    p_drop = 1 - np.sqrt(drop_threshold / prob_negative)\n",
    "\n",
    "    # Start training the model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # randomly order sentences, so we don't always see\n",
    "        # sentences in the same order\n",
    "        np.random.shuffle(sentences_num)\n",
    "\n",
    "        # Initialize the loss for this epoch\n",
    "        loss = ???\n",
    "\n",
    "        # A variable to keep track of the number of processed sentences\n",
    "        #  in this epoch\n",
    "        counter = ???\n",
    "\n",
    "        for sent in sentences_num:\n",
    "            # Drop words from the sentence with probability p_drop\n",
    "            \n",
    "            # TODO\n",
    "            \n",
    "            if len(sent) < 2:\n",
    "                continue\n",
    "\n",
    "            # Start iterating over the words in the sentence\n",
    "            for pos, word in enumerate(sent):\n",
    "                # get the positive context words/negative samples\n",
    "                context_words = ???\n",
    "                context_words_array = ???\n",
    "\n",
    "                neg_words = # TODO\n",
    "\n",
    "                # Now we have the input (the center word) and the targets (the positive context words and negative samples)\n",
    "                # We can call the SGD function for the positive words (that are actually in the context)\n",
    "                c = ???\n",
    "                loss += c\n",
    "\n",
    "                # And then for the negative samples\n",
    "                c = ???\n",
    "                \n",
    "                loss += c\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Print some information about the training progress\n",
    "            if counter % 100 == 0:\n",
    "                print(\"processed %s / %s\\r\" % (counter, len(sentences_num)))\n",
    "\n",
    "        # Print the number of the epoch and the loss\n",
    "        print(\"epoch complete:\", epoch, \"loss:\", loss)\n",
    "\n",
    "        # save the loss\n",
    "        losses.append(loss)\n",
    "\n",
    "    # return the model\n",
    "    return W1, W2, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7034403b894aa61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nsd(freq: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculate the negative sampling distribution given a list of sentences (words encoded as integers)\n",
    "    and the vocabulary size.\n",
    "    :param freq: A numpy array holding the word frequencies\n",
    "    :return:\n",
    "    A numpy array of the same shape as freq containing the probabilities of drawing each word as a negative sample.\n",
    "    \"\"\"\n",
    "\n",
    "    # Deemphasize frequent words by raising their frequencies to the 3/4 power\n",
    "    # TODO\n",
    "\n",
    "    # Now we sum all the adjusted frequencies and divide each adjusted frequency by the sum\n",
    "    # so that the adjusted frequencies now form a probability distribution.\n",
    "    # TODO\n",
    "\n",
    "    return p\n",
    "\n",
    "def get_context(pos: int, sentence: list, window_size: int) -> list:\n",
    "    \"\"\"\n",
    "    Return the context words for a given position in a sentence.\n",
    "    :param pos: The index of the context word in the sentence\n",
    "    :param sentence: A list of words (encoded as integers)\n",
    "    :param window_size: The size of the window to use for context words\n",
    "    :return:\n",
    "    A list of context words (encoded as integers).\n",
    "    For a sentence of the form and window size 3: 0 12 4 2 [2 8 1 pos 0 55 2] 2 98 2 3\n",
    "    the function will return the list [2, 8, 1, 0, 55, 2]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def sigmoid(x: np.array):\n",
    "    \"\"\"\n",
    "    The sigmoid activation function\n",
    "    :param x: np.array\n",
    "    :return: np.float64\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sgd(\n",
    "        center_word_idx: int,\n",
    "        target_words_indices: list,\n",
    "        is_context_word: int,\n",
    "        lr: float,\n",
    "        W1: np.array,\n",
    "        W2: np.array\n",
    ") -> np.float64:\n",
    "    \"\"\"\n",
    "    Performs a single step of stochastic gradient descent. It updates the weights in the\n",
    "    W1 and W2 matrices.\n",
    "    :param center_word_idx: The index of the center word\n",
    "    :param target_words_indices: The indices of the target words\n",
    "    :param is_context_word: A 0/1 flag indicating whether the target word is a context word (1) or a negative sample (0)\n",
    "    :param lr: The learning rate to use for gradient descent\n",
    "    :param W1: The matrix of hidden weights (input to hidden)\n",
    "    :param W2: The matrix of output weights (hidden to output)\n",
    "    :return: The loss for the batch\n",
    "    \"\"\"\n",
    "\n",
    "    # W[input_] shape: D\n",
    "    # V[:,targets] shape: D x N\n",
    "    # activation shape: N\n",
    "    # print(\"input_:\", input_, \"targets:\", targets)\n",
    "\n",
    "    # Because the input is a one-hot-encoded vector\n",
    "    # with one at the index position of the input word and zero otherwise,\n",
    "    # multiplying the input with the hidden weights is equivalent\n",
    "    # to selecting the hidden weights corresponding to the input word.\n",
    "    # This is why we don't need to perform the full matrix multiplication here,\n",
    "    # and we can just pull the corresponding row from W1 by using np indexing.\n",
    "\n",
    "    # Furthermore, we only need the output weights corresponding to the target words\n",
    "    # as these are the only ones relevant for the calculation of the probability\n",
    "    # of the target words given the input word.\n",
    "\n",
    "    # First we calculate the net value of the output neuron\n",
    "    a = ???\n",
    "\n",
    "    # Now we pass the net value through the sigmoid activation function\n",
    "    # to get the probability of the target words given the input word\n",
    "    p = ???\n",
    "\n",
    "    # The is_context_word is the target used in the calculation of the loss\n",
    "    gW2 = ???\n",
    "    gW1 = ???\n",
    "\n",
    "    W2[:, target_words_indices] -= lr * gW2  # D x N\n",
    "    W1[center_word_idx] -= lr * gW1  # D\n",
    "\n",
    "    # Calculate the loss (binary cross entropy) for each sample\n",
    "    # We add a small constant to the probabilities to avoid taking the log of zero\n",
    "    # for very small probabilities that may occur due to finite precision arithmetic.\n",
    "\n",
    "    loss = ???\n",
    "    \n",
    "    # Finally, return the total loss for the batch\n",
    "    return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5f3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca663d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(text: str) -> list:\n",
    "    # Create an empty list to store the sentences\n",
    "    sentences = []\n",
    "\n",
    "    # Pass the text through spacy's pipeline\n",
    "    text_doc = nlp(text)\n",
    "    \n",
    "    # Create a dictionary to store the word to index mapping\n",
    "    word2idx = {}\n",
    "\n",
    "    # Create a dictionary to store the index to word mapping\n",
    "    idx2word = {}\n",
    "    \n",
    "    # Iterate over the sentences in the text\n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        # For each sentence, create a list to store the tokens\n",
    "        # The first token is the \"BEGINNING\" token (beginning of the sentence)\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sentence:\n",
    "            # Omit spaces and punctuation\n",
    "            if ???:\n",
    "                continue\n",
    "\n",
    "            # Lowercase the token\n",
    "            token_normalized = ??? \n",
    "\n",
    "            # Append the lowercased token to the list of tokens\n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            # If the token is not in the word2idx dictionary, add it\n",
    "            if token_normalized not in word2idx:\n",
    "                # The indices of the tokens must be unique, \n",
    "                # so taking the number of entries in the word2idx dictionary will give us the next index            \n",
    "                idx = len(word2idx)\n",
    "\n",
    "                # Add the token to the word2idx and idx2word dictionaries\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        # Append the list of tokens to the list of sentences\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, word2idx, idx2word = tokenize_doc(alice)\n",
    "word_frequencies = create_word_frequencies(sentences, len(word2idx))\n",
    "\n",
    "W1, W2, costs = train(sentences, word_frequencies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
