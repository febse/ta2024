{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24fded7db0e8f68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The Neural Bi-gram Model\n",
    "\n",
    "The logistic regression model that we have considered this far has several considerable disadvantages.\n",
    "\n",
    "It is slow and expensive to train, despite the fact that it is a linear model. Condider the $V \\times V$ weights matrix. It has $V^2$ parameters. For a vocabulary of 100,000 words, this is 10 billion parameters. This is a lot of parameters to estimate, and it is not surprising that it takes a long time to train.\n",
    "  \n",
    "\n",
    "We can try to work around this by compressing the inputs into a lower dimensional space, which is exactly what neural networks do. We can think of the neural network as a non-linear function that maps the inputs into a lower dimensional space. The neural network is then a linear model in this lower dimensional space.\n",
    "\n",
    "![Hidden Layer Network](03-one-hidden-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16234e84c8e63a5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This model will have two sets of weights: $W^h$ ($V \\times D)$ and $W^o$ ($D \\times V)$. Choosing $D$ to be much smaller than $V$ will reduce the number of parameters that we need to estimate. With $D = 100$ and $V = 10000$ we have 1 million parameters, which is a lot less than 10 billion.\n",
    "\n",
    "Let's define the neural network model:\n",
    "The hidden layer will have $D = 30$ neurons and a $\\tanh$ activation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{H} = \\tanh(\\mathbf{X} \\mathbf{W^h}) \\\\\n",
    "\\hat{Y} = \\text{softmax}(\\mathbf{H}\\mathbf{W^o})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The loss function is the same as before:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}^h, \\mathbf{W}^o) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^V Y_{ij} \\log \\hat{Y}_{ij}\n",
    "$$\n",
    "\n",
    "The weights are updated using gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{h, \\text{new}} = \\mathbf{W}^{h, \\text{old}} - \\eta \\nabla_{\\mathbf{W}^h} J \\\\\n",
    "\\mathbf{W}^{o, \\text{new}} = \\mathbf{W}^{o, \\text{old}} - \\eta \\nabla_{\\mathbf{W}^o} J \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradients are computed using the chain rule and can be shown to be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{W}^o} J = \\mathbf{H}^T (\\hat{Y} - Y) \\\\\n",
    "\\nabla_{\\mathbf{W}^h} J = \\mathbf{X}^T (\\hat{Y} - Y) \\odot (1 - \\mathbf{H}^2) \\mathbf{W}^{oT}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the above, the $\\odot$ operator is the element-wise product of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e29936834430c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:04.743763007Z",
     "start_time": "2023-12-20T16:04:04.514879793Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As before, we start by producing the training data\n",
    "import numpy as np\n",
    "\n",
    "# This is the same softmax function as before\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def train_neural_bigram(sentences: list, V: int, D: int = 100, learning_rate: float = 0.01, epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Train a neural bi-gram model with tanh activation function\n",
    "    \n",
    "    :param sentences: A list sentences. Each sentence is a list of integers corresponding to the indices of the words in the vocabulary\n",
    "    :param V: The size of the vocabulary\n",
    "    :param D: The size of the hidden layer (size of the word vectors)\n",
    "    :param learning_rate: The learning rate of the gradient descent algorithm\n",
    "    :param epochs: Number of epochs to train the model\n",
    "    :return: Hidden and output weights and a list of losses at each iteration\n",
    "    \"\"\"\n",
    "    # initialize weights\n",
    "    Wh = np.random.randn(V, D) / np.sqrt(V)\n",
    "    Wo = np.random.randn(D, V) / np.sqrt(D)\n",
    "    \n",
    "    # A list to store the loss at each iteration\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        j = 0 # keep track of iterations\n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            n = len(sentence)\n",
    "            inputs = np.zeros((n - 1, V))\n",
    "            targets = np.zeros((n - 1, V))\n",
    "            inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "            \n",
    "            # get output predictions\n",
    "            hidden = np.tanh(inputs.dot(Wh))\n",
    "            predictions = softmax(hidden.dot(Wo))\n",
    "            \n",
    "            # do a gradient descent step\n",
    "            W2 = Wo - learning_rate * hidden.T.dot(predictions - targets)\n",
    "            dhidden = (predictions - targets).dot(W2.T) * (1 - hidden * hidden)\n",
    "            Wh = Wh - learning_rate * inputs.T.dot(dhidden)\n",
    "            \n",
    "            # keep track of the loss\n",
    "            loss = -np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if j % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "            j += 1\n",
    "            \n",
    "    return Wh, Wo, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190844f20ef41886",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The Skip-Gram Model\n",
    "\n",
    "\n",
    "In the previous model, we used the first word in a bi-gram to predict the second word. \n",
    "\n",
    "$$\n",
    "P(w_{t+1} | w_t) = \\text{softmax}(\\mathbf{W}_{o}^T f(\\mathbf{W}_{h}^T x_t))\n",
    "$$\n",
    "\n",
    "The model already produces word vectors (embeddings) for each word in the vocabulary. A problem with the model, however, is that these\n",
    "word vectors perform poorly on word similarity and other NLP tasks.\n",
    "\n",
    "The skip-gram model is a modification of the bi-gram model that produces better word vectors. The skip-gram model uses the current word to predict the surrounding words. The model is trained on a corpus of words and their surrounding words. The model is trained to maximize the probability of the surrounding words given the current word.\n",
    "\n",
    "Furthermore, it uses the dot product between the sets of weights (word vectors) to compute the probability of the surrounding words. This is equivalent to a cosine similarity between the word vectors. The skip-gram model is trained to maximize the cosine similarity between the word vectors of the current word and the surrounding words.\n",
    "\n",
    "Mathematically, this implies that the activation function of the hidden layer is the identity function.\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "The bi-gram model would produce the following training data the word \"tired\":\n",
    "\n",
    "- (tired, of)\n",
    "\n",
    "The skip-gram model looks at a set of nearby words (context window $m$) and produces the following training data for the word tired $m = 2$\n",
    "\n",
    "- (tired, get)\n",
    "- (tired, very)\n",
    "- (tired, of)\n",
    "- (tired, sitting)\n",
    "\n",
    "You can think about the model in two ways\n",
    "\n",
    "1. One sample with four targets\n",
    "    - tired -> (very, get, of, sitting)\n",
    "2. Four samples with a single target each\n",
    "    - tired -> very\n",
    "    - tired -> get\n",
    "    - tired -> of\n",
    "    - tired -> sitting\n",
    "\n",
    "$$\n",
    "p(\\text{context words} | \\text{center word}) = \\prod_{j \\in \\text{context}} \\text{softmax}(\\mathbf{W}^o_j \\mathbf{W}^h_i)\n",
    "$$\n",
    "\n",
    "We see that regarding the training data the skip-gram model is an extension of the bi-gram model. Because it strives to maximize the probability of similar words, it drops the tanh activation function that we used in the previous example and replaces it with the identity function.\n",
    "\n",
    "The equations for the skip-gram model are:\n",
    "\n",
    "$$\n",
    "H = X \\mathbf{W}^h \\\\\n",
    "\\hat{Y} = softmax(H \\mathbf{W}^o)\n",
    "$$\n",
    "\n",
    "and the prediction of the model is based on the dot-products (similarities) between the center word vector (input) and the context word vectors (output).\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "The practical application of the skip-gram model is computationally expensive. Consider what will happen if we try to train the model on a large corpus with a vocabulary size e.g., $V = 250000$ words and word vector size $D = 300$. Each of the two sets of weights $W^h$ and $W^o$ will be $250000 \\times 300 = 7.5 \\times 10^7$. In order to train the model, we will need a huge amount of training data to avoid overfitting because of the high number of parameters. This means that the model will take a long time to train.\n",
    "\n",
    "A practical solution to this problem is to use negative sampling. To illustrate the idea, let's consider the following example:\n",
    "\n",
    "The output of each neuron in the output layer is a vector of probabilities for each of the $V$ words in the vocabulary. The perfect prediction of that neuron will be to have 1 at the index of the correct context word and zeros everywhere else.\n",
    "\n",
    "$$\n",
    "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n",
    "$$\n",
    "\n",
    "Instead of computing a probability for all words in the vocabulary, we can select a sample of words from that vocabulary and use them as negative examples. The model is then trained to maximize the probability of the correct context word and minimize the probability of the negative examples.\n",
    "\n",
    "If we choose 5 negative examples, the perfect prediction output of the neuron will be a 6-dimensional vector with 1 at the index of the correct context word and zeros everywhere else.\n",
    "\n",
    "$$\n",
    "(0, 0, 0, 0, 0, 1)\n",
    "$$\n",
    "\n",
    "This will massively reduce the computational cost of the model. A natural question is how to choose the negative examples. The authors of the skip-gram model suggest using the uni-gram distribution raised to the 3/4 power. This is equivalent to sampling from the distribution.\n",
    " \n",
    "The uni-gram distribution is simply the frequency of each word in the corpus. \n",
    "\n",
    "$$\n",
    "p(\"tiger\") = \\frac{\\text{frequency of \"tiger\"}}{\\text{total number of words}}\n",
    "$$\n",
    "\n",
    "The uni-gram distribution raised to the 3/4 power is:\n",
    "\n",
    "$$\n",
    "p^1(\"tiger\") = \\frac{p(\\text{\"tiger\"})^{3/4}}{\n",
    "\\sum}\n",
    "$$\n",
    "\n",
    "\n",
    "The 3/4 power is used to reduce the probability of sampling very frequent words and is a hyperparameter of the model.\n",
    "\n",
    "\n",
    "## Multi-class and Binary Classification\n",
    "\n",
    "The first version of the skip-gram model that we presented solves a multi-class classification problem. The output of the model is a vector of probabilities for each of the $V$ words in the vocabulary.\n",
    "\n",
    "The idea of negative sampling is to transform the multi-class classification problem into a sequence binary classification problems (logistic regressions). Instead of predicting the probability of each word in the vocabulary, we predict the probability of the correct context word and the probability of the negative examples.\n",
    "\n",
    "$$\n",
    "p(y | x) = \\frac{\\exp(W_o^TW_h)}{\\sum_{j=1}^V \\exp(W_{oj}^TW^h)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y = 1 | x) = \\sigma(W_h^TW_o)\n",
    "$$\n",
    "\n",
    "In our example the logistic regressions for the center word \"tired\" will be:\n",
    "\n",
    "- (tired, very): $p(very | tired) = \\sigma(W_{o,\\text{very}}^TW_{h, \\text{tired}})$\n",
    "- (tired, get): $p(get | tired) = \\sigma(W_{o,\\text{get}}^TW_{h,\\text{tired}})$\n",
    "- (tired, very): $p(of | tired) = \\sigma(W_{o,\\text{of}}^TW{h,\\text{tired}})$\n",
    "- (tired, very): $p(sitting | tired) = \\sigma(W_{o,\\text{sitting}}^TW^_{h, \\text{tired}})$\n",
    "\n",
    "By sampling from the modified uni-gram distribution we can select a couple (hyperparameter) of negative examples for each center word. For example, we can sample the words \"cat\", \"dog\", \"mouse\", \"rabbit\", and \"horse\" as negative examples for the center word \"tired\". The logistic regressions for the negative examples will be:\n",
    "\n",
    "- (tired, cat): $p(cat | tired) = \\sigma(W_{o, \\text{cat}}^T W_{h, \\text{tired}})$\n",
    "- (tired, dog): $p(dog | tired) = \\sigma(W_{o, \\text{dog}}W^{o}_{h, \\text{tired}})$\n",
    "- (tired, mouse): $p(mouse | tired) = \\sigma(W_{o, \\text{mouse}}W_{h, \\text{tired}})$\n",
    "- (tired, rabbit): $p(rabbit | tired) = \\sigma(W_{o, \\text{rabbit}}W_{h, \\text{tired}})$\n",
    "- (tired, horse): $p(horse | tired) = \\sigma(W_{o, \\text{horse}}W_{h, \\text{tired}})$\n",
    "\n",
    "\n",
    "The loss function with negative sampling is:\n",
    "\n",
    "$$\n",
    "J = \\log p(very | tired) + \\log p(get | tired) + \\log p(of | tired) + \\log p(sitting | tired) + \\log (1 - p(cat | tired)) + \\log (1 - p(dog | tired)) + \\log (1 - p(mouse | tired)) + \\log (1 - p(rabbit | tired)) + \\log (1 - p(horse | tired))\n",
    "$$\n",
    "\n",
    "Generalizing the loss will give us\n",
    "\n",
    "$$\n",
    "J = \\sum_{j \\in \\text{pos. examples}} \\log \\sigma(W^o_jW^h) + \\sum_{j \\in \\text{neg. examples}} \\log (1 - \\sigma(W^o_jW^h))\n",
    "$$\n",
    "\n",
    "We can use a property of the sigmoid function to simplify the loss function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) + \\sigma(-x) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "J = \\sum_{j \\in \\text{pos. examples}} \\log \\sigma(W^o_jW^h) + \\sum_{j \\in \\text{neg. examples}} \\log (\\sigma(-W^o_jW^h))\n",
    "$$\n",
    "\n",
    "\n",
    "## Subsampling Frequent Words\n",
    "\n",
    "A pattern occurring in most human language texts is that some words are much more frequent than others. There is an empirical finding known as Zipf's law that states that the frequency of a word is inversely proportional to its rank in the frequency table. For example, the most frequent word will occur twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "$$\n",
    "\\text{frequency} \\propto \\frac{1}{\\text{rank}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6a78a3b8e23f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:08.141505642Z",
     "start_time": "2023-12-20T16:04:04.741182309Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "gutenberg.fileids()\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc633315d76f0cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:08.280391534Z",
     "start_time": "2023-12-20T16:04:08.146221431Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1616),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 620),\n",
       " ('she', 544),\n",
       " ('it', 539),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 396),\n",
       " ('was', 366),\n",
       " ('i', 364),\n",
       " ('in', 359),\n",
       " ('you', 356),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " ('at', 209),\n",
       " ('on', 191),\n",
       " ('had', 184),\n",
       " ('with', 179)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "words = word_tokenize(alice)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "FreqDist(words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0c29e92d2debc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because some words are used very often (e.g., the), they provide little information about the context. Furthermore, the training will spend a lot of time learning the word vectors for these words.\n",
    "\n",
    "A solution is to drop some of the frequent words from the training data. The authors of the skip-gram model suggest dropping words with a probability of:\n",
    "\n",
    "$$\n",
    "P_{drop}(w) = 1 - \\sqrt{\\frac{\\text{threshold}}{p^1(w)}}\n",
    "$$\n",
    "\n",
    "with a threshold of $t = 10^{-5}$ and $f(w)$ is the frequency of the word in the corpus.\n",
    "\n",
    "\n",
    "Consider our running example:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "and suppose that we drop the words \"get\" and \"of\" of the words in the sentence. The sentence will become:\n",
    "\n",
    "```\n",
    "Alice was beginning to very **tired** sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "What is the effect of this technique on the context window size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe60a6e601f029",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Negative Sampling\n",
    "\n",
    "The basic idea is to restrict the number of cases where a word is considered wrong.\n",
    "\n",
    "$$\n",
    "p(\\text{very} | \\text{tired}) = \\frac{\\exp(W^o_{\\text{very}}W^h_{\\text{tired}})}{\\sum_{j=1}^V \\exp(W^o_jW^h_{\\text{tired}})}\n",
    "$$\n",
    "\n",
    "Let's say that we have selected a number of words that did not occur in the context of the word \"tired\" as negative examples: \"boat\", \"egg\", \"horse\".\n",
    "\n",
    "Then we can formulate a series of binary classification problems to maximize the probability that the words \"very\", \"sitting\", \"by\", \"her\" are in the context of the word \"tired\" and minimize the probability that the words \"boat\", \"egg\", \"horse\" are in the context of the word \"tired\".\n",
    "\n",
    "For the positive samples (words that occurred in the context of the word \"tired\") we have:\n",
    "$$\n",
    "p(\\text{very} | \\text{tired}) = \\sigma(W^o_{\\text{very}}W^h_{\\text{tired}}) \\\\\n",
    "p(\\text{sitting} | \\text{tired}) = \\sigma(W^o_{\\text{sitting}}W^h_{\\text{tired}}) \\\\\n",
    "p(\\text{by} | \\text{tired}) = \\sigma(W^o_{\\text{by}}W^h_{\\text{tired}}) \\\\\n",
    "p(\\text{her} | \\text{tired}) = \\sigma(W^o_{\\text{her}}W^h_{\\text{tired}}) \\\\\n",
    "$$\n",
    "\n",
    "For the negative samples (words that did not occur in the context of the word \"tired\") we have:\n",
    "\n",
    "$$\n",
    "p(\\text{boat} | \\text{tired}) = \\sigma(W^o_{\\text{boat}}W^h_{\\text{tired}}) \\\\\n",
    "p(\\text{egg} | \\text{tired}) = \\sigma(W^o_{\\text{egg}}W^h_{\\text{tired}}) \\\\\n",
    "p(\\text{horse} | \\text{tired}) = \\sigma(W^o_{\\text{horse}}W^h_{\\text{tired}}) \\\\\n",
    "$$\n",
    "\n",
    "The loss function for this example will be:\n",
    "\n",
    "$$\n",
    "J = \\log p(\\text{very} | \\text{tired}) + \\log p(\\text{sitting} | \\text{tired}) + \\log p(\\text{by} | \\text{tired}) + \\log p(\\text{her} | \\text{tired}) + \\log (1 - p(\\text{boat} | \\text{tired})) + \\log (1 - p(\\text{egg} | \\text{tired})) + \\log (1 - p(\\text{horse} | \\text{tired}))\n",
    "$$\n",
    "\n",
    "Generalizing the loss function will give us:\n",
    "\n",
    "(XXX, TODO: check notation)\n",
    "$$\n",
    "J = \\sum_{c in \\text{context}} \\log \\sigma(W^o_cW^h_{\\text{tired}}) + \\sum_{j \\in \\text{neg. examples}} \\log (1 - \\sigma(W^o_jW^h_{\\text{tired}}))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30717dcb73a319a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Weights Update with Negative Sampling\n",
    "\n",
    "### Updating the Output Weights\n",
    "\n",
    "The loss function is given by:\n",
    "\n",
    "$$\n",
    "J = -\\sum_{n = 1}^{N} t_n \\log p_n + (1 - t_n) \\log (1 - p_n)\n",
    "$$\n",
    "\n",
    "From our notes on logistic regression we know (XXX, TODO: check if this is included) that the gradient of the loss function with respect to the weights is:\n",
    "\n",
    "(XXX, TODO, check notation consistency)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{o}} = H^T (P - T)\n",
    "$$ \n",
    "\n",
    "### Updating the Hidden Weights\n",
    "\n",
    "The gradient of the loss function with respect to the hidden weights is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{h}} = X^T ((P - T)W^o)^T\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
