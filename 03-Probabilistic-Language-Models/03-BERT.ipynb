{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a41f439558a450bb044e683f4b5750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d895861393e0406c98085b8d55e1c1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab9e1c702ef499e966025d0a9758d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9192cb1121a428ba8d53af4b16396aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "    \"Hello, my dog is cute\",\n",
    "    max_length=20,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    # return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodings\n",
    "\n",
    "Right before the transformer layer we add positional encodings. Reason: transformers process in parallel and not sequentially, so they don't know about the order of the words in the sentence. Positional encodings are added to the embeddings to give the model information about the position of the words in the sentence.\n",
    "\n",
    "The positional encodings are added to the embeddings.\n",
    "\n",
    "The order of the words is accounted for in RNN architectures because the words are processed sequentially. Transformers look at the entire sentence at once, so they need positional encodings to know the order of the words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input text: \"This is a input text.\"\n",
    "\n",
    "- Tokenization -> [\"CLS\", \"This\", \"is\", \"a\", \"input\", \"text\"]\n",
    "\n",
    "\"I am a robot\"\n",
    "\n",
    "- Tokenization\n",
    "- [\"CLS\", \"I\", \"am\", \"a\", \"robot\"]\n",
    "\n",
    "- Embeddings\n",
    "\n",
    "- $E_I = [1.0, 0.5, 0.3, 0.0]$\n",
    "- $E_am = [0.9, 0.8, 0.2, 1.0]$\n",
    "- $E_a = [0.7, 0.1, 0.4, 0.0]$\n",
    "- $E_robot = [1.1, 0.4, 0.3, 1.0]$\n",
    "\n",
    "- Item positional encodings\n",
    "\n",
    "- $[P_0], [P_1], [P_2], [P_3], [P_4]$\n",
    "\n",
    "- Item combined embeddings:\n",
    "\n",
    "- $E_I + P_0$, $E_am + P_1$, $E_a + P_2$, $E_robot + P_3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed calculations for positions 0 (\"I\") and 1 (\"am\"):\n",
    "\n",
    "For even indices (0, 2, 4, 6, 8, ...), the positional encoding is calculated as follows:\n",
    "\n",
    "$$\n",
    "PE_{pos, 2i} = \\sin(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "\n",
    "Positional encodings need to be added to every dimension of the embeddings.\n",
    "\n",
    "For odd indices (1, 3, 5, 7, 9, ...), the positional encoding is calculated as follows:\n",
    "\n",
    "$$\n",
    "PE_{pos, 2i+1} = \\cos(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "\n",
    "$E_I$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why sin and cos?\n",
    "\n",
    "- Smooth and continuous\n",
    "- Repeating patterns: helps model understand different lengths? Even in a long sentence sin and cos will repeat the same pattern.\n",
    "\n",
    "Word \"am\" is at position 1 with positional encoding $PE_1 = [0.84, 0.54, 0.1, 1.0]$. No other word in any sentence will have the same positional encoding (because of the sin and cos functions ???).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
