{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e65b70ca58cf6a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# A Probabilistic Language Model\n",
    "\n",
    "[Open in Colab](https://colab.research.google.com/github/febse/ta2024/blob/main/03-Probabilistic-Language-Models/01-The-Bigram-Model.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc208377",
   "metadata": {},
   "source": [
    "## The Bi-gram Model\n",
    "\n",
    "A very simple probabilistic model for a natural language is the Markov bi-gram model.\n",
    "\n",
    ":::{.callout-note}\n",
    "## Bi-grams\n",
    "\n",
    "Bi-grams are sequences of two consecutive words in a sentence. For example, consider the sentence\n",
    "\n",
    "```\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again. \n",
    "```\n",
    "\n",
    "The bi-grams here are\n",
    "\n",
    "1. \"In another\"\n",
    "2. \"another moment\"\n",
    "3. \"moment down\"\n",
    "4. ...\n",
    ":::\n",
    "\n",
    "Applications of the bi-gram model include\n",
    "- Speech recognition\n",
    "- Optical character recognition\n",
    "- Spelling and grammar correction, nonsense detection (low probability sequences)\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cff452388432fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "In most languages, the units of meaning are sentences (a sequence of words and punctuation). An interesting question would be to ask: what is the probability to see a specific sentence in a corpus of text?\n",
    "\n",
    "Take the following quote from Oscar Wilde as an example:\n",
    "\n",
    "```\n",
    "Be yourself, everyone else is already taken.\n",
    "```\n",
    "\n",
    "Ignoring the punctuation, there are 7 words in this sentence: $w_1$ to $w_7$.\n",
    "\n",
    "$w_1$ = \"Be\"\n",
    "$w_2$ = \"yourself\"\n",
    "$w_3$ = \"everyone\"\n",
    "...\n",
    "$w_7$ = \"taken\"\n",
    "\n",
    "We could write the probability of this sentence as\n",
    "\n",
    "$$\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1)\n",
    "$$\n",
    "\n",
    "and (in principle) we could estimate this probability by counting the number of times this sentence occurs in a large corpus of text. However, this may not work well because the number of times a whole sentence (or any other long sequence of tokens) occurs in a corpus is likely to be very small or zero.\n",
    "\n",
    "Let's look into a way to model this probability in a more tractable way.\n",
    "\n",
    "Using the chain rule of probability, we can write the probability of a sentence as the product of the probabilities of each word given the preceding words. With the 7 words in the sentence, we can write\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1) = & P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6, w_5, w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5, w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3, w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2, w_1) \\\\\n",
    "& = P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) \\cdot P(w_6 | w_5, w_4, w_3, w_2, w_1) \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, all these probabilities are not much more tractable than the probability of the whole sentence, so we need to make an assumption to simplify this model. The most strict assumption that we can make is that the conditional probability of a word only depends on the preceding word. This is called the **Markov assumption**. With this assumption, the probability of the 7-th word given the preceding words can be written as\n",
    "\n",
    "$$\n",
    "P(w_7 | w_6, w_5, w_4, w_3, w_2, w_1) = P(w_7 | w_6)\n",
    "$$\n",
    "\n",
    "Analogously, we can write the probability of the sentence as\n",
    "\n",
    "$$\n",
    "P(w_7, w_6, w_5, w_4, w_3, w_2, w_1) = P(w_7 | w_6) \\cdot P(w_6 | w_5) \\cdot P(w_5 | w_4) \\cdot P(w_4 | w_3) \\cdot P(w_3 | w_2) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "$$\n",
    "\n",
    "The probabilities of the bi-grams (the probability so see the second word given the first word) are much easier to estimate than the probability of a whole sentence. We can estimate these probabilities by counting the number of times a word $w_t$ follows a word $w_{t - 1}$ in a corpus of text.\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_t | w_{t - 1}) = \\frac{\\text{Count}(w_{t - 1}, w_t)}{\\text{Count}(w_{t - 1})}\n",
    "$$\n",
    "\n",
    "More generally, the probability of a sentence with $T$ words can be written as\n",
    "\n",
    "$$\n",
    "P(w_T, w_{T - 1}, \\ldots, w_1) = P(w_1) \\prod_{t = 2}^{T} P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "As these probabilities would tend to be small, multiplying them in long sequences may lead to underflow problems because computer precision is limited. A common solution to this problem is to use the log-probabilities instead of the probabilities.\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P(w_T, w_{T - 1}, \\ldots, w_1) = \\log P(w_1) +  \\sum_{t = 2}^{T} \\log P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "\n",
    "Another problem is that the log probabilities of sentences will be biased toward shorter sentences, simply because there are less terms in a short sentence. To solve this problem, we can normalize the log probabilities by dividing by the number of words in the sentence.\n",
    "\n",
    "Instead of the maximum likelihood estimates of the bi-gram probabilities, we can use a smoothed version of these estimates to allow a small probability for bi-grams that do not appear in the corpus. A common approach is to add one to the count of each bi-gram before normalizing.\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_t | w_{t - 1}) = \\frac{\\text{Count}(w_{t - 1}, w_t) + 1}{\\text{Count}(w_{t - 1}) + V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2f45e",
   "metadata": {},
   "source": [
    "## Implementation of the Bi-gram Model\n",
    "\n",
    "Let's implement the bi-gram model in Python.\n",
    "\n",
    ":::{.callout-important}\n",
    "## SpaCy Installation\n",
    "\n",
    "The following relies on the `spaCy` library and its English language model. You can install `spaCy` and the English language model by running the following commands in your python environment:\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "or you can uncomment the commands in the next cell and run it.\n",
    ":::\n",
    "\n",
    "The bi-gram model relies on a matrix of bi-gram probabilities to estimate the probability of a sentence. We will create this matrix by counting the number of times each bi-gram appears in \"Alice's Adventures in Wonderland\" by Lewis Carroll.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. We will tokenize the text using `spaCy`. For convenience, we will write a small function that accepts a string and returns a list of sentences, each sentence being a list of tokens.\n",
    "2. We will introduce two special tokens \"BEGINNING\" and \"END\" to mark the beginning and the end of each sentence.\n",
    "3. We will count the number of times each bi-gram appears in the text using the smoothed probability estimates.\n",
    "4. We will normalize the counts to get the probabilities.\n",
    "5. We will write a function that accepts a sentence and returns the log probability of the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a245d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code uses the `!` operator to run shell commands in the notebook and % to run magic commands in the notebook\n",
    "# This step is not necessary if you are running the notebook in Google Colab\n",
    "\n",
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:58:38.929473979Z",
     "start_time": "2023-12-18T15:58:35.901907219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import nltk in order to download the gutenberg corpus which contains a large collection of text data, including our Alice in Wonderland example\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Load spaCy and the English model (in case of errors check if you have downloaded the model)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917d7e9f8c053444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:03.150423303Z",
     "start_time": "2023-12-18T15:30:03.142622821Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\\nOh dear! I shall be late!' (when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text of Alice in Wonderland\n",
    "\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "\n",
    "# Check the first few characters of the text\n",
    "alice[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dbeef",
   "metadata": {},
   "source": [
    "Next, we will pass the whole string that contains the book to the `nlp` object. It will return an object of class `Doc` that we can use extract a list of sentences and tokens. We will use the `sent()` generator to iterate over the sentences in the book and save them in a list. See the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5289a38f6498fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:06.858917781Z",
     "start_time": "2023-12-18T15:30:03.149423919Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we will pass the whole text through spacy's pipeline\n",
    "\n",
    "doc = nlp(alice)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e4541-8af1-46f6-bbe6-ed89d44c429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token      POS             is_stop    is_punct   is_space  \n",
      "[          X                   0          1          0\n",
      "alice      PROPN               0          0          0\n",
      "'s         PART                1          0          0\n",
      "adventures PROPN               0          0          0\n",
      "in         ADP                 1          0          0\n",
      "wonderland PROPN               0          0          0\n",
      "by         ADP                 1          0          0\n",
      "lewis      PROPN               0          0          0\n",
      "carroll    PROPN               0          0          0\n",
      "1865       NUM                 0          0          0\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first 10 tokens (lowercased) and a couple of their properties \n",
    "# POS = Part of Speech\n",
    "# is_stop = is the token a stop word?\n",
    "# is_punct = is the token punctuation?\n",
    "# is_space = is the token a space?\n",
    "\n",
    "print(f\"{'Token':10} {'POS':15} {'is_stop':10} {'is_punct':10} {'is_space':10}\")\n",
    "\n",
    "for token in doc[0:10]:\n",
    "    print(f\"{token.lower_:10} {token.pos_:10} {token.is_stop:10} {token.is_punct:10} {token.is_space:10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee929852-223b-42c4-a3d7-51182aa7b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure\n",
      "of making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "\n",
      "\n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so\n",
      "VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n",
      "Oh dear!\n",
      "I shall be late!'\n"
     ]
    }
   ],
   "source": [
    "# You can iterate over the first few sentences in the document\n",
    "\n",
    "sent_n = 0\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    sent_n += 1\n",
    "    if sent_n == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2774138c46bc034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:07.850090121Z",
     "start_time": "2023-12-18T15:30:07.806243633Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we define a function that tokenizes the text into sentences and words\n",
    "# It will remove the punctuation and spaces and return a list of sentences where each sentence is a list of words\n",
    "\n",
    "def tokenize_doc(text: str) -> list:\n",
    "    # Create an empty list to store the sentences\n",
    "    sentences = []\n",
    "\n",
    "    # Pass the text through spacy's pipeline\n",
    "    text_doc = nlp(text)\n",
    "    \n",
    "    # Create a dictionary to store the word to index mapping\n",
    "    word2idx = {\n",
    "        \"BEGINNING\": 0,\n",
    "        \"END\": 1\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store the index to word mapping\n",
    "    idx2word = {\n",
    "        0: \"BEGINNING\",\n",
    "        1: \"END\"\n",
    "    }\n",
    "    \n",
    "    # Iterate over the sentences in the text\n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        # For each sentence, create a list to store the tokens\n",
    "        # The first token is the \"BEGINNING\" token (beginning of the sentence)\n",
    "\n",
    "        tokens = [\"BEGINNING\"]\n",
    "        \n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sentence:\n",
    "            # Omit spaces and punctuation\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "\n",
    "            # Lowercase the token\n",
    "            token_normalized = token.lower_ \n",
    "\n",
    "            # Append the lowercased token to the list of tokens\n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            # If the token is not in the word2idx dictionary, add it\n",
    "            if token_normalized not in word2idx:\n",
    "                # The indices of the tokens must be unique, \n",
    "                # so taking the number of entries in the word2idx dictionary will give us the next index            \n",
    "                idx = len(word2idx)\n",
    "\n",
    "                # Add the token to the word2idx and idx2word dictionaries\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        # Here we have already finished iterating over the tokens in the sentence\n",
    "        # so we append the \"END\" (end of sentence) token to the list of tokens\n",
    "        tokens.append(\"END\")\n",
    "        # Append the list of tokens to the list of sentences\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eec1941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to a sample string. \n",
    "\n",
    "sample_sents, sample_word2idx, sample_idx2word = tokenize_doc(\"This is a sample sentence. This is another sample sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08c18311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BEGINNING', 'this', 'is', 'a', 'sample', 'sentence', 'END'],\n",
       " ['BEGINNING', 'this', 'is', 'another', 'sample', 'sentence', 'END']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d823483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BEGINNING': 0,\n",
       " 'END': 1,\n",
       " 'this': 2,\n",
       " 'is': 3,\n",
       " 'a': 4,\n",
       " 'sample': 5,\n",
       " 'sentence': 6,\n",
       " 'another': 7}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d68e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'BEGINNING',\n",
       " 1: 'END',\n",
       " 2: 'this',\n",
       " 3: 'is',\n",
       " 4: 'a',\n",
       " 5: 'sample',\n",
       " 6: 'sentence',\n",
       " 7: 'another'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4b429",
   "metadata": {},
   "source": [
    "Now we are ready to start counting the bi-grams. We will use a matrix to store the counts. The rows of the matrix will correspond to the first word of the bi-gram, and the columns will correspond to the second word. The matrix must be square because the number of rows and columns must be equal to the number of unique words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd1fc7bfbbe3609a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:15.998848401Z",
     "start_time": "2023-12-18T15:30:15.996349040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we can create a V x V matrix where V is the size of the vocabulary (number of unique tokens)\n",
    "\n",
    "def compute_bigrams_prob_mtx(sentences, word2idx: dict):\n",
    "    \n",
    "    # Get the vocabulary size (this is the number of unique tokens in the text plus the \"BEGINNING\" and \"END\" tokens)\n",
    "    V = len(word2idx)\n",
    "    \n",
    "    # Let's first create a matrix of counts\n",
    "    # We will initialize it with ones to avoid zero probabilities (this is the smoothing we mentioned earlier)\n",
    "    bigram_counts = np.ones((V, V))\n",
    "\n",
    "    # Now let us loop over all sentences, extract the bi-grams and count their occurrences\n",
    "    # Each time we encounter the sequence \"is strong\" for example, we will increment the count of the\n",
    "    # Row index of the first word (is) and the column index of the second word (strong)\n",
    "\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            # Skip the first word to avoid indexing errors as we have\n",
    "            # no word before the start of sentence token \"BEGINNING\"\n",
    "            if i == 0:            \n",
    "                continue\n",
    "            \n",
    "            # Here i is greater than 0, so we can get the previous word by \n",
    "            # subtracting 1 from the index\n",
    "            first_word = sent[i - 1]\n",
    "            \n",
    "            # We will use the word2idx dictionary to get the index of the word\n",
    "            first_word_idx = word2idx[first_word]\n",
    "            second_word_idx = word2idx[word]\n",
    "\n",
    "            # We will use the index to increment the count of the word\n",
    "            bigram_counts[first_word_idx, second_word_idx] += 1\n",
    "    \n",
    "    # Now we can divide each row by the sum of the row to get the probabilities\n",
    "    \n",
    "    BGP = bigram_counts / bigram_counts.sum(axis=1, keepdims=True)\n",
    "    return BGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fa0ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1       , 0.1       , 0.3       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       ],\n",
       "       [0.125     , 0.125     , 0.125     , 0.125     , 0.125     ,\n",
       "        0.125     , 0.125     , 0.125     ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.3       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.2       ,\n",
       "        0.1       , 0.1       , 0.2       ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.22222222, 0.11111111, 0.11111111],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.3       , 0.1       ],\n",
       "       [0.1       , 0.3       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.22222222, 0.11111111, 0.11111111]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the sample sentences\n",
    "\n",
    "sample_bigram_prob_mtx = compute_bigrams_prob_mtx(sample_sents, sample_word2idx)\n",
    "sample_bigram_prob_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713e4ac",
   "metadata": {},
   "source": [
    "## Sentence Scores\n",
    "\n",
    "Once we have obtained the bi-gram probabilities, we can use them to estimate the probability of a sentence. We will write a function that accepts a sentence and returns the log probability of the sentence. The function will iterate over the words in the sentence and sum the log probabilities of the bi-grams. We will also normalize the log probability by dividing by the number of words in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dce8337c18f5e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:18.970391193Z",
     "start_time": "2023-12-18T15:30:18.967392052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def score_sentence_str(sentence_str: str, word2idx: dict, bigram_probs: np.ndarray):\n",
    "    # First we tokenize the sentence\n",
    "    sents, _, _ = tokenize_doc(sentence_str)\n",
    "    \n",
    "    # Set the sentence score to zero initially\n",
    "    sentence_score = 0\n",
    "    \n",
    "    # As our tokenize_doc function returns a list of sentences, we will only have one sentence\n",
    "    # which is the first element of the list\n",
    "\n",
    "    words = sents[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            first_word_idx = word2idx[words[i - 1]]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {words[i - 1]} not in vocabulary\")\n",
    "        \n",
    "        try:\n",
    "            second_word_idx = word2idx[word]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {word} not in vocabulary\")\n",
    "        \n",
    "        # Get the log probability of the bigram and add it to the sentence score\n",
    "        sentence_score += np.log(bigram_probs[first_word_idx, second_word_idx])\n",
    "        \n",
    "    # Normalize the score by the number of words in the sentence\n",
    "    return sentence_score / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51d3f46d341e62ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:26.193805458Z",
     "start_time": "2023-12-18T15:30:23.055131383Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let us run the whole thing\n",
    "\n",
    "alice_sentences, alice_word2idx, alice_idx2word = tokenize_doc(alice)\n",
    "BGP = compute_bigrams_prob_mtx(alice_sentences, alice_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79ed2001369513c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:28.350304554Z",
     "start_time": "2023-12-18T15:30:28.344750384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2680, 2680)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4485438d3cbc7465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:32.196484124Z",
     "start_time": "2023-12-18T15:30:32.151874863Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGINNING',\n",
       " 'no',\n",
       " 'it',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'do',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " 'shall',\n",
       " 'see',\n",
       " 'it',\n",
       " 'written',\n",
       " 'up',\n",
       " 'somewhere',\n",
       " 'END']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence_list = alice_sentences[32]\n",
    "# second_sentence\n",
    "second_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2918b438-009a-4616-8c27-5ca15535cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.7696548515779575"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence_str = \" \".join(second_sentence_list)\n",
    "score_sentence_str(second_sentence_str, alice_word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928e1924e808af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:34.845609857Z",
     "start_time": "2023-12-18T15:30:34.817295475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.0020184778362875"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it out with a valid sentence that is not in the text\n",
    "\n",
    "score_sentence_str(\"Be nice, everybody else is already taken.\", alice_word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b9b0a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.734067793628513"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it out with some nonsense sentence (note that the words must be in the vocabulary)\n",
    "\n",
    "score_sentence_str(\"Rude foot fun ran egg.\", alice_word2idx, BGP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
