{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e65b70ca58cf6a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# A Probabilistic Language Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc208377",
   "metadata": {},
   "source": [
    "A very simple model for a natural language is the Markov bi-gram model. Given a sequence of words,\n",
    "the Markov model.\n",
    "\n",
    "Bi-grams are sequences of two consecutive words.\n",
    "\n",
    "```\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again. \n",
    "```\n",
    "\n",
    "The bi-grams of the sentence are\n",
    "\n",
    "1. \"In another\"\n",
    "2. \"another moment\"\n",
    "3. \"moment down\"\n",
    "4. ...\n",
    "\n",
    "The model bi-gram model is a probabilistic language model based on the probabilities of bi-grams. The probability of a bi-gram is the conditional probability of the second word given the first word.\n",
    "\n",
    "$$\n",
    "P(\\text{another} | \\text{in}), \\\\\n",
    "P(\\text{moment} | \\text{another}) \\\\\n",
    "P(\\text{down} | \\text{moment})\n",
    "$$\n",
    "\n",
    "Applications of the bi-gram model include\n",
    "- Speech recognition\n",
    "- Optical character recognition\n",
    "- Spelling and grammar correction, nonsense detection (low probability sequences)\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cff452388432fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Bi-gram model\n",
    "\n",
    "In most languages, the units of meaning are sentences. A sentence is a sequence of words. A language model is a model that assigns a probability to a sentence. The probability of a sentence is the product of the probabilities of the words in the sentence.\n",
    "\n",
    "Take the following quote from Oscar Wilde as an example:\n",
    "\n",
    "```\n",
    "Be yourself, everyone else is already taken.\n",
    "```\n",
    "\n",
    "There are 7 words in this sentence: $w_1$ to $w_7$.\n",
    "\n",
    "$w_1$ = \"Be\"\n",
    "$w_2$ = \"yourself\"\n",
    "$w_3$ = \"everyone\"\n",
    "...\n",
    "$w_7$ = \"taken\"\n",
    "\n",
    "We could write the probability of this sentence as\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, w_3, w_4, w_5, w_6, w_7)\n",
    "$$\n",
    "\n",
    "and (in theory) we could estimate this probability by counting the number of times this sentence occurs in a large corpus of text. However, this may not work well because the number of times a whole sentence (or any other long sequence of tokens) occurs in a corpus is likely to be very small or zero.\n",
    "\n",
    "Let's look into a way to represent this probability in a more tractable way.\n",
    "\n",
    "\n",
    "$$\n",
    "P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "be the probability of a word $w_t$ given the preceding word $w_{t - 1}$. This is a much easier probability to estimate because we can count the number of times a word $w_t$ follows a word $w_{t - 1}$ in a corpus.\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_t | w_{t - 1}) = \\frac{\\text{Count}(w_{t - 1}, w_t)}{\\text{Count}(w_{t - 1})}\n",
    "$$\n",
    "\n",
    "For example, the estimated probability of \"Be yourself\" will be the number of times \"be -> yourself\" occurs in the corpus divided by the number of times \"be\" occurs in the corpus.\n",
    "\n",
    "How can we use this to estimate the probability of a sentence? The Bayes theorem comes to the rescue. The Bayes theorem states that the conditional probability of $A$ given $B$ is equal to the joint probability of $A$ and $B$ divided by the probability of $B$.\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{P(A, B)}{P(B)} \\implies P(A, B) = P(A | B) P(B)\n",
    "$$\n",
    "\n",
    "Applying the Bayes theorem to our sentence probability we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1 \\to w_2 \\to w_3) & = P(w_3 | w_2, w_1)P(w_2, w_1) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can apply the Bayes theorem again to $P(w_2, w_1)$ to get\n",
    "\n",
    "$$\n",
    "P(w_1 \\to w_2 \\to w_3) = P(w_3 | w_2 \\to w_1)P(w_2 | w_1)P(w_1)\n",
    "$$\n",
    "\n",
    "The last term is simply the probability of $w_1$ occurring in the corpus. We can estimate this probability by counting the number of times $w_1$ occurs in the corpus and dividing by the total number of words in the corpus.\n",
    "\n",
    "The second term is the probability of the bi-gram $w_2$ given $w_1$. We can estimate this probability by counting the number of times $w_2$ follows $w_1$ in the corpus and dividing by the number of times $w_1$ occurs in the corpus.\n",
    "\n",
    "The first term is a tri-gram probability. We can estimate this probability by counting the number of times $w_3$ follows $w_2$ and $w_1$ in the corpus and dividing by the number of times $w_2$ and $w_1$ occur together in the corpus.\n",
    "\n",
    "However, if we consider longer sequences of words, the number of times a sequence occurs in the corpus will be very small or zero. For example, the probability of the sequence \"Be yourself, everyone else is already taken\" will be zero because this sequence is not likely to occur in the corpus. This is called the **sparsity problem**.\n",
    "\n",
    "An *assumption* that we can make to solve this problem is that the probability of a word only depends on the preceding word. This is called the **Markov assumption**. As all assumptions, this is not true in general, but it is a good approximation for many applications.\n",
    "\n",
    "\n",
    "$$\n",
    "P(w_{t}| w_{t - 1}, w_{t - 2}, \\ldots) = P(w_{t} | w_{t - 1})\n",
    "$$\n",
    "\n",
    "Given this assumption, the sentence probability can be written as a product of bi-gram probabilities which are easier to estimate by counting.\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, w_3) = P(w_3 | w_2)P(w_2 | w_1)P(w_1)\n",
    "$$\n",
    "\n",
    "More generally, the probability of a sentence with $T$ words can be written as\n",
    "\n",
    "$$\n",
    "P(w_T, w_{T - 1}, \\ldots, w_1) = P(w_1) \\prod_{t = 2}^{T} P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "As these probabilities would tend to be small, multiplying them in long sequences may lead to underflow problems because computer precision is limited. A common solution to this problem is to use the log-probabilities instead of the probabilities.\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P(w_T, w_{T - 1}, \\ldots, w_1) = \\log P(w_1) +  \\sum_{t = 2}^{T} \\log P(w_t | w_{t - 1})\n",
    "$$\n",
    "\n",
    "Another problem is that the log probabilities of sentences will be biased (XXX, w) toward shorter sentences, simply because there are less terms in a short sentence. To solve this problem, we can normalize the log probabilities by dividing by the number of words in the sentence.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:58:38.929473979Z",
     "start_time": "2023-12-18T15:58:35.901907219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917d7e9f8c053444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:03.150423303Z",
     "start_time": "2023-12-18T15:30:03.142622821Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\\nOh dear! I shall be late!' (when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "\n",
    "alice[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5289a38f6498fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:06.858917781Z",
     "start_time": "2023-12-18T15:30:03.149423919Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we will pass the whole text through spacy's pipeline\n",
    "\n",
    "doc = nlp(alice[:200])\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187e4541-8af1-46f6-bbe6-ed89d44c429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[          X\n",
      "alice      PROPN\n",
      "'s         PART\n",
      "adventures PROPN\n",
      "in         ADP\n",
      "wonderland PROPN\n",
      "by         ADP\n",
      "lewis      PROPN\n",
      "carroll    PROPN\n",
      "1865       NUM\n",
      "]          PUNCT\n",
      "\n",
      "\n",
      "         SPACE\n",
      "chapter    NOUN\n",
      "i.         PROPN\n",
      "down       ADP\n",
      "the        DET\n",
      "rabbit     PROPN\n",
      "-          PUNCT\n",
      "hole       PROPN\n",
      "\n",
      "\n",
      "         SPACE\n",
      "alice      PROPN\n",
      "was        AUX\n",
      "beginning  VERB\n",
      "to         PART\n",
      "get        VERB\n",
      "very       ADV\n",
      "tired      ADJ\n",
      "of         ADP\n",
      "sitting    VERB\n",
      "by         ADP\n",
      "her        PRON\n",
      "sister     NOUN\n",
      "on         ADP\n",
      "the        DET\n",
      "\n",
      "          SPACE\n",
      "bank       NOUN\n",
      ",          PUNCT\n",
      "and        CCONJ\n",
      "of         ADP\n",
      "having     VERB\n",
      "nothing    PRON\n",
      "to         PART\n",
      "do         VERB\n",
      ":          PUNCT\n",
      "once       ADV\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.lower_:10} {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee929852-223b-42c4-a3d7-51182aa7b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16549f41b17ec43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The result is an object of class `Doc` that we can use to a sequence of sentences and tokens. We will use the `sent()` generator to iterate over the first few sentences in the book and save them in a list. Next, we will create a small function to tokenize the sentences and remove the punctuation and spaces tokens. It wil also create a word to index dictionary and an index to word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2774138c46bc034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:07.850090121Z",
     "start_time": "2023-12-18T15:30:07.806243633Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_doc(text: str):\n",
    "    sentences = []\n",
    "    text_doc = nlp(text)\n",
    "    \n",
    "    word2idx = {\n",
    "        \"BEGINNING\": 0,\n",
    "        \"END\": 1\n",
    "    }\n",
    "    idx2word = {\n",
    "        0: \"BEGINNING\",\n",
    "        1: \"END\"\n",
    "    }\n",
    "    \n",
    "    for i, sentence in enumerate(text_doc.sents):\n",
    "        tokens = [\"BEGINNING\"]\n",
    "        \n",
    "        for token in sentence:            \n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "            token_normalized = token.lower_ \n",
    "            tokens.append(token_normalized)\n",
    "            \n",
    "            if token_normalized not in word2idx:\n",
    "                idx = len(word2idx)\n",
    "                word2idx[token_normalized] = idx\n",
    "                idx2word[idx] = token_normalized\n",
    "        \n",
    "        tokens.append(\"END\")\n",
    "        sentences.append(tokens)\n",
    "\n",
    "    return sentences, word2idx, idx2word\n",
    "\n",
    "tmp_, tmp_word2idx, tmp_idx2word = tokenize_doc(alice[0:1111])\n",
    "len(tmp_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd1fc7bfbbe3609a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:15.998848401Z",
     "start_time": "2023-12-18T15:30:15.996349040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we can create a V x V matrix where V is the size of the vocabulary\n",
    "\n",
    "def compute_bigrams_prob_mtx(sentences, word2idx: dict, smoothing: float = 1.0):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = len(word2idx)\n",
    "    \n",
    "    # Let's first create a matrix of counts\n",
    "    BGC = np.ones((vocab_size, vocab_size)) * smoothing\n",
    "\n",
    "    # Now let us loop over all sentences, create extract the bi-grams and count their occurrences\n",
    "    # Each time we encounter the sequence \"is strong\" for example, we will increment the count of the\n",
    "    # Row index of the first word and the column index of the second word\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            first_word = sent[i - 1]\n",
    "            \n",
    "            # We will use the word2idx dictionary to get the index of the word\n",
    "            first_word_idx = word2idx[first_word]\n",
    "            second_word_idx = word2idx[word]\n",
    "            # We will use the index to increment the count of the word\n",
    "            BGC[first_word_idx, second_word_idx] += 1\n",
    "    \n",
    "    # Now we can normalize the counts to get the probabilities\n",
    "    \n",
    "    BGP = BGC / BGC.sum(axis=1, keepdims=True)\n",
    "    return BGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dce8337c18f5e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:18.970391193Z",
     "start_time": "2023-12-18T15:30:18.967392052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def score_sentence_str(sentence_str: str, word2idx: dict, bigram_probs: np.ndarray):\n",
    "    # First we tokenize the sentence and remove the punctuation and spaces tokens\n",
    "    sents, _, _ = tokenize_doc(sentence_str)\n",
    "    \n",
    "    sentence_score = 0\n",
    "    \n",
    "    words = sents[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            first_word_idx = word2idx[words[i - 1]]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {words[i - 1]} not in vocabulary\")\n",
    "        \n",
    "        try:\n",
    "            second_word_idx = word2idx[word]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Word {word} not in vocabulary\")\n",
    "                \n",
    "        sentence_score += np.log(bigram_probs[first_word_idx, second_word_idx])\n",
    "    \n",
    "    return sentence_score / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d3f46d341e62ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:26.193805458Z",
     "start_time": "2023-12-18T15:30:23.055131383Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let us run the whole thing\n",
    "\n",
    "sentences, word2idx, idx2word = tokenize_doc(alice)\n",
    "BGP = compute_bigrams_prob_mtx(sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ed2001369513c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:28.350304554Z",
     "start_time": "2023-12-18T15:30:28.344750384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00023596, 0.0261916 ],\n",
       "       [0.00037313, 0.00037313]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGP[0:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4485438d3cbc7465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:32.196484124Z",
     "start_time": "2023-12-18T15:30:32.151874863Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.615723881262366"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence = sentences[32]\n",
    "# second_sentence\n",
    "score_sentence_str(\" \".join(second_sentence), word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918b438-009a-4616-8c27-5ca15535cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGINNING',\n",
       " 'no',\n",
       " 'it',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'do',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " 'shall',\n",
       " 'see',\n",
       " 'it',\n",
       " 'written',\n",
       " 'up',\n",
       " 'somewhere',\n",
       " 'END']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a928e1924e808af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:30:34.845609857Z",
     "start_time": "2023-12-18T15:30:34.817295475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.301816630052659"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it out with a valid sentence\n",
    "score_sentence_str(\"Be nice, everybody else is already taken.\", word2idx, BGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae281944685eaf35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:31:04.060613419Z",
     "start_time": "2023-12-18T15:31:04.013001546Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.605564297623913"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentence_str(\"Rude foot fun egg.\", word2idx, BGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904762ea8eedc39",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "Instead of counting the number of times a word occurs in the corpus, we can use a logistic regression model to estimate the probability of a word given the preceding word. The logistic regression model will learn a vector representation for each word in the vocabulary. The probability of a word given the preceding word will be the dot product of the vector representations of the two words.\n",
    "\n",
    "First we need to map the words to numbers, because the logistic regression model operates on matrices of numbers. What we can do is create a vocabulary (all unique words in our corpus) and represent each word with $V$ dimensional vector where $V$ is the size of the vocabulary. The vector will have a 1 at the index of the word and zeros everywhere else. This is called a one-hot encoding.\n",
    "\n",
    "For example, if our vocabulary is\n",
    "\n",
    "```\n",
    "[\"be\", \"yourself\", \"everyone\", \"else\", \"is\", \"already\", \"taken\"]\n",
    "```\n",
    "\n",
    "then the one-hot encoding of \"yourself\" will be\n",
    "\n",
    "```\n",
    "[0, 1, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "the one-hot encoding of \"is\" will be\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "Let's create a function to create these one-hot encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9ff839d4b9665eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T14:52:01.731480745Z",
     "start_time": "2023-12-17T14:52:01.726790237Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This will return a one-hot encoded vector with 1 at the index of idx\n",
    "def onehot_encode(idx: int, vocab_size: int):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[idx] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c1addb37c64ac3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:34:45.156404475Z",
     "start_time": "2023-12-18T15:34:45.134771469Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# It is convenient to have a function that processes the raw text and returns the word indices\n",
    "def text_to_indexed_sentences(sentences: list, word2idx: dict):\n",
    "    sentences_with_idx = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_with_idx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            idx = word2idx[word]    \n",
    "            sentence_with_idx.append(idx)\n",
    "            \n",
    "        sentences_with_idx.append(sentence_with_idx)\n",
    "    \n",
    "    return sentences_with_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77167fa020d6dd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:34:52.599776649Z",
     "start_time": "2023-12-18T15:34:52.588587258Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGINNING', 'hello', 'my', 'name', 'is', 'john', 'END']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences, sample_word2idx, sample_idx2word = tokenize_doc(\"Hello, my name is John. What is your name?\")\n",
    "sample_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84617605544ce6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:35:05.972585057Z",
     "start_time": "2023-12-18T15:35:05.928908797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 6, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_alice_sentences_idx = text_to_indexed_sentences(sample_sentences, sample_word2idx)\n",
    "sample_alice_sentences_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda35b72d7ebb70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we need to consider how the training data for our problem should look like. In a classification problem we normally a have $N \\times K$ matrix $\\mathbf{X}$, representing the $K$ features of $N$ observations.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\ldots & x_{1K} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{2K} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & x_{N2} & \\ldots & x_{NK} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and a $N$ dimensional vector $\\mathbf{y}$ representing the labels of the $N$ observations. It is convenient to represent the labels as one-hot encoded vectors. For example, with $K = 3$ classes, the labels will be $N \\times K$ matrix $\\mathbf{Y}$ of one-hot encoded vectors.\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the example above, the first observation belongs to the first class, the second observation belongs to the second class, the third observation belongs to the third class and the last observation belongs to the second class.\n",
    "\n",
    "In our specific case the labels are the second words in the bi-grams and each word is represented by a one-hot encoded vector. So the labels will be an $N \\times V$ matrix $\\mathbf{Y}$ where $V$ is the size of the vocabulary.\n",
    "\n",
    "The predictor matrix $\\mathbf{X}$ will also be an $N \\times V$ matrix. The $i$-th row of $\\mathbf{X}$ will be the one-hot encoded vector of the first word in the $i$-th bi-gram.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & 0 & \\ldots & 1 \\\\\n",
    "0 & 0 & 0 & \\ldots & 1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 1 & 0 & \\ldots & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can read this example matrix as: the first bi-gram is \"be yourself\", the second bi-gram is \"yourself everyone\", the third bi-gram is \"everyone else\" and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e8702f29bfc2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "$$\n",
    "p(y | x) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T \\mathbf{x})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb606ce280bdb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " Using the cross-entropy loss\n",
    " \n",
    "$$\n",
    "J(w) = -\\frac{1}{N}\\sum_{i = 1}^{N} \\sum_{j = 1}^{V} y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "the gradient descent update rule for the weights is\n",
    "\n",
    "$$\n",
    "W^{\\text{new}} = W^{\\text{old}} - \\eta \\nabla_{w} J(w) \\\\\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate and the gradient of the loss with respect to the weights is\n",
    "\n",
    "$$\n",
    "\\nabla J = X^T (\\hat{Y} - Y)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "669e01d6e6b5e745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:37:17.418953402Z",
     "start_time": "2023-12-18T15:37:17.417319502Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Next we will define the softmax function that will take a np.array of shape (N, D) and return a np.array of shape (N, D) where each row is\n",
    "# the softmax of the corresponding row in the input array\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def train_logistic(sentences: list[list[int]], vocab_size: int, learning_rate: float = 0.01, epochs: int = 100):    \n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch\n",
    "        np.random.shuffle(sentences)\n",
    "                       \n",
    "        # Initialize weights\n",
    "        W = np.random.randn(vocab_size, vocab_size) / np.sqrt(vocab_size)\n",
    "        \n",
    "        j = 0 # keep track of iterations\n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            \n",
    "            # An example sentence has the form [\"BEGINNING\", \"hello\", \"my\", \"name\", \"is\", \"john\", \"END\"]\n",
    "            # Only with the word indices instead of the words\n",
    "            # It has n = 7 words and therefore n - 1 = 6 bi-grams\n",
    "            # So each row of the inputs and targets matrices will have the shape (1, vocab_size)\n",
    "            \n",
    "            n = len(sentence)\n",
    "            \n",
    "            inputs = np.zeros((n - 1, vocab_size))\n",
    "            targets = np.zeros((n - 1, vocab_size))\n",
    "            inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "\n",
    "            #print(\"Inputs matrix\")\n",
    "            #print(inputs)\n",
    "\n",
    "            #print(\"Targets matrix\")\n",
    "            #print(targets)\n",
    "            \n",
    "            # Compute the predictions\n",
    "            predictions = softmax(inputs.dot(W))\n",
    "            \n",
    "            # Perform a gradient descent update\n",
    "            W = W - learning_rate * inputs.T.dot(predictions - targets)\n",
    "            \n",
    "            # Save the loss at each iteration (we don't use it here, but you may want to plot it later)\n",
    "            loss = - np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "            losses.append(loss)     \n",
    "            \n",
    "            if j % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "            j += 1\n",
    "    \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb92e37d67cb69bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:38:39.782170003Z",
     "start_time": "2023-12-18T15:38:36.747391340Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "full_sentences, full_word2idx, full_idx2word = tokenize_doc(alice)\n",
    "full_sentences_idx = text_to_indexed_sentences(full_sentences, full_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b90d13ab89bf10f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:38:41.221576416Z",
     "start_time": "2023-12-18T15:38:41.051727734Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_logistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_sentences_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword2idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m, in \u001b[0;36mtrain_logistic\u001b[0;34m(sentences, vocab_size, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m targets[np\u001b[38;5;241m.\u001b[39marange(n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), sentence[\u001b[38;5;241m1\u001b[39m:]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get output predictions\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m(inputs\u001b[38;5;241m.\u001b[39mdot(W))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# do a gradient descent step\u001b[39;00m\n\u001b[1;32m     33\u001b[0m W \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(predictions \u001b[38;5;241m-\u001b[39m targets)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "train_logistic(full_sentences_idx, len(word2idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
