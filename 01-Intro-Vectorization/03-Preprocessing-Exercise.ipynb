{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Exercise\n",
    "\n",
    "As an exercise we want to create a model for the classification of documents in a collection of BBC news articles. The dataset is available at <https://www.kaggle.com/c/learn-ai-bbc>\n",
    "\n",
    "The data consists of a training an test sets. The training set contains 1490 documents and the test set contains 735 documents. Each document is a news article and is labeled with one of 5 categories: business, entertainment, politics, sport, technology.\n",
    "\n",
    "- The first goal is to create a model that can classify the documents into two categories: politics and not politics and evaluate its predictive performance.\n",
    "- Next, remove the stopwords and evaluate the model again.\n",
    "- Next, use stemming and evaluate the model again.\n",
    "- Next, use lemmatization and evaluate the model again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>IsPolitics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1756</td>\n",
       "      <td>2d metal slug offers retro fun like some drill...</td>\n",
       "      <td>tech</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1108</td>\n",
       "      <td>blair stresses prosperity goals tony blair say...</td>\n",
       "      <td>politics</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>1955</td>\n",
       "      <td>weak dollar trims cadbury profits the world s ...</td>\n",
       "      <td>business</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>63</td>\n",
       "      <td>court rejects $280bn tobacco case a us governm...</td>\n",
       "      <td>business</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>293</td>\n",
       "      <td>christmas song formula  unveiled a formula for...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "91         1756  2d metal slug offers retro fun like some drill...   \n",
       "1103       1108  blair stresses prosperity goals tony blair say...   \n",
       "909        1955  weak dollar trims cadbury profits the world s ...   \n",
       "683          63  court rejects $280bn tobacco case a us governm...   \n",
       "561         293  christmas song formula  unveiled a formula for...   \n",
       "\n",
       "           Category  IsPolitics  \n",
       "91             tech       False  \n",
       "1103       politics        True  \n",
       "909        business       False  \n",
       "683        business       False  \n",
       "561   entertainment       False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/BBC%20News%20Train.csv.zip\").sample(frac=0.8, random_state=1)\n",
    "\n",
    "df[\"IsPolitics\"] = df[\"Category\"] == \"politics\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsPolitics\n",
       "False    0.810403\n",
       "True     0.189597\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"IsPolitics\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/amarov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.9790794979079498\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def get_pos_tag(self, tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        tokens_with_pos = nltk.pos_tag(word_tokenize(doc))\n",
    "        return [self.wnl.lemmatize(w, self.get_pos_tag(tag)) for w, tag in tokens_with_pos]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "\n",
    "\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(df['Text'], df['IsPolitics'], test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_txt)\n",
    "X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training accuracy:\", model.score(X_train, y_train))\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Politics words: ['$' 'u' 'company' '%' 'firm' 'online' 's' 'win' 'film' 'music']\n",
      "Politics words: ['brown' 'mr' 'britain' 'labour' 'minister' 'secretary' 'mp' 'election'\n",
      " 'blair' 'party']\n"
     ]
    }
   ],
   "source": [
    "# Print the most important words for each class\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print(\"Non-Politics words:\", feature_names[sorted_coef_index[:10]])\n",
    "print(\"Politics words:\", feature_names[sorted_coef_index[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a class `StemTokenizer` that uses the `PorterStemmer` from the `nltk` library to tokenize a text. The class should have a method `__call__` that receives a text and returns a list of tokens. Take the `LemmaTokenizer` class as a reference.\n",
    "\n",
    "- Fit a logistic regression model to the training data using the `StemTokenizer` class to tokenize\n",
    "- Compare the test performance of the model with the previous models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
