{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Depending on the task at hand, we may need to preprocess the text data in different ways. There is no universal solution as to how to preprocess text data, as the choice of action is application dependent. Here we will discuss some common preprocessing steps that are often used in text classification tasks.\n",
    "\n",
    "A key consideration here is how to balance the trade-off between the amount of information we want to retain and the dimensionality of the data. The more information we retain, the higher the dimensionality of the data.\n",
    "\n",
    "With high dimensionality\n",
    "    - More data needed\n",
    "    - More computational resources\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "There are different ways to tokenize text data.\n",
    "\n",
    "- Words\n",
    "    - Large vocabulary (up to a million words)\n",
    "    - Words convey meaning\n",
    "- Characters\n",
    "    - Smaller vocabulary (up to a few hundred characters) (26 letters in the English alphabet)\n",
    "    - Out-of-vocabulary characters\n",
    "- Subwords\n",
    "    - Medium vocabulary (up to a few thousand subwords)\n",
    "    - Sleep and sleeping are similar\n",
    "\n",
    "## Lowercasing\n",
    "\n",
    "A simple string split will treat `Hello` and `hello` as different words. This may not be desirable in some cases (high dimensionality) and we may want to lowercase all the words. However, in sentiment analysis, the capitalization of words may be important as these are often used to express emotions.\n",
    "\n",
    "## Punctuation\n",
    "\n",
    "Punctuation can be removed or kept. In some cases, punctuation can be important for the meaning of the text. For example, `I am happy` and `I am happy?` may have different meanings.\n",
    "\n",
    "## Accent Removal\n",
    "\n",
    "Accents can be removed from text data. This is often done in English text data to reduce the dimensionality of the data.\n",
    "\n",
    "## Stopwords\n",
    "\n",
    "Stopwords are common words that are often removed from text data as these words do not carry much meaning and removing them reduces the dimensionality of the feature space. Examples of stopwords are `the`, `is`, `and`, `but`, `not`, etc. As usual, there is no universal rule as in some cases, these words can be important for the meaning of the text. For example, `not` is a stopword but it can change the meaning of a sentence.\n",
    "\n",
    "- \"I am happy\" vs \"I am not happy\"\n",
    "- \"Problem\" vs \"No problem\"\n",
    "\n",
    "A problem with retaining the stopwords is that as they are very common, the vectors representing documents will be mapped close to each other in the feature space. As machine learning models rely on the distance between vectors, this can lead to poor performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Let's return to a simple string split example. If we have the words `running`, `runs`, and `run`, these words are similar in meaning. However, a simple string split will treat them as different words and `run` will be as different from `running` as is `tree`. Stemming is a rule-based process that removes suffixes from words to reduce them to their root form. \n",
    "\n",
    "Think about a search engine that needs to return results for the query `sleeping`. Simply querying for `sleeping` will not return results that contain `sleep`.\n",
    "\n",
    "There are different stemming algorithms available, such as the Porter Stemmer, the Snowball Stemmer, and the Lancaster Stemmer, among others. Each one of these implements a set of rules to reduce words to their root form.\n",
    "\n",
    "For example, the Porter Stemmer has the following rules:\n",
    "\n",
    "- `s` -> `''`\n",
    "- `ed` -> `''`\n",
    "- `ing` -> `''`\n",
    "- `sses` -> `ss`\n",
    "- `ment` -> `''` (Shipment -> Ship)\n",
    "- `ement` -> `''` (Agreement -> Agree)\n",
    "\n",
    "\n",
    "- Fast and simple\n",
    "- The result of stemming is not always a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "porter.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'replac'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('replacement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'children'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "happi\n",
      "happili\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"happiness\"))\n",
    "print(porter.stem(\"happy\"))\n",
    "print(porter.stem(\"happily\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a more sophisticated process that reduces words to their base or root form. The result of lemmatization is always a valid word. Lemmatization is slower than stemming as it requires a dictionary lookup as well as morphological analysis.\n",
    "\n",
    "Some words have multiple lemmas. For example, the word `better` has two lemmas: `good` and `well`. The choice of lemma depends on the context in which the word is used and the part of speech of the word.\n",
    "\n",
    "\n",
    "- Slower than stemming\n",
    "- Depends on part of speech\n",
    "- The result of lemmatization is always a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('running'))\n",
    "print(wnl.lemmatize('running', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happiness\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('happiness'))\n",
    "print(wnl.lemmatize('happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casting\n",
      "cast\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('casting'))\n",
    "print(wnl.lemmatize('casting', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mice\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"mice\"))\n",
    "print(wnl.lemmatize(\"mice\", 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing the part of speech to the lemmatizer can improve the performance of the lemmatizer. For example, the word `better` can be a noun or a verb. \n",
    "If we provide the part of speech to the lemmatizer, it can choose the correct lemma.\n",
    "\n",
    "- \"He is our better\" (noun)\n",
    "- \"He better run\" (verb)\n",
    "- \"He bettered the situation\" (verb)\n",
    "- \"He is better\" (adjective)\n",
    "- \"He better\" (adverb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "In order to provide the part of speech to the lemmatizer, we need to perform part of speech tagging. Part of speech tagging is the process of assigning a part of speech to each word in a sentence. The part of speech can be a noun, verb, adjective, adverb, etc. Part of speech tagging is a supervised machine learning task and there are many models available that can be used for this task.\n",
    "\n",
    "You can find a list of all the `nltk` part of speech tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), and examples [here](https://medium.com/@faisal-fida/the-complete-list-of-pos-tags-in-nltk-with-examples-eb0485f04321).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/amarov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Suddenly', 'she', 'came', 'upon', 'a', 'little', 'three-legged', 'table', ',', 'all', 'made', 'of', 'solid', 'glass', '.']\n",
      "Suddenly     --> RB\n",
      "she          --> PRP\n",
      "came         --> VBD\n",
      "upon         --> IN\n",
      "a            --> DT\n",
      "little       --> JJ\n",
      "three-legged --> JJ\n",
      "table        --> NN\n",
      ",            --> ,\n",
      "all          --> DT\n",
      "made         --> VBN\n",
      "of           --> IN\n",
      "solid        --> JJ\n",
      "glass        --> NN\n",
      ".            --> .\n"
     ]
    }
   ],
   "source": [
    "sent = \"Suddenly she came upon a little three-legged table, all made of solid glass.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "print(tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "for token in tagged_tokens:\n",
    "    print(f\"{token[0]:12} --> {token[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/amarov/anaconda3/envs/ta2024/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# The default pipeline in spaCy includes a tokenizer, a tagger, and a lemmatizer\n",
    "! python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suddenly --> ADV   --> suddenly\n",
      "she      --> PRON  --> she\n",
      "came     --> VERB  --> come\n",
      "upon     --> SCONJ --> upon\n",
      "a        --> DET   --> a\n",
      "little   --> ADJ   --> little\n",
      "three    --> NUM   --> three\n",
      "-        --> PUNCT --> -\n",
      "legged   --> ADJ   --> legged\n",
      "table    --> NOUN  --> table\n",
      ",        --> PUNCT --> ,\n",
      "all      --> PRON  --> all\n",
      "made     --> VERB  --> make\n",
      "of       --> ADP   --> of\n",
      "solid    --> ADJ   --> solid\n",
      "glass    --> NOUN  --> glass\n",
      ".        --> PUNCT --> .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:8} --> {token.pos_:5} --> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is WordNet?\n",
    "\n",
    "WordNet is a manually constructed lexical database that groups words into set of synonyms (synsets). Furthermore, it describes hierarchical relationships (is part of) between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the synset pike.n.02\n",
      "Meaning of the synset :  highly valued northern freshwater fish with lean flesh\n",
      "Hypernyms  [Synset('freshwater_fish.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synset = wn.synsets('pike')[1]\n",
    "\n",
    "print(\"Name of the synset\", synset.name())\n",
    "print(\"Meaning of the synset : \", synset.definition())\n",
    "print(\"Hypernyms \", synset.hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('expressway.n.01'),\n",
       " Synset('pike.n.02'),\n",
       " Synset('pike.n.03'),\n",
       " Synset('pike.n.04'),\n",
       " Synset('pike.n.05')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('pike')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
